WEBVTT

00:00.000 --> 00:12.000
 If you have any questions or other problems, please post them in the comments.

00:30.000 --> 00:50.000
 If you have any questions or other problems, please post them in the comments.

01:00.000 --> 01:10.000
 If you have any questions or other problems, please post them in the comments.

01:30.000 --> 01:40.000
 If you have any questions or other problems, please post them in the comments.

02:00.000 --> 02:10.000
 If you have any questions or other problems, please post them in the comments.

02:30.000 --> 02:40.000
 If you have any questions or other problems, please post them in the comments.

03:00.000 --> 03:10.000
 If you have any questions or other problems, please post them in the comments.

03:30.000 --> 03:40.000
 If you have any questions or other problems, please post them in the comments.

04:00.000 --> 04:10.000
 If you have any questions or other problems, please post them in the comments.

04:30.000 --> 04:40.000
 If you have any questions or other problems, please post them in the comments.

05:00.000 --> 05:10.000
 If you have any questions or other problems, please post them in the comments.

05:30.000 --> 05:40.000
 If you have any questions or other problems, please post them in the comments.

06:00.000 --> 06:10.000
 Thank you for watching!

06:30.000 --> 06:40.000
 If you have any questions or other problems, please post them in the comments.

07:00.000 --> 07:10.000
 If you have any questions or other problems, please post them in the comments.

07:30.000 --> 07:40.000
 If you have any questions or other problems, please post them in the comments.

08:00.000 --> 08:20.000
 If you have any questions or other problems, please post them in the comments.

08:30.000 --> 08:40.000
 If you have any questions or other problems, please post them in the comments.

09:00.000 --> 09:10.000
 If you have any questions or other problems, please post them in the comments.

09:30.000 --> 09:40.000
 If you have any questions or other problems, please post them in the comments.

10:00.000 --> 10:10.000
 If you have any questions or other problems, please post them in the comments.

10:30.000 --> 10:50.000
 If you have any questions or other problems, please post them in the comments.

11:00.000 --> 11:10.000
 If you have any questions or other problems, please post them in the comments.

11:30.000 --> 11:40.000
 If you have any questions or other problems, please post them in the comments.

12:00.000 --> 12:10.000
 If you have any questions or other problems, please post them in the comments.

12:30.000 --> 12:40.000
 If you have any questions or other problems, please post them in the comments.

13:00.000 --> 13:10.000
 If you have any questions or other problems, please post them in the comments.

13:30.000 --> 13:40.000
 If you have any questions or other problems, please post them in the comments.

14:00.000 --> 14:20.000
 Oh!

14:32.000 --> 14:34.000
 All right. Welcome, everybody.

14:34.000 --> 14:42.000
 Let's give everyone a moment to get back in the audience.

14:42.000 --> 14:46.000
 All right. Great.

14:46.000 --> 14:50.000
 Welcome to Tesla AI Day 2022.

14:50.000 --> 14:56.000
 Thank you.

14:56.000 --> 15:00.000
 We've got some really exciting things to show you.

15:00.000 --> 15:04.000
 I think you'll be pretty impressed.

15:04.000 --> 15:10.000
 I do want to set some expectations with respect to our Optimus robot.

15:10.000 --> 15:16.000
 As you know, last year it was just a person in a robot suit.

15:16.000 --> 15:20.000
 But we've come a long way.

15:20.000 --> 15:24.000
 Compared to that, it's going to be very impressive.

15:24.000 --> 15:32.000
 We're going to talk about the advancements in AI for full self-driving,

15:32.000 --> 15:36.000
 as well as how they apply more generally to real-world AI problems,

15:36.000 --> 15:40.000
 like a humanoid robot and even going beyond that.

15:40.000 --> 15:46.000
 I think there's some potential that what we're doing here at Tesla

15:46.000 --> 15:50.000
 could make a meaningful contribution to AGI.

15:50.000 --> 15:56.000
 I think actually Tesla is a good entity to do it from a governance standpoint,

15:56.000 --> 16:00.000
 because we're a publicly traded company with one class of stock.

16:00.000 --> 16:06.000
 That means the public controls Tesla, and I think that's actually a good thing.

16:06.000 --> 16:10.000
 If I go crazy, you can fire me. This is important.

16:10.000 --> 16:16.000
 Maybe I've gone crazy. I don't know.

16:16.000 --> 16:20.000
 We're going to talk a lot about our progress in AI Autopilot,

16:20.000 --> 16:24.000
 as well as the progress with Dojo.

16:24.000 --> 16:28.000
 Then we're going to bring the team out and do a long Q&A.

16:28.000 --> 16:32.000
 You can ask tough questions, whatever you'd like,

16:32.000 --> 16:34.000
 existential questions, technical questions.

16:34.000 --> 16:40.000
 We want to have as much time for Q&A as possible.

16:40.000 --> 16:46.000
 Let's see. With that, do you guys want to say anything?

16:46.000 --> 16:50.000
 Hey, guys. I'm Milan. I work on Autopilot, and it is rubber.

16:50.000 --> 16:54.000
 I'm Lizzy, a mechanical engineer on the project as well.

16:54.000 --> 16:58.000
 Okay. Should we bring out the bot?

16:58.000 --> 17:03.000
 Before we do that, we have one little bonus tip for the day.

17:03.000 --> 17:08.000
 This is actually the first time we try this robot without any backup support,

17:08.000 --> 17:12.000
 cranes, mechanical mechanisms, no cables, nothing.

17:12.000 --> 17:16.000
 We want to do it with you guys tonight, but it's the first time. Let's see.

17:16.000 --> 17:33.000
 You ready? Let's go.

17:46.000 --> 18:13.000
 I think the bot got some moves here.

18:25.000 --> 18:30.000
 This is essentially the same self-driving computer that runs in your Tesla cars, by the way.

18:35.000 --> 18:39.000
 This is literally the first time the robot has operated without a tether.

18:39.000 --> 18:46.000
 It was on stage tonight.

19:15.000 --> 19:18.000
 The robot can actually do a lot more than we just showed you.

19:18.000 --> 19:21.000
 We just didn't want it to fall on its face.

19:21.000 --> 19:28.000
 We'll show you some videos now of the robot doing a bunch of other things,

19:28.000 --> 19:31.000
 which are less risky.

19:31.000 --> 19:41.000
 We should close the screen, guys.

19:41.000 --> 19:44.000
 Yeah, we wanted to show a little bit more of what we've done over the past few months with the bot

19:44.000 --> 19:49.000
 and just walking around and dancing on stage.

19:49.000 --> 19:54.000
 Just humble beginnings, but you can see the autopilot neural networks running

19:54.000 --> 19:59.000
 as it's just retrained for the bot directly on that new platform.

19:59.000 --> 20:01.000
 That's my watering can.

20:01.000 --> 20:06.000
 Yeah, when you see a rendered view, that's the world the robot sees.

20:06.000 --> 20:18.000
 So it's very clearly identifying objects, like this is the object it should pick up, picking it up.

20:18.000 --> 20:23.000
 We use the same process as we did for autopilot to connect data and train neural networks

20:23.000 --> 20:25.000
 that we then deploy on the robot.

20:25.000 --> 20:31.000
 That's an example that illustrates the upper body a little bit more.

20:31.000 --> 20:38.000
 Something that we'll try to nail down in a few months, over the next few months, I would say, to perfection.

20:38.000 --> 20:43.000
 This is really an actual station in the Fremont factory as well that it's working at.

20:43.000 --> 20:55.000
 Yep.

20:55.000 --> 20:57.000
 That's not the only thing we have to show today, right?

20:57.000 --> 21:00.000
 Yeah, absolutely.

21:00.000 --> 21:02.000
 What you saw was what we call BumbleSea.

21:02.000 --> 21:10.000
 That's our sort of rough development robot using semi-off-the-shelf actuators.

21:10.000 --> 21:14.000
 But we actually have gone a step further than that already.

21:14.000 --> 21:16.000
 The team's done an incredible job.

21:16.000 --> 21:26.000
 And we actually have an Optimus bot with fully Tesla designed and built actuators, battery pack, control system, everything.

21:26.000 --> 21:31.000
 It wasn't quite ready to walk, but I think it will walk in a few weeks.

21:31.000 --> 21:38.000
 But we wanted to show you the robot, something that's actually fairly close to what we'll go into production

21:38.000 --> 21:41.000
 and show you all the things it can do.

21:41.000 --> 22:08.000
 So let's bring it out.

22:08.000 --> 22:33.000
 All right.

22:33.000 --> 22:44.000
 So here you're seeing Optimus with the degrees of freedom that we expect to have in Optimus production unit one,

22:44.000 --> 22:51.000
 which is the ability to move all the fingers independently, move the thumb, have two degrees of freedom.

22:51.000 --> 22:55.000
 So it has opposable thumbs, both left and right hand.

22:55.000 --> 22:58.000
 So it's able to operate tools and do useful things.

22:58.000 --> 23:04.000
 Our goal is to make a useful humanoid robot as quickly as possible.

23:04.000 --> 23:10.000
 And we've also designed it using the same discipline that we use in designing the car,

23:10.000 --> 23:21.000
 which is to say to design it for manufacturing such that it's possible to make the robot in high volume at low cost with high reliability.

23:21.000 --> 23:23.000
 So that's incredibly important.

23:23.000 --> 23:29.000
 I mean, you've all seen very impressive humanoid robot demonstrations, and that's great.

23:29.000 --> 23:31.000
 But what are they missing?

23:31.000 --> 23:33.000
 They're missing a brain.

23:33.000 --> 23:37.000
 They don't have the intelligence to navigate the world by themselves.

23:37.000 --> 23:42.000
 And they're also very expensive and made in low volume.

23:42.000 --> 23:49.000
 Whereas this is, Optimus is designed to be an extremely capable robot, but made in very high volume,

23:49.000 --> 23:56.000
 probably ultimately millions of units, and it is expected to cost much less than a car.

23:56.000 --> 24:02.000
 So I would say probably less than $20,000 would be my guess.

24:02.000 --> 24:14.000
 The potential for Optimus is I think appreciated by very few people.

24:14.000 --> 24:21.000
 As usual, Tesla demos are coming in hot.

24:21.000 --> 24:35.000
 The team has put in an incredible amount of work, working seven days a week,

24:35.000 --> 24:40.000
 running the 3M oil to get to the demonstration today.

24:40.000 --> 24:42.000
 Super proud of what they've done.

24:42.000 --> 24:44.000
 They've really done a great job.

24:44.000 --> 24:57.000
 I'd just like to give a hand to the whole Optimus team.

24:57.000 --> 25:04.000
 So there's still a lot of work to be done to refine Optimus and improve it.

25:04.000 --> 25:07.000
 Obviously this is just Optimus version one.

25:07.000 --> 25:14.000
 And that's really why we're holding this event, which is to convince some of the most talented people in the world,

25:14.000 --> 25:20.000
 like you guys, to join Tesla and help make it a reality,

25:20.000 --> 25:26.000
 and bring it to fruition at scale such that it can help millions of people.

25:26.000 --> 25:32.000
 And the potential, like I said, really boggles the mind.

25:32.000 --> 25:42.000
 Because you have to say, what is an economy? An economy is sort of productive entities times their productivity.

25:42.000 --> 25:46.000
 Capital times productivity per capita.

25:46.000 --> 25:52.000
 At the point at which there is not a limitation on capital, it's not clear what an economy even means at that point.

25:52.000 --> 25:56.000
 An economy becomes quasi-infinite.

25:56.000 --> 26:09.000
 So taken to fruition in the hopefully benign scenario, this means a future of abundance,

26:09.000 --> 26:22.000
 a future where there is no poverty, where you can have whatever you want in terms of products and services.

26:22.000 --> 26:30.000
 It really is a fundamental transformation of civilization as we know it.

26:30.000 --> 26:37.000
 Obviously we want to make sure that transformation is a positive one and safe.

26:37.000 --> 26:42.000
 But that's also why I think Tesla as an entity doing this,

26:42.000 --> 26:50.000
 being a single class of stock publicly traded, owned by the public, is very important and should not be overlooked.

26:50.000 --> 26:55.000
 I think this is essential because then if the public doesn't like what Tesla is doing,

26:55.000 --> 27:00.000
 the public can buy shares in Tesla and vote differently.

27:00.000 --> 27:03.000
 This is a big deal.

27:03.000 --> 27:06.000
 It's very important that I can't just do what I want.

27:06.000 --> 27:14.000
 Sometimes people think that, but it's not true.

27:14.000 --> 27:25.000
 It's very important that the corporate entity that makes this happen is something that the public can properly influence.

27:25.000 --> 27:33.000
 So I think the Tesla structure is ideal for that.

27:33.000 --> 27:40.000
 Like I said, self-driving cars will certainly have a tremendous impact on the world.

27:40.000 --> 27:53.000
 I think they will improve the productivity of transport by at least a half order of magnitude, perhaps an order of magnitude, perhaps more.

27:53.000 --> 28:06.000
 Optimus I think has maybe a two order of magnitude potential improvement in economic output.

28:06.000 --> 28:14.000
 It's not clear what the limit actually even is.

28:14.000 --> 28:16.000
 But we need to do this in the right way.

28:16.000 --> 28:27.000
 We need to do it carefully and safely and ensure that the outcome is one that is beneficial to civilization and one that humanity wants.

28:27.000 --> 28:34.000
 This is also extremely important, obviously.

28:34.000 --> 28:44.000
 And I hope you will consider joining Tesla to achieve those goals.

28:44.000 --> 28:53.000
 At Tesla we really care about doing the right thing here or aspire to do the right thing and really not pave the road to hell with good intentions.

28:53.000 --> 28:58.000
 And I think the road to hell is mostly paved with bad intentions, but every now and again there's a good intention in there.

28:58.000 --> 29:05.000
 So we want to do the right thing, so consider joining us and helping make it happen.

29:05.000 --> 29:08.000
 With that, let's move on to the next phase.

29:08.000 --> 29:16.000
 Right on. Thank you, Elon.

29:16.000 --> 29:20.000
 All right, so you've seen a couple of robots today. Let's do a quick timeline recap.

29:20.000 --> 29:25.000
 So last year we unveiled the Tesla Bot concept, but a concept doesn't get us very far.

29:25.000 --> 29:31.000
 We knew we needed a real development and integration platform to get real-life learnings as quickly as possible.

29:31.000 --> 29:36.000
 So that robot that came out and did the little routine for you guys, we had that within six months.

29:36.000 --> 29:42.000
 Built working on software integration, hardware upgrades over the months since then.

29:42.000 --> 29:47.000
 But in parallel, we've also been designing the next generation, this one over here.

29:47.000 --> 29:53.000
 So this guy is rooted in the foundation of sort of the vehicle design process.

29:53.000 --> 29:56.000
 We're leveraging all of those learnings that we already have.

29:56.000 --> 30:01.000
 Obviously there's a lot that's changed since last year, but there's a few things that are still the same, you'll notice.

30:01.000 --> 30:05.000
 We still have this really detailed focus on the true human form.

30:05.000 --> 30:07.000
 We think that matters for a few reasons.

30:07.000 --> 30:11.000
 But it's fun. We spend a lot of time thinking about how amazing the human body is.

30:11.000 --> 30:16.000
 We have this incredible range of motion, typically really amazing strength.

30:16.000 --> 30:21.000
 A fun exercise is if you put your fingertip on the chair in front of you,

30:21.000 --> 30:27.000
 you'll notice that there's a huge range of motion that you have in your shoulder and your elbow, for example.

30:27.000 --> 30:31.000
 Without moving your fingertip, you can move those joints all over the place.

30:31.000 --> 30:36.000
 But the robot, you know, its main function is to do real useful work.

30:36.000 --> 30:40.000
 And it maybe doesn't necessarily need all of those degrees of freedom right away.

30:40.000 --> 30:44.000
 So we've stripped it down to a minimum sort of 28 fundamental degrees of freedom,

30:44.000 --> 30:47.000
 and then of course our hands in addition to that.

30:47.000 --> 30:52.000
 Humans are also pretty efficient at some things and not so efficient in other times.

30:52.000 --> 30:57.000
 So for example, we can eat a small amount of food to sustain ourselves for several hours. That's great.

30:57.000 --> 31:02.000
 But when we're just kind of sitting around, no offense, but we're kind of inefficient.

31:02.000 --> 31:04.000
 We're just sort of burning energy.

31:04.000 --> 31:09.000
 So on the robot platform, what we're going to do is we're going to minimize that idle power consumption,

31:09.000 --> 31:12.000
 drop it as low as possible, and that way we can just flip a switch

31:12.000 --> 31:17.000
 and immediately the robot turns into something that does useful work.

31:17.000 --> 31:21.000
 So let's talk about this latest generation in some detail, shall we?

31:21.000 --> 31:26.000
 So on the screen here you'll see in orange are actuators, which we'll get to in a little bit,

31:26.000 --> 31:29.000
 and in blue are electrical systems.

31:29.000 --> 31:35.000
 So now that we have our sort of human-based research, and we have our first development platform,

31:35.000 --> 31:39.000
 we have both research and execution to draw from for this design.

31:39.000 --> 31:45.000
 Again, we're using that vehicle design foundation, so we're taking it from concept through design and analysis,

31:45.000 --> 31:48.000
 and then build and validation.

31:48.000 --> 31:52.000
 Along the way we're going to optimize for things like cost and efficiency,

31:52.000 --> 31:56.000
 because those are critical metrics to take this product to scale eventually.

31:56.000 --> 31:58.000
 How are we going to do that?

31:58.000 --> 32:03.000
 Well, we're going to reduce our part count and our power consumption of every element possible.

32:03.000 --> 32:07.000
 We're going to do things like reduce the sensing and the wiring at our extremities.

32:07.000 --> 32:14.000
 You can imagine a lot of mass in your hands and feet is going to be quite difficult and power consumptive to move around.

32:14.000 --> 32:21.000
 And we're going to centralize both our power distribution and our compute to the physical center of the platform.

32:21.000 --> 32:26.000
 So in the middle of our torso, actually it is the torso, we have our battery pack.

32:26.000 --> 32:31.000
 This is sized at 2.3 kilowatt hours, which is perfect for about a full day's worth of work.

32:31.000 --> 32:35.000
 What's really unique about this battery pack is it has all of the battery electronics

32:35.000 --> 32:39.000
 integrated into a single PCB within the pack.

32:39.000 --> 32:48.000
 So that means everything from sensing to fusing, charge management, and power distribution is all in one place.

32:48.000 --> 32:57.000
 We're also leveraging both our vehicle products and our energy products to roll all of those key features into this battery.

32:57.000 --> 33:02.000
 So that's streamlined manufacturing, really efficient and simple cooling methods,

33:02.000 --> 33:05.000
 battery management, and also safety.

33:05.000 --> 33:11.000
 And of course we can leverage Tesla's existing infrastructure and supply chain to make it.

33:11.000 --> 33:16.000
 So going on to sort of our brain, it's not in the head, but it's pretty close.

33:16.000 --> 33:19.000
 Also in our torso we have our central computer.

33:19.000 --> 33:25.000
 So as you know, Tesla already ships full self-driving computers in every vehicle we produce.

33:25.000 --> 33:30.000
 We want to leverage both the autopilot hardware and the software for the humanoid platform,

33:30.000 --> 33:36.000
 but because it's different in requirements and in form factor, we're going to change a few things first.

33:36.000 --> 33:42.000
 So it's going to do everything that a human brain does, processing vision data,

33:42.000 --> 33:48.000
 making split-second decisions based on multiple sensory inputs, and also communications.

33:48.000 --> 33:53.000
 So to support communications, it's equipped with wireless connectivity as well as audio support.

33:53.000 --> 33:56.000
 And then it also has hardware-level security features,

33:56.000 --> 34:02.000
 which are important to protect both the robot and the people around the robot.

34:02.000 --> 34:07.000
 So now that we have our sort of core, we're going to need some limbs on this guy.

34:07.000 --> 34:12.000
 And we'd love to show you a little bit about our actuators and our fully functional hands as well.

34:12.000 --> 34:15.000
 But before we do that, I'd like to introduce Malcolm,

34:15.000 --> 34:19.000
 who's going to speak a little bit about our structural foundation for the robot.

34:19.000 --> 34:29.000
 Thank you, Lizzie.

34:29.000 --> 34:33.000
 Tesla have the capabilities to analyze highly complex systems.

34:33.000 --> 34:36.000
 They don't get much more complex than a crash.

34:36.000 --> 34:42.000
 You can see here a simulated crash on Model 3 superimposed on top of the actual physical crash.

34:42.000 --> 34:45.000
 It's actually incredible how accurate it is.

34:45.000 --> 34:48.000
 Just to give you an idea of the complexity of this model,

34:48.000 --> 34:51.000
 it includes every nut, bolt, and washer, every spot weld,

34:51.000 --> 34:54.000
 and it has 35 million degrees of freedom.

34:54.000 --> 34:55.000
 It's quite amazing.

34:55.000 --> 34:58.000
 And it's true to say that if we didn't have models like this,

34:58.000 --> 35:01.000
 we wouldn't be able to make the safest cars in the world.

35:01.000 --> 35:10.000
 So can we utilize our capabilities and our methods from the automotive side to influence a robot?

35:10.000 --> 35:13.000
 Well, we can make a model, and since we had crash software,

35:13.000 --> 35:16.000
 we're using the same software here, we can make it fall down.

35:16.000 --> 35:20.000
 The purpose of this is to make sure that if it falls down, ideally it doesn't,

35:20.000 --> 35:23.000
 but it's superficial damage.

35:23.000 --> 35:26.000
 We don't want it to, for example, break its gearbox and its arms.

35:26.000 --> 35:29.000
 That's the equivalent of a dislocated shoulder of a robot.

35:29.000 --> 35:32.000
 Difficult and expensive to fix.

35:32.000 --> 35:38.000
 So we want it to dust itself off, get on with the job it's been given.

35:38.000 --> 35:42.000
 We can also take the same model, and we can drive the actuators

35:42.000 --> 35:47.000
 using the inputs from a previously solved model, bringing it to life.

35:47.000 --> 35:51.000
 So this is producing the motions for the tasks we want the robot to do.

35:51.000 --> 35:55.000
 These tasks are picking up boxes, turning, squatting, walking upstairs.

35:55.000 --> 35:59.000
 Whatever the set of tasks are, we can play through the model.

35:59.000 --> 36:01.000
 This is showing just simple walking.

36:01.000 --> 36:04.000
 We can create the stresses in all the components.

36:04.000 --> 36:09.000
 That helps us optimize the components.

36:09.000 --> 36:11.000
 These are not dancing robots.

36:11.000 --> 36:14.000
 This is the modal behavior, the first five modes of the robot.

36:14.000 --> 36:18.000
 Typically, when people make robots, they make sure the first mode

36:18.000 --> 36:22.000
 is up around the top single figures, up towards 10 Hz.

36:22.000 --> 36:26.000
 The reason we do this is to make the controls of walking easier.

36:26.000 --> 36:30.000
 It's very difficult to walk if you can't guarantee where your foot is wobbling around.

36:30.000 --> 36:32.000
 That's okay if you make one robot.

36:32.000 --> 36:34.000
 We want to make thousands, maybe millions.

36:34.000 --> 36:38.000
 We haven't got the luxury of making them from carbon fiber and titanium.

36:38.000 --> 36:42.000
 We want to make them from plastic. Things are not quite as stiff.

36:42.000 --> 36:44.000
 So we can't have these high targets.

36:44.000 --> 36:47.000
 I call them dumb targets.

36:47.000 --> 36:49.000
 We've got to make them work at lower targets.

36:49.000 --> 36:51.000
 So is that going to work?

36:51.000 --> 36:53.000
 Well, if you think about it, sorry about this,

36:53.000 --> 36:57.000
 but we're just bags of soggy, jelly and bones thrown in.

36:57.000 --> 36:59.000
 We're not high frequency.

36:59.000 --> 37:03.000
 If I stand on my leg, I don't vibrate at 10 Hz.

37:03.000 --> 37:07.000
 People operate at low frequency, so we know the robot actually can.

37:07.000 --> 37:09.000
 It just makes controls harder.

37:09.000 --> 37:13.000
 So we take the information from this, the modal data and the stiffness,

37:13.000 --> 37:18.000
 feed it into the control system that allows it to walk.

37:18.000 --> 37:21.000
 Just changing tack slightly, looking at the knee.

37:21.000 --> 37:24.000
 We can take some inspiration from biology,

37:24.000 --> 37:27.000
 and we can look to see what the mechanical advantage of the knee is.

37:27.000 --> 37:31.000
 It turns out it actually represents quite similar to a four-bar link,

37:31.000 --> 37:33.000
 and that's quite non-linear.

37:33.000 --> 37:36.000
 That's not surprising, really, because if you think when you bend your leg down,

37:36.000 --> 37:39.000
 the torque on your knee is much more when it's bent

37:39.000 --> 37:41.000
 than it is when it's straight.

37:41.000 --> 37:43.000
 So you'd expect a non-linear function,

37:43.000 --> 37:46.000
 and in fact, the biology is non-linear.

37:46.000 --> 37:49.000
 This matches it quite accurately.

37:49.000 --> 37:51.000
 So that's the representation.

37:51.000 --> 37:53.000
 The four-bar link is obviously not physically a four-bar link.

37:53.000 --> 37:55.000
 As I said, the characteristics are similar.

37:55.000 --> 37:58.000
 But me bending down, that's not very scientific.

37:58.000 --> 38:00.000
 Let's be a bit more scientific.

38:00.000 --> 38:04.000
 We've played all the tasks through this graph,

38:04.000 --> 38:07.000
 and this is showing picketing up, walking, squatting,

38:07.000 --> 38:10.000
 the tasks I said we did on the stress.

38:10.000 --> 38:14.000
 And that's the torque seen at the knee

38:14.000 --> 38:17.000
 against the knee bend on the horizontal axis.

38:17.000 --> 38:20.000
 This is showing the requirement for the knee to do all these tasks.

38:20.000 --> 38:24.000
 And then put a curve through it, surfing over the top of the peaks,

38:24.000 --> 38:28.000
 and that's saying this is what's required to make the robot do these tasks.

38:28.000 --> 38:32.000
 So if we look at the four-bar link, that's actually the green curve,

38:32.000 --> 38:35.000
 and it's saying that the non-linearity of the four-bar link

38:35.000 --> 38:38.000
 has actually linearized the characteristic of the force.

38:38.000 --> 38:40.000
 What that really says is that's lower the force.

38:40.000 --> 38:43.000
 That's what makes the actuator have the lowest possible force,

38:43.000 --> 38:45.000
 which is the most efficient.

38:45.000 --> 38:47.000
 We want to burn energy up slowly.

38:47.000 --> 38:49.000
 What's the blue curve?

38:49.000 --> 38:52.000
 Well, the blue curve is actually if we didn't have a four-bar link,

38:52.000 --> 38:54.000
 we just had an arm sticking out of my leg here

38:54.000 --> 38:56.000
 with an actuarial force.

38:56.000 --> 39:00.000
 I have my leg here with an actuator on it, a simple two-bar link.

39:00.000 --> 39:03.000
 That's the best we could do with a simple two-bar link,

39:03.000 --> 39:06.000
 and it shows that that would create much more force in the actuator,

39:06.000 --> 39:08.000
 which would not be efficient.

39:10.000 --> 39:12.000
 So what's that look like in practice?

39:12.000 --> 39:16.000
 Well, as you'll see, it's very tightly packaged in the knee.

39:16.000 --> 39:18.000
 You'll see it go transparent in a second.

39:18.000 --> 39:21.000
 You'll see the four-bar link there is operating on the actuator.

39:21.000 --> 39:25.000
 This is determining the force and the displacements on the actuator.

39:25.000 --> 39:28.000
 I now pass you over to Constantinos to tell you a lot more detail

39:28.000 --> 39:31.000
 about how these actuators are made and designed and optimized.

39:31.000 --> 39:33.000
 Thank you.

39:33.000 --> 39:35.000
 Thank you, Malcolm.

39:41.000 --> 39:46.000
 So I would like to talk to you about the design process

39:46.000 --> 39:50.000
 and the actuator portfolio in our robot.

39:50.000 --> 39:53.000
 So there are many similarities between a car and a robot

39:53.000 --> 39:56.000
 when it comes to powertrain design.

39:56.000 --> 40:01.000
 The most important thing that matters here is energy, mass, and cost.

40:01.000 --> 40:04.000
 We are carrying over most of our designing experience

40:04.000 --> 40:06.000
 from the car to the robot.

40:09.000 --> 40:13.000
 So in the particular case, you see a car with two drive units,

40:13.000 --> 40:17.000
 and the drive units are used in order to accelerate the car

40:17.000 --> 40:23.000
 0 to 60 miles per hour time or drive a city, drive site.

40:23.000 --> 40:28.000
 While the robot that has 28 actuators,

40:28.000 --> 40:32.000
 it's not obvious what are the tasks at the actuator level.

40:32.000 --> 40:38.000
 So we have tasks that are higher level, like walking or climbing stairs

40:38.000 --> 40:45.000
 or carrying a heavy object which need to be translated into joint specs.

40:45.000 --> 40:51.000
 Therefore, we use our model that generates the torque speed trajectories

40:51.000 --> 40:56.000
 for our joints, which subsequently is going to be fed in our optimization model

40:56.000 --> 40:59.000
 to run through the optimization process.

41:01.000 --> 41:05.000
 This is one of the scenarios that the robot is capable of doing,

41:05.000 --> 41:08.000
 which is turning and walking.

41:08.000 --> 41:11.000
 So when we have this torque speed trajectory,

41:11.000 --> 41:14.000
 we lay it over an efficiency map of an actuator,

41:14.000 --> 41:20.000
 and we are able along the trajectory to generate the power consumption

41:20.000 --> 41:26.000
 and the cumulative energy for the task versus time.

41:26.000 --> 41:30.000
 So this allows us to define the system cost for the particular actuator

41:30.000 --> 41:33.000
 and put a simple point into the cloud.

41:33.000 --> 41:36.000
 Then we do this for hundreds of thousands of actuators

41:36.000 --> 41:38.000
 by solving in our cluster.

41:38.000 --> 41:40.000
 And the red line denotes the Pareto front,

41:40.000 --> 41:44.000
 which is the preferred area where we will look for optimal.

41:44.000 --> 41:48.000
 So the X denotes the preferred actuator design

41:48.000 --> 41:50.000
 we have picked for this particular joint.

41:50.000 --> 41:53.000
 So now we need to do this for every joint.

41:53.000 --> 41:57.000
 We have 28 joints to optimize, and we parse our cloud.

41:57.000 --> 42:01.000
 We parse our cloud, again, for every joint spec,

42:01.000 --> 42:06.000
 and the red axis this time denotes the bespoke actuator designs

42:06.000 --> 42:07.000
 for every joint.

42:07.000 --> 42:12.000
 The problem here is that we have too many unique actuator designs,

42:12.000 --> 42:14.000
 and even if we take advantage of the symmetry,

42:14.000 --> 42:16.000
 still there are too many.

42:16.000 --> 42:19.000
 In order to make something mass-manufacturable,

42:19.000 --> 42:23.000
 we need to be able to reduce the amount of unique actuator designs.

42:23.000 --> 42:26.000
 Therefore, we run something called commonality study,

42:26.000 --> 42:29.000
 which we parse our cloud, again,

42:29.000 --> 42:33.000
 looking this time for actuators that simultaneously meet

42:33.000 --> 42:35.000
 the joint performance requirements

42:35.000 --> 42:37.000
 for more than one joint at the same time.

42:37.000 --> 42:40.000
 So the resulting portfolio is six actuators,

42:40.000 --> 42:44.000
 and they show in a color map in the middle figure.

42:44.000 --> 42:49.000
 And the actuators can be also viewed in this slide.

42:49.000 --> 42:52.000
 We have three rotary and three linear actuators,

42:52.000 --> 42:59.000
 all of which have a great output force or torque per mass.

42:59.000 --> 43:02.000
 The rotary actuator in particular has a mechanical clutch

43:02.000 --> 43:06.000
 integrated on the high-speed side, angular contact ball bearing,

43:06.000 --> 43:10.000
 and on the high-speed side and on the low-speed side,

43:10.000 --> 43:17.000
 a cross roller bearing, and the gear train is a strain-wave gear.

43:17.000 --> 43:20.000
 There are three integrated sensors here

43:20.000 --> 43:25.000
 and the bespoke permanent magnet machine.

43:25.000 --> 43:36.000
 The linear actuator has planetary rollers

43:36.000 --> 43:40.000
 and an inverted planetary screw as a gear train,

43:40.000 --> 43:44.000
 which allows efficiency and compaction and durability.

43:44.000 --> 43:49.000
 So in order to demonstrate the force capability of our linear actuators,

43:49.000 --> 43:59.000
 we have set up an experiment in order to test it under its limits.

43:59.000 --> 44:08.000
 And I will let you enjoy the video.

44:08.000 --> 44:26.000
 So our actuator is able to lift a half-ton, nine-foot concert grand piano.

44:26.000 --> 44:33.000
 And this is a requirement.

44:33.000 --> 44:35.000
 It's not something nice to have,

44:35.000 --> 44:39.000
 because our muscles can do the same when they are directly driven.

44:39.000 --> 44:44.000
 When they are directly driven, our quadricep muscles can do the same thing.

44:44.000 --> 44:48.000
 It's just that the knee is an up-gearing linkage system

44:48.000 --> 44:52.000
 that converts the force into velocity at the end effector of our heels

44:52.000 --> 44:56.000
 for purposes of giving to the human body agility.

44:56.000 --> 45:00.000
 So this is one of the main things that are amazing about the human body.

45:00.000 --> 45:03.000
 And I'm concluding my part at this point,

45:03.000 --> 45:05.000
 and I'd like to welcome my colleague, Mike,

45:05.000 --> 45:07.000
 who is going to talk to you about hand design.

45:07.000 --> 45:10.000
 Thank you very much.

45:10.000 --> 45:13.000
 Thanks, Konstantinos.

45:13.000 --> 45:18.000
 So we just saw how powerful a human and a humanoid actuator can be.

45:18.000 --> 45:24.000
 However, humans are also incredibly dexterous.

45:24.000 --> 45:28.000
 The human hand has the ability to move at 300 degrees per second.

45:28.000 --> 45:31.000
 It has tens of thousands of tactile sensors.

45:31.000 --> 45:37.000
 And it has the ability to grasp and manipulate almost every object in our daily lives.

45:37.000 --> 45:40.000
 For our robotic hand design, we were inspired by biology.

45:40.000 --> 45:43.000
 We have five fingers and an opposable thumb.

45:43.000 --> 45:48.000
 Our fingers are driven by metallic tendons that are both flexible and strong.

45:48.000 --> 45:51.000
 We have the ability to complete wide-aperture power grasps

45:51.000 --> 45:57.000
 while also being optimized for precision gripping of small, thin, and delicate objects.

45:57.000 --> 46:00.000
 So why a human-like robotic hand?

46:00.000 --> 46:03.000
 Well, the main reason is that our factories and the world around us

46:03.000 --> 46:05.000
 is designed to be ergonomic.

46:05.000 --> 46:09.000
 So what that means is that it ensures that objects in our factory are graspable,

46:09.000 --> 46:13.000
 but it also ensures that new objects that we may have never seen before

46:13.000 --> 46:18.000
 can be grasped by the human hand and by our robotic hand as well.

46:18.000 --> 46:20.000
 The converse there is pretty interesting,

46:20.000 --> 46:22.000
 because it's saying that these objects are designed to our hand

46:22.000 --> 46:27.000
 instead of having to make changes to our hand to accompany a new object.

46:27.000 --> 46:31.000
 Some basic stats about our hand is that it has six actuators and 11 degrees of freedom.

46:31.000 --> 46:37.000
 It has an in-hand controller, which drives the fingers and receives sensor feedback.

46:37.000 --> 46:39.000
 Sensor feedback is really important to learn a little bit more

46:39.000 --> 46:43.000
 about the objects that we're grasping and also for proprioception,

46:43.000 --> 46:48.000
 and that's the ability for us to recognize where our hand is in space.

46:48.000 --> 46:51.000
 One of the important aspects of our hand is that it's adaptive.

46:51.000 --> 46:54.000
 This adaptability is involved essentially as complex mechanisms

46:54.000 --> 46:58.000
 that allow the hand to adapt to the objects that's being grasped.

46:58.000 --> 47:02.000
 Another important part is that we have a non-backdrivable finger drive.

47:02.000 --> 47:05.000
 This clutching mechanism allows us to hold and transport objects

47:05.000 --> 47:08.000
 without having to turn on the hand motors.

47:08.000 --> 47:12.000
 You just heard how we went about designing the Tesla Bot hardware.

47:12.000 --> 47:16.000
 Now I'll hand it off to Milan and our autonomy team to bring this robot to life.

47:16.000 --> 47:29.000
 All right, so all those cool things we've shown earlier in the video

47:29.000 --> 47:33.000
 were possible just in a matter of a few months,

47:33.000 --> 47:37.000
 thanks to the amazing work that we've done on autopilot over the past few years.

47:37.000 --> 47:41.000
 Most of those components ported quite easily over to the bot's environment.

47:41.000 --> 47:45.000
 If you think about it, we're just moving from a robot on wheels to a robot on legs.

47:45.000 --> 47:48.000
 So some of the components are pretty similar,

47:48.000 --> 47:51.000
 and some other require more heavy lifting.

47:51.000 --> 47:54.000
 For example, our computer vision neural networks

47:54.000 --> 47:59.000
 were ported directly from autopilot to the bot's situation.

47:59.000 --> 48:02.000
 It's exactly the same occupancy network that we'll talk into

48:02.000 --> 48:05.000
 a little bit more details later with the autopilot team

48:05.000 --> 48:08.000
 that is now running on the bot here in this video.

48:08.000 --> 48:14.000
 The only thing that changed really is the training data that we had to recollect.

48:14.000 --> 48:18.000
 We're also trying to find ways to improve those occupancy networks

48:18.000 --> 48:21.000
 using work made on neural radiance fields

48:21.000 --> 48:25.000
 to get really great volumetric rendering of the bot's environments.

48:25.000 --> 48:32.000
 For example here, some machinery that the bot might have to interact with.

48:32.000 --> 48:35.000
 Another interesting problem to think about is,

48:35.000 --> 48:39.000
 in indoor environments, mostly with absence of GPS signal,

48:39.000 --> 48:42.000
 how do you get the bot to navigate to its destination?

48:42.000 --> 48:45.000
 Say for instance, to find its nearest charging station.

48:45.000 --> 48:51.000
 So we've been training more neural networks to identify high frequency features,

48:51.000 --> 48:53.000
 key points within the bot's camera streams,

48:53.000 --> 48:59.000
 and track them across frames over time as the bot navigates with its environment.

48:59.000 --> 49:03.000
 And we're using those points to get a better estimate of the bot's pose

49:03.000 --> 49:09.000
 and trajectory within its environment as it's walking.

49:09.000 --> 49:11.000
 We also did quite some work on the simulation side,

49:11.000 --> 49:14.000
 and this is literally the autopilot simulator,

49:14.000 --> 49:18.000
 to which we've integrated the robot's locomotion code.

49:18.000 --> 49:23.000
 And this is a video of the motion control code running in the autopilot simulator,

49:23.000 --> 49:27.000
 showing the evolution of the robot's walk over time.

49:27.000 --> 49:29.000
 And so as you can see, we started quite slowly in April,

49:29.000 --> 49:32.000
 and started accelerating as we unlocked more joints

49:32.000 --> 49:37.000
 and deployed more advanced techniques like arms balancing over the past few months.

49:37.000 --> 49:41.000
 And so locomotion is specifically one component that's very different

49:41.000 --> 49:44.000
 as we're moving from the car to the bot's environment.

49:44.000 --> 49:46.000
 And so I think it warrants a little bit more depth,

49:46.000 --> 49:57.000
 and I'd like my colleagues to start talking about this now.

49:57.000 --> 49:59.000
 Thank you Milan.

49:59.000 --> 50:02.000
 Hi everyone, I'm Felix, I'm a robotics engineer on the project,

50:02.000 --> 50:04.000
 and I'm going to talk about walking.

50:04.000 --> 50:07.000
 Walking seems easy, right?

50:07.000 --> 50:08.000
 People do it every day.

50:08.000 --> 50:10.000
 You don't even have to think about it.

50:10.000 --> 50:15.000
 But there are some aspects of walking which are challenging from an engineering perspective.

50:15.000 --> 50:19.000
 For example, physical self-awareness.

50:19.000 --> 50:22.000
 That means having a good representation of yourself.

50:22.000 --> 50:23.000
 What is the length of your limbs?

50:23.000 --> 50:25.000
 What is the mass of your limbs?

50:25.000 --> 50:26.000
 What is the size of your feet?

50:26.000 --> 50:28.000
 All that matters.

50:28.000 --> 50:31.000
 Also, having an energy-efficient gait.

50:31.000 --> 50:37.000
 You can imagine there's different styles of walking, and all of them are equally efficient.

50:37.000 --> 50:39.000
 Most important, keep balance.

50:39.000 --> 50:41.000
 Don't fall.

50:41.000 --> 50:45.000
 And of course, also coordinate the motion of all of your limbs together.

50:45.000 --> 50:48.000
 So now, humans do all of this naturally,

50:48.000 --> 50:52.000
 but as engineers or roboticists, we have to think about these problems.

50:52.000 --> 50:57.000
 And I'm going to show you how we address them in our locomotion planning and control stack.

50:57.000 --> 51:02.000
 So we start with locomotion planning and our representation of the bot.

51:02.000 --> 51:07.000
 That means a model of the robot's kinematics, dynamics, and the contact properties.

51:07.000 --> 51:11.000
 And using that model and the desired path for the bots,

51:11.000 --> 51:16.000
 our locomotion planner generates reference trajectories for the entire system.

51:16.000 --> 51:22.000
 This means feasible trajectories with respect to the assumptions of our model.

51:22.000 --> 51:25.000
 The planner currently works in three stages.

51:25.000 --> 51:30.000
 It starts planning footsteps and ends with the entire motion for the system.

51:30.000 --> 51:33.000
 And let's dive a little bit deeper in how this works.

51:33.000 --> 51:37.000
 So in this video, we see footsteps being planned over a planning horizon,

51:37.000 --> 51:40.000
 following the desired path.

51:40.000 --> 51:45.000
 And we start from this and add then trajectories that connect these footsteps

51:45.000 --> 51:50.000
 using toe-off and heel strike, just as humans do.

51:50.000 --> 51:55.000
 And this gives us a larger stride and less knee bend for high efficiency of the system.

51:55.000 --> 51:59.000
 The last stage is then finding the center of mass trajectory,

51:59.000 --> 52:04.000
 which gives us a dynamically feasible motion of the entire system to keep balance.

52:04.000 --> 52:09.000
 As we all know, plans are good, but we also have to realize them in reality.

52:09.000 --> 52:12.000
 Let's see how we can do this.

52:12.000 --> 52:21.000
 Thank you, Felix.

52:21.000 --> 52:23.000
 Hello, everyone. My name is Anand.

52:23.000 --> 52:26.000
 And I'm going to talk to you about controls.

52:26.000 --> 52:30.000
 So let's take the motion plan that Felix just talked about

52:30.000 --> 52:33.000
 and put it in the real world on a real robot.

52:33.000 --> 52:37.000
 Let's see what happens.

52:37.000 --> 52:40.000
 It takes a couple steps and falls down.

52:40.000 --> 52:43.000
 Well, that's a little disappointing.

52:43.000 --> 52:48.000
 But we're missing a few key pieces here which will make it walk.

52:48.000 --> 52:54.000
 Now, as Felix mentioned, the motion planner is using an idealized version of itself

52:54.000 --> 52:57.000
 and a version of reality around it.

52:57.000 --> 52:59.000
 This is not exactly correct.

52:59.000 --> 53:05.000
 It also expresses its intention through trajectories and wrenches,

53:05.000 --> 53:07.000
 wrenches are forces and torques,

53:07.000 --> 53:12.000
 that it wants to exert on the world to locomote.

53:12.000 --> 53:16.000
 Reality is way more complex than any similar model.

53:16.000 --> 53:18.000
 Also, the robot is not simplified.

53:18.000 --> 53:25.000
 It's got vibrations and modes, compliance, sensor noise, and on and on and on.

53:25.000 --> 53:30.000
 So what does that do to the real world when you put the bot in the real world?

53:30.000 --> 53:34.000
 Well, the unexpected forces cause unmodeled dynamics,

53:34.000 --> 53:36.000
 which essentially the planner doesn't know about,

53:36.000 --> 53:38.000
 and that causes destabilization,

53:38.000 --> 53:44.000
 especially for a system that is dynamically stable like biped locomotion.

53:44.000 --> 53:46.000
 So what can we do about it?

53:46.000 --> 53:48.000
 Well, we measure reality.

53:48.000 --> 53:53.000
 We use sensors and our understanding of the world to do state estimation.

53:53.000 --> 53:57.000
 And here you can see the attitude and pelvis pose,

53:57.000 --> 54:00.000
 which is essentially the vestibular system in a human,

54:00.000 --> 54:03.000
 along with the center of mass trajectory being tracked

54:03.000 --> 54:07.000
 when the robot's walking in the office environment.

54:07.000 --> 54:11.000
 Now we have all the pieces we need in order to close the loop.

54:11.000 --> 54:14.000
 So we use our better bot model.

54:14.000 --> 54:18.000
 We use the understanding of reality that we've gained through state estimation,

54:18.000 --> 54:22.000
 and we compare what we want versus what we expect the reality,

54:22.000 --> 54:28.000
 expect that reality is doing to us in order to add corrections

54:28.000 --> 54:30.000
 to the behavior of the robot.

54:30.000 --> 54:34.000
 Here, the robot certainly doesn't appreciate being poked,

54:34.000 --> 54:38.000
 but it does an admirable job of staying upright.

54:38.000 --> 54:43.000
 The final point here is a robot that walks is not enough.

54:43.000 --> 54:48.000
 We need it to use its hands and arms to be useful.

54:48.000 --> 54:51.000
 Let's talk about manipulation.

54:51.000 --> 55:01.000
 Hi, everyone.

55:01.000 --> 55:04.000
 My name is Eric, robotics engineer on Tesla Bot,

55:04.000 --> 55:09.000
 and I want to talk about how we've made the robot manipulate things in the real world.

55:09.000 --> 55:15.000
 We wanted to manipulate objects while looking as natural as possible

55:15.000 --> 55:17.000
 and also get there quickly.

55:17.000 --> 55:21.000
 So what we've done is we've broken this process down into two steps.

55:21.000 --> 55:24.000
 First is generating a library of natural motion references,

55:24.000 --> 55:26.000
 or we could call them demonstrations,

55:26.000 --> 55:29.000
 and then we've adapted these motion references online

55:29.000 --> 55:33.000
 to the current real world situation.

55:33.000 --> 55:36.000
 So let's say we have a human demonstration of picking up an object.

55:36.000 --> 55:39.000
 We can get a motion capture of that demonstration,

55:39.000 --> 55:43.000
 which is visualized right here as a bunch of key frames

55:43.000 --> 55:46.000
 representing the locations of the hands, the elbows, the torso.

55:46.000 --> 55:49.000
 We can map that to the robot using inverse kinematics.

55:49.000 --> 55:55.000
 And if we collect a lot of these, now we have a library that we can work with.

55:55.000 --> 56:01.000
 But a single demonstration is not generalizable to the variation in the real world.

56:01.000 --> 56:06.000
 For instance, this would only work for a box in a very particular location.

56:06.000 --> 56:10.000
 So what we've also done is run these reference trajectories

56:10.000 --> 56:12.000
 through a trajectory optimization program,

56:12.000 --> 56:15.000
 which solves for where the hand should be,

56:15.000 --> 56:22.000
 how the robot should balance when it needs to adapt the motion to the real world.

56:22.000 --> 56:25.000
 So for instance, if the box is in this location,

56:25.000 --> 56:31.000
 then our optimizer will create this trajectory instead.

56:31.000 --> 56:37.000
 Next, Milan is going to talk about what's next for the optimist, Tesla Lab.

56:37.000 --> 56:38.000
 Thanks.

56:38.000 --> 56:45.000
 Thanks, Eric.

56:45.000 --> 56:47.000
 Right, so hopefully by now you guys got a good idea

56:47.000 --> 56:50.000
 of what we've been up to over the past few months.

56:50.000 --> 56:54.000
 We started having something that's usable, but it's far from being useful.

56:54.000 --> 56:58.000
 There's still a long and exciting road ahead of us.

56:58.000 --> 57:00.000
 I think the first thing within the next few weeks

57:00.000 --> 57:03.000
 is to get optimists at least at par with BumbleSea,

57:03.000 --> 57:07.000
 the other bot prototype you saw earlier, and probably beyond.

57:07.000 --> 57:10.000
 We are also going to start focusing on the real use case

57:10.000 --> 57:16.000
 at one of our factories and really going to try to nail this down

57:16.000 --> 57:20.000
 and iron out all the elements needed to deploy this product in the real world.

57:20.000 --> 57:24.000
 I was mentioning earlier, you know, indoor navigation,

57:24.000 --> 57:27.000
 graceful form management, or even servicing,

57:27.000 --> 57:31.000
 all components needed to scale this product up.

57:31.000 --> 57:35.000
 But I don't know about you, but after seeing what we've shown tonight,

57:35.000 --> 57:38.000
 I'm pretty sure we can get this done within the next few months or years

57:38.000 --> 57:43.000
 and make this product a reality and change the entire economy.

57:43.000 --> 57:46.000
 So I would like to thank the entire optimist team

57:46.000 --> 57:48.000
 for their hard work over the past few months.

57:48.000 --> 57:49.000
 I think it's pretty amazing.

57:49.000 --> 57:51.000
 All of this was done in barely six or eight months.

57:51.000 --> 57:53.000
 Thank you very much.

57:53.000 --> 58:08.000
 Thank you.

58:08.000 --> 58:11.000
 Hey, everyone.

58:11.000 --> 58:12.000
 Hi, I'm Ashok.

58:12.000 --> 58:15.000
 I lead the Autopilot team alongside Milan.

58:15.000 --> 58:19.000
 God, it's coming so hard to top that optimist section.

58:19.000 --> 58:22.000
 We'll try nonetheless.

58:22.000 --> 58:27.000
 Every Tesla that has been built over the last several years,

58:27.000 --> 58:31.000
 we think has the hardware to make the car drive itself.

58:31.000 --> 58:37.000
 We have been working on the software to add higher and higher levels of autonomy.

58:37.000 --> 58:40.000
 This time around last year, we had roughly 2,000 cars

58:40.000 --> 58:43.000
 driving our FSD beta software.

58:43.000 --> 58:46.000
 Since then, we have significantly improved the software's robustness

58:46.000 --> 58:54.000
 and capability that we have now shipped into 160,000 customers as of today.

58:54.000 --> 59:00.000
 Thank you.

59:00.000 --> 59:01.000
 This has not come for free.

59:01.000 --> 59:04.000
 It came from the sweat and blood of the engineering team

59:04.000 --> 59:06.000
 over the last one year.

59:06.000 --> 59:11.000
 For example, we trained 75,000 neural network models just last one year.

59:11.000 --> 59:15.000
 That's roughly a model every eight minutes.

59:15.000 --> 59:17.000
 That's coming out of the team,

59:17.000 --> 59:19.000
 and then we evaluate them on our clusters,

59:19.000 --> 59:22.000
 and then we ship 281 of those models

59:22.000 --> 59:25.000
 that actually improve the performance of the car.

59:25.000 --> 59:29.000
 And this space of innovation is happening throughout the stack.

59:29.000 --> 59:33.000
 The planning software, the infrastructure, the tools, even hiring,

59:33.000 --> 59:38.000
 everything is progressing to the next level.

59:38.000 --> 59:42.000
 The FSD beta software is quite capable of driving the car.

59:42.000 --> 59:45.000
 It should be able to navigate from parking lot to parking lot,

59:45.000 --> 59:49.000
 handling city street driving, stopping for traffic lights and stop signs,

59:49.000 --> 59:56.000
 negotiating with objects at intersections, making turns, and so on.

59:56.000 --> 59:59.000
 All of this comes from the camera streams

59:59.000 --> 1:00:02.000
 that go through our neural networks that run on the car itself.

1:00:02.000 --> 1:00:04.000
 It's not coming back to the server or anything.

1:00:04.000 --> 1:00:07.000
 It runs on the car and produces all the outputs

1:00:07.000 --> 1:00:09.000
 to form the world model around the car,

1:00:09.000 --> 1:00:13.000
 and the FSD beta software drives the car based on that.

1:00:13.000 --> 1:00:17.000
 Today we'll go into a lot of the components that make up the system.

1:00:17.000 --> 1:00:23.000
 The occupancy network acts as the base geometry layer of the system.

1:00:23.000 --> 1:00:27.000
 This is a multi-camera video neural network

1:00:27.000 --> 1:00:31.000
 that from the images predicts the full physical occupancy

1:00:31.000 --> 1:00:34.000
 of the world around the robot.

1:00:34.000 --> 1:00:36.000
 So anything that's physically present,

1:00:36.000 --> 1:00:40.000
 trees, walls, buildings, cars, walls, what have you,

1:00:40.000 --> 1:00:42.000
 if it's physically present, it predicts them,

1:00:42.000 --> 1:00:46.000
 along with their future motion.

1:00:46.000 --> 1:00:51.000
 On top of this base level of geometry, we have more semantic layers.

1:00:51.000 --> 1:00:56.000
 In order to navigate the roadways, we need the lanes, of course.

1:00:56.000 --> 1:00:58.000
 But then the roadways have lots of different lanes,

1:00:58.000 --> 1:01:00.000
 and they connect in all kinds of ways.

1:01:00.000 --> 1:01:02.000
 So it's actually a really difficult problem

1:01:02.000 --> 1:01:04.000
 for typical computer vision techniques

1:01:04.000 --> 1:01:06.000
 to set up lanes and their activities.

1:01:06.000 --> 1:01:09.000
 So we reached all the way into language technologies

1:01:09.000 --> 1:01:11.000
 and then pulled the state of the art from other domains

1:01:11.000 --> 1:01:16.000
 and not just computer vision to make this task possible.

1:01:16.000 --> 1:01:21.000
 For vehicles, we need their full kinematic state to control for them.

1:01:21.000 --> 1:01:24.000
 All of this directly comes from neural networks.

1:01:24.000 --> 1:01:28.000
 Video streams, raw video streams, come into the networks,

1:01:28.000 --> 1:01:31.000
 go through a lot of processing, and then outputs the full kinematic state,

1:01:31.000 --> 1:01:34.000
 their positions, velocities, acceleration, jerk,

1:01:34.000 --> 1:01:38.000
 all of that directly comes out of networks with minimal post-processing.

1:01:38.000 --> 1:01:41.000
 That's really fascinating to me because how is this even possible?

1:01:41.000 --> 1:01:44.000
 What world do we live in that this magic is possible

1:01:44.000 --> 1:01:47.000
 that these networks predict fourth derivatives of these positions

1:01:47.000 --> 1:01:53.000
 when people thought we couldn't even detect these objects?

1:01:53.000 --> 1:01:55.000
 My opinion is that it did not come for free.

1:01:55.000 --> 1:01:57.000
 It required tons of data,

1:01:57.000 --> 1:02:00.000
 so we had to build sophisticated auto-labeling systems.

1:02:00.000 --> 1:02:03.000
 That churn through raw sensor data,

1:02:03.000 --> 1:02:05.000
 run a ton of offline compute on the servers.

1:02:05.000 --> 1:02:09.000
 It can take a few hours, run expensive neural networks,

1:02:09.000 --> 1:02:15.000
 distill the information into labels that train our in-car neural networks.

1:02:15.000 --> 1:02:18.000
 On top of this, we also use our simulation system

1:02:18.000 --> 1:02:20.000
 to synthetically create images,

1:02:20.000 --> 1:02:25.000
 and since it's a simulation, we trivially have all the labels.

1:02:25.000 --> 1:02:29.000
 All of this goes through a well-oiled data engine pipeline

1:02:29.000 --> 1:02:33.000
 where we first train a baseline model with some data,

1:02:33.000 --> 1:02:36.000
 ship it to the car, see what the failures are,

1:02:36.000 --> 1:02:41.000
 and once we know the failures, we mine the fleet for the cases where it fails,

1:02:41.000 --> 1:02:45.000
 provide the correct labels, and add the data to the training set.

1:02:45.000 --> 1:02:48.000
 This process systematically fixes the issues,

1:02:48.000 --> 1:02:52.000
 and we do this for every task that runs in the car.

1:02:52.000 --> 1:02:55.000
 Yeah, and to train these new massive neural networks,

1:02:55.000 --> 1:02:59.000
 this year we expanded our training infrastructure by roughly 40 to 50%,

1:02:59.000 --> 1:03:03.000
 so that fits us at about 14,000 GPUs today

1:03:03.000 --> 1:03:07.000
 across multiple training clusters in the United States.

1:03:07.000 --> 1:03:09.000
 We also worked on our AI compiler,

1:03:09.000 --> 1:03:13.000
 which now supports new operations needed by those neural networks

1:03:13.000 --> 1:03:18.000
 and map them to the best of our underlying hardware resources.

1:03:18.000 --> 1:03:21.000
 And our inference engine today is capable of distributing

1:03:21.000 --> 1:03:23.000
 the execution of a single neural network

1:03:23.000 --> 1:03:26.000
 across two independent systems on chips,

1:03:26.000 --> 1:03:28.000
 essentially two independent computers

1:03:28.000 --> 1:03:32.000
 interconnected within the same full self-driving computer.

1:03:32.000 --> 1:03:35.000
 And to make this possible, we had to keep a tight control

1:03:35.000 --> 1:03:37.000
 on the end-to-end latency of this new system,

1:03:37.000 --> 1:03:43.000
 so we deployed more advanced scheduling code across the full FSD platform.

1:03:43.000 --> 1:03:45.000
 All of these neural networks running in the car

1:03:45.000 --> 1:03:47.000
 together produce the vector space,

1:03:47.000 --> 1:03:50.000
 which is again the model of the world around the robot or the car,

1:03:50.000 --> 1:03:53.000
 and then the planning system operates on top of this,

1:03:53.000 --> 1:03:56.000
 coming up with trajectories that avoid collisions or smooth,

1:03:56.000 --> 1:03:58.000
 make progress towards the destination,

1:03:58.000 --> 1:04:00.000
 using a combination of model-based optimization

1:04:00.000 --> 1:04:07.000
 plus neural network that helps optimize it to be really fast.

1:04:07.000 --> 1:04:11.000
 Today, we are really excited to present progress on all of these areas.

1:04:11.000 --> 1:04:13.000
 We have the engineering leads standing by

1:04:13.000 --> 1:04:15.000
 to come in and explain these various blocks,

1:04:15.000 --> 1:04:17.000
 and these power not just the car,

1:04:17.000 --> 1:04:20.000
 but the same components also run on the Optimus robot

1:04:20.000 --> 1:04:22.000
 that Milan showed earlier.

1:04:22.000 --> 1:04:25.000
 With that, I welcome Parul to start talking about the planning section.

1:04:25.000 --> 1:04:36.000
 Hi, all. I'm Parul Jain.

1:04:36.000 --> 1:04:39.000
 Let's use this intersection scenario to dive straight into

1:04:39.000 --> 1:04:43.000
 how we do the planning and decision-making in Autopilot.

1:04:43.000 --> 1:04:46.000
 So we are approaching this intersection from a side street,

1:04:46.000 --> 1:04:49.000
 and we have to yield to all the crossing vehicles.

1:04:49.000 --> 1:04:52.000
 Right as we are about to enter the intersection,

1:04:52.000 --> 1:04:55.000
 the pedestrian on the other side of the intersection

1:04:55.000 --> 1:04:58.000
 decides to cross the road without a crosswalk.

1:04:58.000 --> 1:05:00.000
 Now, we need to yield to this pedestrian,

1:05:00.000 --> 1:05:02.000
 yield to the vehicles from the right,

1:05:02.000 --> 1:05:05.000
 and also understand the relation between the pedestrian

1:05:05.000 --> 1:05:09.000
 and the vehicle on the other side of the intersection.

1:05:09.000 --> 1:05:12.000
 So a lot of these intra-object dependencies

1:05:12.000 --> 1:05:15.000
 that we need to resolve in a quick glance.

1:05:15.000 --> 1:05:17.000
 And humans are really good at this.

1:05:17.000 --> 1:05:20.000
 We look at a scene, understand all the possible interactions,

1:05:20.000 --> 1:05:23.000
 evaluate the most promising ones,

1:05:23.000 --> 1:05:27.000
 and generally end up choosing a reasonable one.

1:05:27.000 --> 1:05:29.000
 So let's look at a few of these interactions

1:05:29.000 --> 1:05:31.000
 that Autopilot system evaluated.

1:05:31.000 --> 1:05:33.000
 We could have gone in front of this pedestrian

1:05:33.000 --> 1:05:36.000
 with a very aggressive launch and lateral profile.

1:05:36.000 --> 1:05:38.000
 Now, obviously we are being a jerk to the pedestrian,

1:05:38.000 --> 1:05:41.000
 and we would spook the pedestrian and his cute pet.

1:05:41.000 --> 1:05:44.000
 We could have moved forward slowly,

1:05:44.000 --> 1:05:46.000
 short for a gap between the pedestrian

1:05:46.000 --> 1:05:48.000
 and the vehicle from the right.

1:05:48.000 --> 1:05:51.000
 Again, we are being a jerk to the vehicle coming from the right,

1:05:51.000 --> 1:05:54.000
 but you should not outright reject this interaction

1:05:54.000 --> 1:05:58.000
 in case this is only safe interaction available.

1:05:58.000 --> 1:06:01.000
 Lastly, the interaction we ended up choosing,

1:06:01.000 --> 1:06:04.000
 stay slow initially, find the reasonable gap,

1:06:04.000 --> 1:06:09.000
 and then finish the maneuver after all the agents pass.

1:06:09.000 --> 1:06:12.000
 Now, evaluation of all of these interactions is not trivial,

1:06:12.000 --> 1:06:15.000
 especially when you care about modeling

1:06:15.000 --> 1:06:18.000
 the higher-order derivatives for other agents.

1:06:18.000 --> 1:06:21.000
 For example, what is the longitudinal jerk required

1:06:21.000 --> 1:06:23.000
 by the vehicle coming from the right

1:06:23.000 --> 1:06:25.000
 when you assert in front of it?

1:06:25.000 --> 1:06:28.000
 Relying purely on collision checks with marginal predictions

1:06:28.000 --> 1:06:30.000
 will only get you so far,

1:06:30.000 --> 1:06:33.000
 because you will miss out on a lot of valid interactions.

1:06:33.000 --> 1:06:35.000
 This basically boils down to solving

1:06:35.000 --> 1:06:38.000
 the multi-agent joint trajectory planning problem

1:06:38.000 --> 1:06:42.000
 over the trajectories of ego and all the other agents.

1:06:42.000 --> 1:06:44.000
 Now, how much ever you optimize,

1:06:44.000 --> 1:06:46.000
 there's gonna be a limit to how fast

1:06:46.000 --> 1:06:48.000
 you can run this optimization problem.

1:06:48.000 --> 1:06:50.000
 It will be close to order of 10 milliseconds,

1:06:50.000 --> 1:06:54.000
 even after a lot of incremental approximations.

1:06:54.000 --> 1:06:57.000
 Now, for a typical crowded unprotected lift,

1:06:57.000 --> 1:06:59.000
 say you have more than 20 objects,

1:06:59.000 --> 1:07:02.000
 each object having multiple different future modes,

1:07:02.000 --> 1:07:06.000
 the number of relevant interaction combinations will blow up.

1:07:06.000 --> 1:07:11.000
 The planner needs to make a decision every 50 milliseconds.

1:07:11.000 --> 1:07:14.000
 So how do we solve this in real time?

1:07:14.000 --> 1:07:17.000
 We rely on a framework what we call as interaction search,

1:07:17.000 --> 1:07:19.000
 which is basically a paralyzed tree search

1:07:19.000 --> 1:07:22.000
 over a bunch of maneuver trajectories.

1:07:22.000 --> 1:07:26.000
 The state space here corresponds to the kinematic state of ego,

1:07:26.000 --> 1:07:28.000
 the kinematic state of other agents,

1:07:28.000 --> 1:07:31.000
 the nominal future multimodal predictions,

1:07:31.000 --> 1:07:34.000
 and all the static entities in the scene.

1:07:34.000 --> 1:07:39.000
 The action space is where things get interesting.

1:07:39.000 --> 1:07:42.000
 We use a set of maneuver trajectory candidates

1:07:42.000 --> 1:07:45.000
 to branch over a bunch of interaction decisions

1:07:45.000 --> 1:07:49.000
 and also incremental goals for a longer horizon maneuver.

1:07:49.000 --> 1:07:52.000
 Let's walk through this tree search very quickly

1:07:52.000 --> 1:07:54.000
 to get a sense of how it works.

1:07:54.000 --> 1:07:57.000
 We start with a set of vision measurements,

1:07:57.000 --> 1:08:00.000
 namely lanes, occupancy, moving objects.

1:08:00.000 --> 1:08:02.000
 These get represented as fast extractions

1:08:02.000 --> 1:08:05.000
 as well as latent features.

1:08:05.000 --> 1:08:08.000
 We use this to create a set of goal candidates,

1:08:08.000 --> 1:08:10.000
 lanes again from the lanes network,

1:08:10.000 --> 1:08:13.000
 or unstructured regions which correspond to a probability mask

1:08:13.000 --> 1:08:17.000
 derived from human demonstrations.

1:08:17.000 --> 1:08:19.000
 Once we have a bunch of these goal candidates,

1:08:19.000 --> 1:08:21.000
 we create tree trajectories using a combination

1:08:21.000 --> 1:08:23.000
 of classical optimization approaches

1:08:23.000 --> 1:08:25.000
 as well as our network planner,

1:08:25.000 --> 1:08:28.000
 again trained on data from the customer fleet.

1:08:28.000 --> 1:08:31.000
 Now once we get a bunch of these tree trajectories,

1:08:31.000 --> 1:08:35.000
 we use them to start branching on the interactions.

1:08:35.000 --> 1:08:37.000
 We find the most critical interaction.

1:08:37.000 --> 1:08:39.000
 In our case, this would be the interaction

1:08:39.000 --> 1:08:41.000
 with respect to the pedestrian,

1:08:41.000 --> 1:08:43.000
 whether we assert in front of it or yield to it.

1:08:43.000 --> 1:08:47.000
 Obviously the option on the left is a high penalty option.

1:08:47.000 --> 1:08:49.000
 It likely won't get prioritized.

1:08:49.000 --> 1:08:51.000
 So we branch further onto the option on the right,

1:08:51.000 --> 1:08:54.000
 and that's where we bring in more and more complex interactions,

1:08:54.000 --> 1:08:57.000
 building this optimization problem incrementally

1:08:57.000 --> 1:08:59.000
 with more and more constraints.

1:08:59.000 --> 1:09:01.000
 And the tree search keeps flowing,

1:09:01.000 --> 1:09:04.000
 branching on more interactions, branching on more goals.

1:09:04.000 --> 1:09:07.000
 Now a lot of tricks here lie in evaluation

1:09:07.000 --> 1:09:10.000
 of each of this node of the tree search.

1:09:10.000 --> 1:09:12.000
 Inside each node,

1:09:12.000 --> 1:09:15.000
 initially we started with creating trajectories

1:09:15.000 --> 1:09:17.000
 using classical optimization approaches

1:09:17.000 --> 1:09:19.000
 where the constraints like I described

1:09:19.000 --> 1:09:21.000
 would be added incrementally.

1:09:21.000 --> 1:09:25.000
 And this would take close to 1 to 5 milliseconds per action.

1:09:25.000 --> 1:09:27.000
 Now even though this is a fairly good number,

1:09:27.000 --> 1:09:30.000
 when you want to evaluate more than 100 plus interactions,

1:09:30.000 --> 1:09:32.000
 this does not scale.

1:09:32.000 --> 1:09:35.000
 So we ended up building lightweight queryable networks

1:09:35.000 --> 1:09:38.000
 that you can run in the loop of the planner.

1:09:38.000 --> 1:09:41.000
 These networks are trained on human demonstrations from the fleet

1:09:41.000 --> 1:09:45.000
 as well as offline solvers with relaxed time limits.

1:09:45.000 --> 1:09:48.000
 With this, we were able to bring the run time down

1:09:48.000 --> 1:09:52.000
 to close to 100 microseconds per action.

1:09:52.000 --> 1:09:57.000
 Now doing this alone is not enough

1:09:57.000 --> 1:09:59.000
 because you still have this massive tree search

1:09:59.000 --> 1:10:01.000
 that you need to go through,

1:10:01.000 --> 1:10:04.000
 and you need to efficiently prune the search space.

1:10:04.000 --> 1:10:07.000
 So you need to do a new scoring on each of these trajectories.

1:10:07.000 --> 1:10:09.000
 Few of these are fairly standard.

1:10:09.000 --> 1:10:11.000
 You do a bunch of collision checks.

1:10:11.000 --> 1:10:13.000
 You do a bunch of comfort analysis.

1:10:13.000 --> 1:10:16.000
 What is the jerk and axel required for a given maneuver?

1:10:16.000 --> 1:10:19.000
 The customer fleet data plays an important role here

1:10:19.000 --> 1:10:22.000
 again.

1:10:22.000 --> 1:10:25.000
 We run two sets of, again, lightweight queryable networks,

1:10:25.000 --> 1:10:27.000
 both really augmenting each other,

1:10:27.000 --> 1:10:29.000
 one of them trained from interventions

1:10:29.000 --> 1:10:31.000
 from the FSD beta fleet,

1:10:31.000 --> 1:10:34.000
 which gives a score on how likely is a given maneuver

1:10:34.000 --> 1:10:37.000
 to result in interventions over the next few seconds,

1:10:37.000 --> 1:10:39.000
 and second, which is purely on human demonstrations,

1:10:39.000 --> 1:10:41.000
 human-driven data,

1:10:41.000 --> 1:10:44.000
 giving a score on how close is your given selected action

1:10:44.000 --> 1:10:47.000
 to a human-driven trajectory.

1:10:47.000 --> 1:10:50.000
 The scoring helps us prune the search space,

1:10:50.000 --> 1:10:53.000
 keep branching further on the interactions,

1:10:53.000 --> 1:10:56.000
 and focus the compute on the most promising outcomes.

1:10:56.000 --> 1:11:00.000
 The cool part about this architecture

1:11:00.000 --> 1:11:03.000
 is that it allows us to create a cool blend

1:11:03.000 --> 1:11:06.000
 between data-driven approaches

1:11:06.000 --> 1:11:09.000
 where you don't have to rely on a lot of hand-engineered costs,

1:11:09.000 --> 1:11:12.000
 but also ground it in reality with physics-based checks.

1:11:12.000 --> 1:11:15.000
 Now, a lot of what I described

1:11:15.000 --> 1:11:18.000
 was with respect to the agents we could observe in the scene,

1:11:18.000 --> 1:11:23.000
 but the same framework extends to objects behind occlusions.

1:11:23.000 --> 1:11:26.000
 We use the video feed from eight cameras

1:11:26.000 --> 1:11:29.000
 to generate the 3D occupancy of the world.

1:11:29.000 --> 1:11:34.000
 The blue mask here corresponds to the visibility region we call it.

1:11:34.000 --> 1:11:36.000
 It basically gets blocked

1:11:36.000 --> 1:11:38.000
 at the first occlusion you see in the scene.

1:11:38.000 --> 1:11:40.000
 We consume this visibility mask

1:11:40.000 --> 1:11:42.000
 to generate what we call as ghost objects,

1:11:42.000 --> 1:11:45.000
 which you can see on the top left.

1:11:45.000 --> 1:11:47.000
 Now, if you model the spawn regions

1:11:47.000 --> 1:11:50.000
 and the state transitions of these ghost objects correctly,

1:11:50.000 --> 1:11:53.000
 if you tune your control response

1:11:53.000 --> 1:11:56.000
 as a function of that existence likelihood,

1:11:56.000 --> 1:11:59.000
 you can extract some really nice human-like behaviors.

1:11:59.000 --> 1:12:01.000
 Now I'll pass it on to Phil

1:12:01.000 --> 1:12:04.000
 to describe more on how we generate these occupancy networks.

1:12:04.000 --> 1:12:06.000
 Thank you.

1:12:06.000 --> 1:12:12.000
 applause

1:12:12.000 --> 1:12:14.000
 Hey guys, my name is Phil.

1:12:14.000 --> 1:12:17.000
 I will share the details of the occupancy network

1:12:17.000 --> 1:12:19.000
 we built over the past year.

1:12:19.000 --> 1:12:22.000
 This network is our solution to model the physical world

1:12:22.000 --> 1:12:24.000
 in 3D around our cars,

1:12:24.000 --> 1:12:28.000
 and it is currently not shown in our customer-facing visualization.

1:12:28.000 --> 1:12:30.000
 What you will see here

1:12:30.000 --> 1:12:36.000
 is the visual network output from our internal dev tool.

1:12:36.000 --> 1:12:38.000
 The occupancy network takes video streams

1:12:38.000 --> 1:12:41.000
 of all our eight cameras as input,

1:12:41.000 --> 1:12:44.000
 produces a single unified volumetric occupancy

1:12:44.000 --> 1:12:46.000
 in vector space directly.

1:12:46.000 --> 1:12:49.000
 For every 3D location around our car,

1:12:49.000 --> 1:12:54.000
 it predicts the probability of that location being occupied or not.

1:12:54.000 --> 1:12:56.000
 Since it has video context,

1:12:56.000 --> 1:12:59.000
 it is capable of predicting obstacles

1:12:59.000 --> 1:13:03.000
 that are occluded instantaneously.

1:13:03.000 --> 1:13:07.000
 For each location, it also produces a set of semantics

1:13:07.000 --> 1:13:11.000
 such as a curb, car, pedestrian, and road debris,

1:13:11.000 --> 1:13:16.000
 as color-coded here.

1:13:16.000 --> 1:13:20.000
 Occupancy flow is also predicted for motion.

1:13:20.000 --> 1:13:22.000
 Since the model is a generalized network,

1:13:22.000 --> 1:13:26.000
 it does not tell static and dynamic objects explicitly.

1:13:26.000 --> 1:13:30.000
 It is able to produce and model the random motion

1:13:30.000 --> 1:13:34.000
 such as a swerving trainer here.

1:13:34.000 --> 1:13:38.000
 This network is currently running in all Teslas with FSD computers,

1:13:38.000 --> 1:13:40.000
 and it is incredibly efficient.

1:13:40.000 --> 1:13:42.000
 It runs about every 10 milliseconds

1:13:42.000 --> 1:13:45.000
 with our neural net accelerator.

1:13:45.000 --> 1:13:47.000
 So how does this work?

1:13:47.000 --> 1:13:49.000
 Let's take a look at the architecture.

1:13:49.000 --> 1:13:54.000
 First, we rectify each camera images with the camera calibration.

1:13:54.000 --> 1:13:56.000
 And the images we're showing here,

1:13:56.000 --> 1:13:57.000
 we're giving to the network,

1:13:57.000 --> 1:14:00.000
 is actually not the typical 8-bit RGB image.

1:14:00.000 --> 1:14:03.000
 As you can see from the first image on top,

1:14:03.000 --> 1:14:07.000
 we're giving the 12-bit raw photo account image to the network.

1:14:07.000 --> 1:14:10.000
 Since it has 4 bits more information,

1:14:10.000 --> 1:14:13.000
 it has 16 times better dynamic range,

1:14:13.000 --> 1:14:15.000
 as well as reduced latency,

1:14:15.000 --> 1:14:18.000
 since we don't have to run ISP in the loop anymore.

1:14:18.000 --> 1:14:21.000
 We use a set of reglets and a value of FPS

1:14:21.000 --> 1:14:25.000
 as a backbone to extract image space features.

1:14:25.000 --> 1:14:29.000
 Next, we construct a set of 3D position query

1:14:29.000 --> 1:14:31.000
 along with the image space features

1:14:31.000 --> 1:14:35.000
 as keys and values fit into an attention module.

1:14:35.000 --> 1:14:37.000
 The output of the attention module

1:14:37.000 --> 1:14:40.000
 is high-dimensional spatial features.

1:14:40.000 --> 1:14:44.000
 These spatial features are aligned temporally

1:14:44.000 --> 1:14:49.000
 using vehicle odometry to derive motion.

1:14:49.000 --> 1:14:52.000
 Next, these spatial temporal features

1:14:52.000 --> 1:14:54.000
 go through a set of deconvolutions

1:14:54.000 --> 1:14:57.000
 to produce the final occupancy and occupancy flow output.

1:14:57.000 --> 1:15:00.000
 They're formed as fixed-size voxel grid,

1:15:00.000 --> 1:15:04.000
 which might not be precise enough for planning and control.

1:15:04.000 --> 1:15:06.000
 In order to get a higher resolution,

1:15:06.000 --> 1:15:09.000
 we also produce per-voxel feature maps,

1:15:09.000 --> 1:15:13.000
 which we feed into MLP with 3D spatial point queries

1:15:13.000 --> 1:15:18.000
 to get position and semantics at any arbitrary location.

1:15:18.000 --> 1:15:20.000
 After knowing the model better,

1:15:20.000 --> 1:15:22.000
 let's take a look at another example.

1:15:22.000 --> 1:15:26.000
 Here we have an articulated bus parked on the right side of the road,

1:15:26.000 --> 1:15:29.000
 highlighted as an L-shaped voxel here.

1:15:29.000 --> 1:15:32.000
 As we approach, the bus starts to move.

1:15:32.000 --> 1:15:35.000
 The front of the car turns blue first,

1:15:35.000 --> 1:15:38.000
 indicating the model predicts the front of the bus

1:15:38.000 --> 1:15:41.000
 has a non-zero occupancy flow.

1:15:41.000 --> 1:15:46.000
 And as the bus keeps moving, the entire bus turns blue,

1:15:46.000 --> 1:15:49.000
 and you can also see that the network predicts

1:15:49.000 --> 1:15:52.000
 the precise curvature of the bus.

1:15:52.000 --> 1:15:55.000
 Well, this is a very complicated problem

1:15:55.000 --> 1:15:57.000
 for a traditional object detection network,

1:15:57.000 --> 1:16:00.000
 as you have to see whether I'm going to use one cuboid

1:16:00.000 --> 1:16:03.000
 or perhaps two to fit the curvature.

1:16:03.000 --> 1:16:05.000
 But for occupancy networks,

1:16:05.000 --> 1:16:08.000
 all we care about is the occupancy in the visible space,

1:16:08.000 --> 1:16:13.000
 and we'll be able to model the curvature precisely.

1:16:13.000 --> 1:16:16.000
 Besides the voxel grid, the occupancy network

1:16:16.000 --> 1:16:19.000
 also produces a drivable surface.

1:16:19.000 --> 1:16:22.000
 The drivable surface has both 3D geometry and semantics.

1:16:22.000 --> 1:16:24.000
 They are very useful for control,

1:16:24.000 --> 1:16:28.000
 especially on hilly and curvy roads.

1:16:28.000 --> 1:16:32.000
 The surface and the voxel grid are not predicted independently.

1:16:32.000 --> 1:16:38.000
 Instead, the voxel grid actually aligns with the surface implicitly.

1:16:38.000 --> 1:16:42.000
 Here we are at a here quest where you can see

1:16:42.000 --> 1:16:47.000
 the 3D geometry of the surface being predicted nicely.

1:16:47.000 --> 1:16:49.000
 Planner can use this information to decide

1:16:49.000 --> 1:16:52.000
 perhaps when to slow down more for the hill crest.

1:16:52.000 --> 1:16:55.000
 And as you can also see, the voxel grid aligns

1:16:55.000 --> 1:16:59.000
 with the surface consistently.

1:16:59.000 --> 1:17:01.000
 Besides the voxels and the surface,

1:17:01.000 --> 1:17:04.000
 we're also very excited about the recent breakthrough

1:17:04.000 --> 1:17:07.000
 in neural radiance field, or LERF.

1:17:07.000 --> 1:17:10.000
 We're looking into both incorporating

1:17:10.000 --> 1:17:13.000
 personalized LERF features into occupancy network training,

1:17:13.000 --> 1:17:19.000
 as well as using our network output as the input state for LERF.

1:17:19.000 --> 1:17:22.000
 As a matter of fact, Ashok is very excited about this.

1:17:22.000 --> 1:17:29.000
 This has been his personal weekend project for a while.

1:17:29.000 --> 1:17:32.000
 About these LERFs, because I think the academia

1:17:32.000 --> 1:17:36.000
 is building a lot of these foundation models for language,

1:17:36.000 --> 1:17:39.000
 using tons of large data sets for language.

1:17:39.000 --> 1:17:42.000
 Our vision nerves are going to provide the foundation models

1:17:42.000 --> 1:17:46.000
 for computer vision because they are grounded in geometry,

1:17:46.000 --> 1:17:49.000
 and geometry gives us a nice way to supervise these networks

1:17:49.000 --> 1:17:52.000
 and frees us off the requirement for different ontology.

1:17:52.000 --> 1:17:54.000
 And the supervision is essentially free

1:17:54.000 --> 1:17:57.000
 because you just have to differentially render these images.

1:17:57.000 --> 1:18:00.000
 So I think in the future, this occupancy network idea

1:18:00.000 --> 1:18:03.000
 where images come in and then the network produces

1:18:03.000 --> 1:18:07.000
 a consistent volumetric representation of the scene

1:18:07.000 --> 1:18:11.000
 that can then be differentially rendered into any image that was observed,

1:18:11.000 --> 1:18:15.000
 I personally think is the future of computer vision.

1:18:15.000 --> 1:18:18.000
 And we do some initial work on it right now,

1:18:18.000 --> 1:18:22.000
 but I think in the future, both at Tesla and in academia,

1:18:22.000 --> 1:18:26.000
 we will see that this combination of one-shot prediction

1:18:26.000 --> 1:18:29.000
 of volumetric occupancy will be the future.

1:18:29.000 --> 1:18:32.000
 That's my personal bet.

1:18:32.000 --> 1:18:34.000
 Thanks, Ashok.

1:18:34.000 --> 1:18:39.000
 So here's an example early result of a 3D reconstruction from our fleet data.

1:18:39.000 --> 1:18:44.000
 Instead of focusing on getting perfect RGB reprojection in imaging space,

1:18:44.000 --> 1:18:49.000
 our primary goal here is to accurately represent the world in 3D space for driving.

1:18:49.000 --> 1:18:52.000
 And we want to do this for all our fleet data over the world

1:18:52.000 --> 1:18:55.000
 in all weather and lighting conditions.

1:18:55.000 --> 1:18:57.000
 And obviously, this is a very challenging problem,

1:18:57.000 --> 1:19:00.000
 and we're looking for you guys to help.

1:19:00.000 --> 1:19:04.000
 Finally, the occupancy network is trained with large auto-labeled data set

1:19:04.000 --> 1:19:07.000
 without any human in the loop.

1:19:07.000 --> 1:19:12.000
 And with that, I'll pass to Tim to talk about what it takes to train this network.

1:19:12.000 --> 1:19:14.000
 Thanks, Phil.

1:19:18.000 --> 1:19:20.000
 All right. Hey, everyone.

1:19:20.000 --> 1:19:24.000
 Let's talk about some training infrastructure.

1:19:24.000 --> 1:19:27.000
 So we've seen a couple videos, you know, four or five.

1:19:27.000 --> 1:19:32.000
 I think and care more and worry more about a lot more clips on that.

1:19:32.000 --> 1:19:36.000
 So we've been looking at the occupancy networks just from Phil.

1:19:36.000 --> 1:19:42.000
 Just Phil's videos, it takes 1.4 billion frames to train that network,

1:19:42.000 --> 1:19:44.000
 what you just saw.

1:19:44.000 --> 1:19:48.000
 And if you have 100,000 GPUs, it would take one hour.

1:19:48.000 --> 1:19:52.000
 But if you have one GPU, it would take 100,000 hours.

1:19:52.000 --> 1:19:56.000
 So that is not a humane time period that you can wait for your training job to run, right?

1:19:56.000 --> 1:19:58.000
 We want to ship faster than that.

1:19:58.000 --> 1:20:01.000
 So that means you're going to need to go parallel.

1:20:01.000 --> 1:20:03.000
 So you need more compute for that.

1:20:03.000 --> 1:20:06.000
 That means you're going to need a supercomputer.

1:20:06.000 --> 1:20:11.000
 So this is why we've built in-house three supercomputers comprising of 14,000 GPUs

1:20:11.000 --> 1:20:18.000
 where we use 10,000 GPUs for training and run 4,000 GPUs for auto-labeling.

1:20:18.000 --> 1:20:24.000
 All these videos are stored in 30 petabytes of a distributed managed video cache.

1:20:24.000 --> 1:20:28.000
 You shouldn't think of our data sets as fixed, let's say,

1:20:28.000 --> 1:20:32.000
 as you think of your ImageNet or something, you know, with like a million frames.

1:20:32.000 --> 1:20:34.000
 You should think of it as a very fluid thing.

1:20:34.000 --> 1:20:42.000
 So we've got half a million of these videos flowing in and out of these clusters every single day.

1:20:42.000 --> 1:20:49.000
 And we track 400,000 of these kind of Python video instantiations every second.

1:20:49.000 --> 1:20:51.000
 So that's a lot of calls.

1:20:51.000 --> 1:20:57.000
 We're going to need to capture that in order to govern the retention policies of this distributed video cache.

1:20:57.000 --> 1:21:04.000
 So underlying all of this is a huge amount of infra, all of which we build and manage in-house.

1:21:04.000 --> 1:21:10.000
 So you cannot just buy, you know, 14,000 GPUs and then 30 petabytes of Flash NVMe

1:21:10.000 --> 1:21:13.000
 and just put it together and let it go train.

1:21:13.000 --> 1:21:17.000
 It actually takes a lot of work, and I'm going to go into a little bit of that.

1:21:17.000 --> 1:21:21.000
 What you actually typically want to do is you want to take your accelerator,

1:21:21.000 --> 1:21:25.000
 so that could be the GPU or Dojo, which we'll talk about later,

1:21:25.000 --> 1:21:31.000
 and because that's the most expensive component, that's where you want to put your bottleneck.

1:21:31.000 --> 1:21:37.000
 And so that means that every single part of your system is going to need to outperform this accelerator.

1:21:37.000 --> 1:21:39.000
 And so that is really complicated.

1:21:39.000 --> 1:21:46.000
 That means that your storage is going to need to have the size and the bandwidth to deliver all the data down into the nodes.

1:21:46.000 --> 1:21:53.000
 These nodes need to have the right amount of CPU and memory capabilities to feed into your machine learning framework.

1:21:53.000 --> 1:21:58.000
 This machine learning framework then needs to hand it off to your GPU, and then you can start training.

1:21:58.000 --> 1:22:03.000
 But then you need to do so across hundreds or thousands of GPU in a reliable way,

1:22:03.000 --> 1:22:08.000
 in lockstep, and in a way that's also fast, so you're also going to need an interconnect.

1:22:08.000 --> 1:22:10.000
 Extremely complicated.

1:22:10.000 --> 1:22:13.000
 We'll talk more about Dojo in a second.

1:22:13.000 --> 1:22:18.000
 So first I want to take you through some optimizations that we've done on our cluster.

1:22:18.000 --> 1:22:25.000
 So we're getting in a lot of videos, and video is very much unlike, let's say, training on images or text,

1:22:25.000 --> 1:22:27.000
 which I think is very well established.

1:22:27.000 --> 1:22:31.000
 Video is quite literally a dimension more complicated.

1:22:31.000 --> 1:22:37.000
 And so that's why we needed to go end-to-end from the storage layer down to the accelerator

1:22:37.000 --> 1:22:39.000
 and optimize every single piece of that,

1:22:39.000 --> 1:22:44.000
 because we train on the photon count videos that come directly from our fleet.

1:22:44.000 --> 1:22:48.000
 We train on those directly. We do not post-process those at all.

1:22:48.000 --> 1:22:53.000
 The way it's just done is we seek exactly to the frames we select for our batch.

1:22:53.000 --> 1:22:58.000
 We load those in, including the frames that they depend on, so these are your iframes or your keyframes.

1:22:58.000 --> 1:23:03.000
 We package those up, move them into shared memory, move them into a double buffer on the GPU,

1:23:03.000 --> 1:23:09.000
 and then use the hardware decoder that's only accelerated to actually decode the video.

1:23:09.000 --> 1:23:15.000
 So we do that on the GPU natively, and this is all in a very nice PyTorch extension.

1:23:15.000 --> 1:23:20.000
 Doing so unlocks more than 30% training speed increase for the occupancy networks

1:23:20.000 --> 1:23:26.000
 and frees up basically the whole CPU to do any other thing.

1:23:26.000 --> 1:23:31.000
 You cannot just do training with just videos. Of course, you need some kind of a ground truth.

1:23:31.000 --> 1:23:34.000
 That is actually an interesting problem as well.

1:23:34.000 --> 1:23:39.000
 The objective for storing your ground truth is that you want to make sure you get to your ground truth

1:23:39.000 --> 1:23:43.000
 that you need in the minimal amount of file system operations

1:23:43.000 --> 1:23:49.000
 and load in the minimal size of what you need in order to optimize for aggregate cross-cluster throughput,

1:23:49.000 --> 1:23:56.000
 because you should see a compute cluster as one big device which has internally fixed constraints and thresholds.

1:23:56.000 --> 1:24:02.000
 So for this we rolled out a format that is native to us that's called Small.

1:24:02.000 --> 1:24:06.000
 We use this for our ground truth, our feature cache, and any inference outputs.

1:24:06.000 --> 1:24:08.000
 So a lot of tensors that are in there.

1:24:08.000 --> 1:24:13.000
 And so just a cartoon here. Let's say this is your table that you want to store.

1:24:13.000 --> 1:24:16.000
 Then that's how that would look out if you rolled out on disk.

1:24:16.000 --> 1:24:21.000
 So what you do is you take anything you'd want to index on, so for example video timestamps,

1:24:21.000 --> 1:24:26.000
 and you put those all in the header so that in your initial header read you know exactly where to go on disk.

1:24:26.000 --> 1:24:31.000
 Then if you have any tensors, you're going to try to transpose the dimensions

1:24:31.000 --> 1:24:34.000
 to put a different dimension last as the contiguous dimension,

1:24:34.000 --> 1:24:37.000
 and then also try different types of compression.

1:24:37.000 --> 1:24:41.000
 Then you check out which one was most optimal and then store that one.

1:24:41.000 --> 1:24:44.000
 This is actually a huge step if you do feature caching.

1:24:44.000 --> 1:24:47.000
 Unintelligible output from the machine learning network.

1:24:47.000 --> 1:24:53.000
 If you rotate around the dimensions a little bit, you can get up to 20% increase in efficiency of storage.

1:24:53.000 --> 1:24:59.000
 Then when you store that, we also order the columns by size

1:24:59.000 --> 1:25:02.000
 so that all your small columns and small values are together

1:25:02.000 --> 1:25:07.000
 so that when you seek for a single value, you're likely to overlap with the read on more values

1:25:07.000 --> 1:25:12.000
 which you'll use later so that you don't need to do another file system operation.

1:25:12.000 --> 1:25:14.000
 So I could go on and on.

1:25:14.000 --> 1:25:18.000
 I just touched on two projects that we have internally,

1:25:18.000 --> 1:25:24.000
 but this is actually part of a huge continuous effort to optimize the compute that we have in-house.

1:25:24.000 --> 1:25:27.000
 So accumulating and aggregating through all these optimizations,

1:25:27.000 --> 1:25:32.000
 we now train our occupancy networks twice as fast just because it's twice as efficient.

1:25:32.000 --> 1:25:39.000
 Now if we add in a bunch more compute and go parallel, we can now train this in hours instead of days.

1:25:39.000 --> 1:25:50.000
 And with that, I'd like to hand it off to the biggest user of compute, John.

1:25:50.000 --> 1:25:55.000
 Hi everybody. My name is John Emmons. I lead the Autopilot vision team.

1:25:55.000 --> 1:25:57.000
 I'm going to cover two topics with you today.

1:25:57.000 --> 1:25:59.000
 The first is how we predict lanes,

1:25:59.000 --> 1:26:04.000
 and the second is how we predict the future behavior of other agents on the road.

1:26:04.000 --> 1:26:06.000
 In the early days of Autopilot,

1:26:06.000 --> 1:26:11.000
 we modeled the lane detection problem as an image space instant segmentation task.

1:26:11.000 --> 1:26:13.000
 Our network was super simple though.

1:26:13.000 --> 1:26:18.000
 In fact, it was only capable of predicting lanes of a few different kinds of geometries.

1:26:18.000 --> 1:26:23.000
 Specifically, it would segment the ego lane, it could segment adjacent lanes,

1:26:23.000 --> 1:26:26.000
 and then it has some special casing for forks and merges.

1:26:26.000 --> 1:26:31.000
 This simplistic modeling of the problem worked for highly structured roads like highways.

1:26:31.000 --> 1:26:35.000
 But today we're trying to build a system that's capable of much more complex maneuvers.

1:26:35.000 --> 1:26:38.000
 Specifically, we want to make left and right turns at intersections,

1:26:38.000 --> 1:26:41.000
 where the road topology can be quite a bit more complex and diverse.

1:26:41.000 --> 1:26:47.000
 When we try to apply this simplistic modeling of the problem here, it just totally breaks down.

1:26:47.000 --> 1:26:49.000
 Taking a step back for a moment,

1:26:49.000 --> 1:26:54.000
 what we're trying to do here is to predict the sparse set of lane instances and their connectivity.

1:26:54.000 --> 1:26:58.000
 And what we want to do is to have a neural network that basically predicts this graph,

1:26:58.000 --> 1:27:05.000
 where the nodes are the lane segments and the edges encode the connectivities between these lanes.

1:27:05.000 --> 1:27:08.000
 So what we have is our lane detection neural network.

1:27:08.000 --> 1:27:11.000
 It's made up of three components.

1:27:11.000 --> 1:27:16.000
 In the first component, we have a set of convolutional layers, attention layers, and other neural network layers

1:27:16.000 --> 1:27:19.000
 that encode the video streams from our eight cameras on the vehicle

1:27:19.000 --> 1:27:23.000
 and produce a rich visual representation.

1:27:23.000 --> 1:27:29.000
 We then enhance this visual representation with a coarse road level map data,

1:27:29.000 --> 1:27:35.000
 which we encode with a set of additional neural network layers that we call the lane guidance module.

1:27:35.000 --> 1:27:40.000
 This map is not an HD map, but it provides a lot of useful hints about the topology of lanes inside of intersections,

1:27:40.000 --> 1:27:46.000
 the lane counts on various roads, and a set of other attributes that help us.

1:27:46.000 --> 1:27:51.000
 The first two components here produce a dense tensor that sort of encodes the world.

1:27:51.000 --> 1:27:58.000
 But what we really want to do is to convert this dense tensor into a smart set of lanes and their connectivities.

1:27:58.000 --> 1:28:01.000
 We approach this problem like an image captioning task,

1:28:01.000 --> 1:28:06.000
 where the input is this dense tensor and the output text is predicted into a special language

1:28:06.000 --> 1:28:10.000
 that we developed at Tesla for encoding lanes and their connectivities.

1:28:10.000 --> 1:28:14.000
 In this language of lanes, the words and tokens are the lane positions in 3D space.

1:28:14.000 --> 1:28:21.000
 In the ordering of the tokens, inferred modifiers in the tokens encode the connected relationships between these lanes.

1:28:21.000 --> 1:28:26.000
 By modeling the task as a language problem, we can capitalize on recent autoregressive architectures

1:28:26.000 --> 1:28:31.000
 and techniques from the language community for handling the multi-modality of the problem.

1:28:31.000 --> 1:28:33.000
 We're not just solving the computer vision problem at Autopilot,

1:28:33.000 --> 1:28:38.000
 we're also applying the state-of-the-art in language modeling and machine learning more generally.

1:28:38.000 --> 1:28:43.000
 I'm now going to dive into a little bit more detail of this language component.

1:28:43.000 --> 1:28:49.000
 What I have depicted on the screen here is a satellite image which sort of represents the local area around the vehicle.

1:28:49.000 --> 1:28:52.000
 The set of nodes and edges is what we refer to as the lane graph,

1:28:52.000 --> 1:28:56.000
 and it's ultimately what we want to come out of this neural network.

1:28:56.000 --> 1:28:59.000
 We start with a blank slate.

1:28:59.000 --> 1:29:03.000
 We're going to want to make our first prediction here at this green dot.

1:29:03.000 --> 1:29:09.000
 This green dot's position is encoded as an index into a coarse grid which discretizes the 3D world.

1:29:09.000 --> 1:29:13.000
 Now we don't predict this index directly because it would be too computationally expensive to do so.

1:29:13.000 --> 1:29:17.000
 There's just too many grid points, and predicting a categorical distribution over this

1:29:17.000 --> 1:29:20.000
 has both implications at training time and test time.

1:29:20.000 --> 1:29:23.000
 So instead what we do is we discretize the world coarsely first.

1:29:23.000 --> 1:29:29.000
 We predict a heat map over the possible locations, and then we latch in the most probable location.

1:29:29.000 --> 1:29:35.000
 Conditioned on this, we then refine the prediction and get the precise point.

1:29:35.000 --> 1:29:38.000
 Now we know where the position of this token is, but we don't know its type.

1:29:38.000 --> 1:29:44.000
 In this case, though, it's the beginning of a new lane, so we predict it as a start token.

1:29:44.000 --> 1:29:49.000
 And because it's a start token, there's no additional attributes in our language.

1:29:49.000 --> 1:29:51.000
 We then take the predictions from this first forward pass,

1:29:51.000 --> 1:29:54.000
 and we encode them using a learned positional embedding,

1:29:54.000 --> 1:29:58.000
 which produces a set of tensors that we combine together,

1:29:58.000 --> 1:30:01.000
 which is actually the first word in our language of lanes.

1:30:01.000 --> 1:30:04.000
 We add this to the first position in our sentence here.

1:30:04.000 --> 1:30:10.000
 We then continue this process by predicting the next lane point in a similar fashion.

1:30:10.000 --> 1:30:12.000
 Now this lane point is not the beginning of a new lane.

1:30:12.000 --> 1:30:15.000
 It's actually a continuation of the previous lane.

1:30:15.000 --> 1:30:19.000
 So it's a continuation token type.

1:30:19.000 --> 1:30:23.000
 Now it's not enough just to know that this lane is connected to the previously predicted lane.

1:30:23.000 --> 1:30:29.000
 We want to encode its precise geometry, which we do by regressing a set of spline coefficients.

1:30:29.000 --> 1:30:35.000
 We then take this lane, we encode it again, and add it as the next word in the sentence.

1:30:35.000 --> 1:30:40.000
 We continue predicting these continuation lanes until we get to the end of the prediction grid.

1:30:40.000 --> 1:30:43.000
 We then move on to a different lane segment.

1:30:43.000 --> 1:30:45.000
 So you can see that cyan dot there.

1:30:45.000 --> 1:30:47.000
 Now it's not topologically connected to that pink point.

1:30:47.000 --> 1:30:53.000
 It's actually forking off of that blue, sorry, that green point there.

1:30:53.000 --> 1:30:55.000
 So it's got a fork type.

1:30:55.000 --> 1:31:01.000
 Fork tokens actually point back to previous tokens from which the fork originates.

1:31:01.000 --> 1:31:04.000
 So you can see here the fork point predictor is actually the index zero.

1:31:04.000 --> 1:31:10.000
 So it's actually referencing back to a token that is already predicted, like you would in language.

1:31:10.000 --> 1:31:15.000
 We continue this process over and over again until we've enumerated all of the tokens in the lane graph.

1:31:15.000 --> 1:31:19.000
 And then the network predicts the end of sentence token.

1:31:19.000 --> 1:31:24.000
 Yeah, I just wanted to note that the reason we do this is not just because we want to build something complicated.

1:31:24.000 --> 1:31:27.000
 It almost feels like a Turing complete machine here with neural networks though.

1:31:27.000 --> 1:31:29.000
 It's that we tried simple approaches.

1:31:29.000 --> 1:31:34.000
 For example, trying to just segment the lanes along the road or something like that.

1:31:34.000 --> 1:31:38.000
 But then the problem is when there's uncertainty, say you cannot see the road clearly,

1:31:38.000 --> 1:31:41.000
 and there could be two lanes or three lanes and you can't tell,

1:31:41.000 --> 1:31:44.000
 a simple segmentation based approach would just draw both of them.

1:31:44.000 --> 1:31:47.000
 It's kind of a 2.5 lane situation.

1:31:47.000 --> 1:31:51.000
 And the post-processing algorithm would hilariously fail when the predictions are such.

1:31:51.000 --> 1:31:53.000
 Yeah, and the problems don't end there.

1:31:53.000 --> 1:31:57.000
 I mean, you need to predict these connective lanes inside of intersections,

1:31:57.000 --> 1:32:00.000
 which it's just not possible with the approach that Ashok's mentioning,

1:32:00.000 --> 1:32:02.000
 which is why we had to upgrade to this sort of approach.

1:32:02.000 --> 1:32:05.000
 Yeah, when it overlaps like this, segmentation would just go haywire.

1:32:05.000 --> 1:32:09.000
 But even if you try very hard to put them on separate layers, it's just a really hard problem.

1:32:09.000 --> 1:32:15.000
 But language just offers a really nice framework for getting a sample from a posterior

1:32:15.000 --> 1:32:19.000
 as opposed to trying to do all of this in post-processing.

1:32:19.000 --> 1:32:22.000
 But this doesn't actually stop for just autopilot, right, John?

1:32:22.000 --> 1:32:24.000
 It can be used for optimists.

1:32:24.000 --> 1:32:26.000
 Yeah, you know, I guess they wouldn't be called lanes,

1:32:26.000 --> 1:32:30.000
 but you could imagine sort of in this stage here that you might have sort of paths

1:32:30.000 --> 1:32:34.000
 that sort of encode the possible places that people could walk.

1:32:34.000 --> 1:32:38.000
 Yeah, basically, if you're in a factory or in a home setting,

1:32:38.000 --> 1:32:41.000
 you can just ask the robot, okay, please route to the kitchen

1:32:41.000 --> 1:32:44.000
 or please route to some location in the factory,

1:32:44.000 --> 1:32:47.000
 and then we predict a set of pathways that would go through the aisles,

1:32:47.000 --> 1:32:50.000
 take the robot, and say, okay, this is how you get to the kitchen.

1:32:50.000 --> 1:32:53.000
 It just really gives us a nice framework to model these different paths

1:32:53.000 --> 1:33:00.000
 that simplify the navigation problem for the downstream planner.

1:33:00.000 --> 1:33:03.000
 All right, so ultimately what we get from this lane detection network

1:33:03.000 --> 1:33:07.000
 is a set of lanes and their conductivities, which comes directly from the network.

1:33:07.000 --> 1:33:13.000
 There's no additional step here for sparsifying these dense predictions into sparse ones.

1:33:13.000 --> 1:33:18.000
 This is just a direct unfiltered output of the network.

1:33:18.000 --> 1:33:20.000
 Okay, so I talked a little bit about lanes.

1:33:20.000 --> 1:33:24.000
 I'm going to briefly touch on how we model and predict the future paths

1:33:24.000 --> 1:33:27.000
 and other semantics on objects.

1:33:27.000 --> 1:33:29.000
 So I'm just going to go really quickly through two examples.

1:33:29.000 --> 1:33:32.000
 The video on the right here, we've got a car that's actually running a red light

1:33:32.000 --> 1:33:34.000
 and turning in front of us.

1:33:34.000 --> 1:33:38.000
 What we do to handle situations like this is we predict a set of short time horizons

1:33:38.000 --> 1:33:41.000
 and future trajectories on all objects.

1:33:41.000 --> 1:33:44.000
 We can use these to anticipate the dangerous situation here

1:33:44.000 --> 1:33:48.000
 and apply whatever braking and steering actions required to avoid a collision.

1:33:48.000 --> 1:33:51.000
 In the video on the right, there's two vehicles in front of us.

1:33:51.000 --> 1:33:54.000
 The one on the left lane is parked.

1:33:54.000 --> 1:33:56.000
 Apparently it's being loaded, unloaded.

1:33:56.000 --> 1:33:58.000
 I don't know why the driver decided to park there.

1:33:58.000 --> 1:34:01.000
 But the important thing is that our neural network predicted that it was stopped,

1:34:01.000 --> 1:34:03.000
 which is the red color there.

1:34:03.000 --> 1:34:06.000
 The vehicle on the other lane, as you notice, also is stationary.

1:34:06.000 --> 1:34:09.000
 But that one's obviously just waiting for that red light to turn green.

1:34:09.000 --> 1:34:12.000
 So even though both objects are stationary and have zero velocity,

1:34:12.000 --> 1:34:14.000
 the semantics of that is really important here

1:34:14.000 --> 1:34:19.000
 so that we don't get stuck behind that awkwardly parked car.

1:34:19.000 --> 1:34:22.000
 Predicting all of these agent attributes presents some practical problems

1:34:22.000 --> 1:34:24.000
 when trying to build a real-time system.

1:34:24.000 --> 1:34:27.000
 We need to maximize the frame rate of our object section stack

1:34:27.000 --> 1:34:30.000
 so that Autopilot can quickly react to the changing environment.

1:34:30.000 --> 1:34:33.000
 Every millisecond really matters here.

1:34:33.000 --> 1:34:37.000
 To minimize the inference latency, our neural network is split into two phases.

1:34:37.000 --> 1:34:42.000
 In the first phase, we identify the locations in 3D space where agents exist.

1:34:42.000 --> 1:34:46.000
 In the second stage, we then pull out tensors at those 3D locations,

1:34:46.000 --> 1:34:49.000
 append it with additional data that's on the vehicle,

1:34:49.000 --> 1:34:52.000
 and then we do the rest of the processing.

1:34:52.000 --> 1:34:55.000
 This sparsification step allows the neural network to focus compute

1:34:55.000 --> 1:34:57.000
 on the areas that matter most,

1:34:57.000 --> 1:35:01.000
 which gives us superior performance for a fraction of the latency cost.

1:35:01.000 --> 1:35:04.000
 So putting it all together, the Autopilot vision stack predicts

1:35:04.000 --> 1:35:06.000
 more than just the geometry and kinematics of the world.

1:35:06.000 --> 1:35:11.000
 It also predicts a rich set of semantics which enable safe and human-like driving.

1:35:11.000 --> 1:35:13.000
 I'm now going to hand things off to Shree, who will tell us how we run

1:35:13.000 --> 1:35:15.000
 all these cool neural networks on our FSD computer.

1:35:15.000 --> 1:35:17.000
 Thank you.

1:35:17.000 --> 1:35:23.000
 Applause

1:35:23.000 --> 1:35:26.000
 Hi, everyone. I'm Shree.

1:35:26.000 --> 1:35:30.000
 Today I'm going to give a glimpse of what it takes to run these FSD networks in the car

1:35:30.000 --> 1:35:34.000
 and how do we optimize for the inference latency.

1:35:34.000 --> 1:35:41.000
 Today I'm going to focus just on the FSD lanes network that John just talked about.

1:35:41.000 --> 1:35:46.000
 So when we started this track, we wanted to know if we can run this FSD lanes network

1:35:46.000 --> 1:35:51.000
 natively on the trip engine, which is our in-house neural network accelerator

1:35:51.000 --> 1:35:53.000
 that we built in the FSD computer.

1:35:53.000 --> 1:35:56.000
 When we built this hardware, we kept it simple,

1:35:56.000 --> 1:36:02.000
 and we made sure it can do one thing ridiculously fast, dense dot products.

1:36:02.000 --> 1:36:07.000
 But this architecture is autoregressive and iterative,

1:36:07.000 --> 1:36:11.000
 where it crunches through multiple attention blocks in the inner loop,

1:36:11.000 --> 1:36:14.000
 producing sparse points directly at every step.

1:36:14.000 --> 1:36:18.000
 So the challenge here was, how can we do this sparse point prediction

1:36:18.000 --> 1:36:22.000
 and sparse computation on a dense dot product engine?

1:36:22.000 --> 1:36:25.000
 Let's see how we did this on the trip.

1:36:25.000 --> 1:36:32.000
 So the network predicts the heat map of most probable spatial locations of the point.

1:36:32.000 --> 1:36:36.000
 Now we do an argmax and a one-hot operation,

1:36:36.000 --> 1:36:41.000
 which gives the one-hot encoding of the index of the spatial location.

1:36:41.000 --> 1:36:45.000
 Now we need to select the embedding associated with this index

1:36:45.000 --> 1:36:49.000
 from an embedding table that is learned during training.

1:36:49.000 --> 1:36:53.000
 To do this on trip, we actually built a lookup table in SRAM,

1:36:53.000 --> 1:36:57.000
 and we engineered the dimensions of this embedding such that

1:36:57.000 --> 1:37:02.000
 we could achieve all of this thing with just matrix multiplication.

1:37:02.000 --> 1:37:07.000
 Not just that, we also wanted to store this embedding into a token cache

1:37:07.000 --> 1:37:10.000
 so that we don't recompute this for every iteration,

1:37:10.000 --> 1:37:13.000
 rather reuse it for future point prediction.

1:37:13.000 --> 1:37:17.000
 Again, we pulled some tricks here where we did all these operations

1:37:17.000 --> 1:37:19.000
 just on the dot product engine.

1:37:19.000 --> 1:37:22.000
 It's actually cool that our team found creative ways

1:37:22.000 --> 1:37:25.000
 to map all these operations on the trip engine

1:37:25.000 --> 1:37:31.000
 in ways that were not even imagined when this hardware was designed.

1:37:31.000 --> 1:37:34.000
 But that's not the only thing we had to do to make this work.

1:37:34.000 --> 1:37:38.000
 We actually implemented a whole lot of operations and features

1:37:38.000 --> 1:37:42.000
 to make this model compilable, to improve the intake accuracy,

1:37:42.000 --> 1:37:45.000
 as well as to optimize performance.

1:37:45.000 --> 1:37:50.000
 All of these things helped us run the 75 million parameter model

1:37:50.000 --> 1:37:57.000
 just under 10 milliseconds of latency, consuming just 8 watts of power.

1:37:57.000 --> 1:38:00.000
 But this is not the only architecture running in the car.

1:38:00.000 --> 1:38:03.000
 There are so many other architectures, modules, and networks

1:38:03.000 --> 1:38:05.000
 we need to run in the car.

1:38:05.000 --> 1:38:09.000
 To give a sense of scale, there are about a billion parameters

1:38:09.000 --> 1:38:11.000
 of all the networks combined,

1:38:11.000 --> 1:38:14.000
 producing around 1,000 neural network signals.

1:38:14.000 --> 1:38:17.000
 So we need to make sure we optimize them jointly

1:38:17.000 --> 1:38:22.000
 and such that we maximize the compute utilization,

1:38:22.000 --> 1:38:25.000
 throughput, and minimize the latency.

1:38:25.000 --> 1:38:29.000
 So we built a compiler just for neural networks

1:38:29.000 --> 1:38:32.000
 that shares the structure to traditional compilers.

1:38:32.000 --> 1:38:36.000
 As you can see, it takes the massive graph of neural nets

1:38:36.000 --> 1:38:41.000
 with 150k nodes and 375k connection, takes this thing,

1:38:41.000 --> 1:38:44.000
 partitions them into independent subgraphs,

1:38:44.000 --> 1:38:47.000
 and compiles each of those subgraphs natively

1:38:47.000 --> 1:38:49.000
 for the inference devices.

1:38:49.000 --> 1:38:52.000
 Then we have a neural network linker,

1:38:52.000 --> 1:38:54.000
 which shares the structure to traditional linker,

1:38:54.000 --> 1:38:57.000
 where we perform this link-time optimization.

1:38:57.000 --> 1:39:02.000
 There we solve an offline optimization problem

1:39:02.000 --> 1:39:05.000
 with compute memory and memory bandwidth constraints

1:39:05.000 --> 1:39:07.000
 so that it comes with an optimized schedule

1:39:07.000 --> 1:39:10.000
 that gets executed in the car.

1:39:10.000 --> 1:39:14.000
 On the runtime, we designed a hybrid scheduling system

1:39:14.000 --> 1:39:17.000
 which basically does heterogeneous scheduling on one SOC

1:39:17.000 --> 1:39:20.000
 and distributed scheduling across both the SOCs

1:39:20.000 --> 1:39:24.000
 to run these networks in a model-parallel fashion.

1:39:24.000 --> 1:39:27.000
 To get 100 tops of compute utilization,

1:39:27.000 --> 1:39:30.000
 we need to optimize across all the layers of software

1:39:30.000 --> 1:39:34.000
 right from tuning the network architecture, the compiler,

1:39:34.000 --> 1:39:38.000
 all the way to implementing a low latency, high bandwidth RDMA link

1:39:38.000 --> 1:39:42.000
 across both the SOCs, and in fact, going even deeper

1:39:42.000 --> 1:39:45.000
 to understanding and optimizing the cache coherent

1:39:45.000 --> 1:39:49.000
 and non-coherent data paths of the accelerator in the SOC.

1:39:49.000 --> 1:39:51.000
 This is a lot of optimization at every level

1:39:51.000 --> 1:39:54.000
 in order to make sure we get the highest frame rate

1:39:54.000 --> 1:39:59.000
 and as every millisecond counts here.

1:39:59.000 --> 1:40:03.000
 And this is just the visualization

1:40:03.000 --> 1:40:05.000
 of the neural networks that are running in the car.

1:40:05.000 --> 1:40:08.000
 This is our digital brain, essentially.

1:40:08.000 --> 1:40:10.000
 As you can see, these operations are nothing

1:40:10.000 --> 1:40:12.000
 but just the matrix multiplication convolution

1:40:12.000 --> 1:40:16.000
 to name a few real operations running in the car.

1:40:16.000 --> 1:40:20.000
 To train this network with a billion parameters,

1:40:20.000 --> 1:40:22.000
 you need a lot of labeled data.

1:40:22.000 --> 1:40:26.000
 So Egan is going to talk about how do we achieve this

1:40:26.000 --> 1:40:28.000
 with the auto labeling pipeline.

1:40:28.000 --> 1:40:37.000
 Thank you, Shui.

1:40:37.000 --> 1:40:39.000
 Hi, everyone. I'm Egan Zhang,

1:40:39.000 --> 1:40:42.000
 and I'm leading geometric vision at Autopilot.

1:40:42.000 --> 1:40:47.000
 So, yeah, let's talk about auto labeling.

1:40:47.000 --> 1:40:51.000
 So we have several kinds of auto labeling frameworks

1:40:51.000 --> 1:40:53.000
 to support various types of networks.

1:40:53.000 --> 1:40:57.000
 But today I'd like to focus on the awesome LanesNet here.

1:40:57.000 --> 1:41:02.000
 So to successfully train and generalize this network

1:41:02.000 --> 1:41:06.000
 to everywhere, we think we went tens of millions of trips

1:41:06.000 --> 1:41:11.000
 from probably one million intersection or even more.

1:41:11.000 --> 1:41:14.000
 So then how to do that?

1:41:14.000 --> 1:41:17.000
 So it is certainly achievable

1:41:17.000 --> 1:41:20.000
 to source sufficient amount of trips

1:41:20.000 --> 1:41:23.000
 because we already have, as Tim explained earlier,

1:41:23.000 --> 1:41:28.000
 we already have like 500,000 trips per day cache rate.

1:41:28.000 --> 1:41:32.000
 However, converting all those data into a training form

1:41:32.000 --> 1:41:36.000
 is a very challenging technical problem.

1:41:36.000 --> 1:41:38.000
 To solve this challenge,

1:41:38.000 --> 1:41:42.000
 we've tried various ways of manual and auto labeling.

1:41:42.000 --> 1:41:44.000
 So from the first column to the second,

1:41:44.000 --> 1:41:46.000
 from the second to the third,

1:41:46.000 --> 1:41:50.000
 each advance provided us nearly 100x improvement in throughput.

1:41:50.000 --> 1:41:54.000
 But still, we want an even better auto labeling machine

1:41:54.000 --> 1:42:02.000
 that can provide us good quality, diversity, and scalability.

1:42:02.000 --> 1:42:05.000
 To meet all these requirements,

1:42:05.000 --> 1:42:09.000
 despite the huge amount of engineering effort required here,

1:42:09.000 --> 1:42:12.000
 we've developed a new auto labeling machine

1:42:12.000 --> 1:42:15.000
 powered by multi-trip reconstruction.

1:42:15.000 --> 1:42:19.000
 So this can replace 5 million hours of manual labeling

1:42:19.000 --> 1:42:24.000
 with just 12 hours of cluster for labeling 10,000 trips.

1:42:24.000 --> 1:42:26.000
 So how are we solved?

1:42:26.000 --> 1:42:27.000
 There are three big steps.

1:42:27.000 --> 1:42:30.000
 The first step is high-precision trajectory

1:42:30.000 --> 1:42:33.000
 and structural recovery by multi-camera, visual,

1:42:33.000 --> 1:42:35.000
 inertia, or geometry.

1:42:35.000 --> 1:42:37.000
 So here, all the features, including ground surface,

1:42:37.000 --> 1:42:40.000
 are inferred from videos by neural networks,

1:42:40.000 --> 1:42:44.000
 then tracked and reconstructed in the vector space.

1:42:44.000 --> 1:42:48.000
 So the typical drift rate of this trajectory in CAR

1:42:48.000 --> 1:42:53.000
 is like 1.3 cm per meter and 0.45 mRm per meter,

1:42:53.000 --> 1:42:54.000
 which is pretty decent,

1:42:54.000 --> 1:42:57.000
 considering its compact compute requirement.

1:42:57.000 --> 1:43:00.000
 Then the recovered surface and road details

1:43:00.000 --> 1:43:02.000
 are also used as a strong guidance

1:43:02.000 --> 1:43:05.000
 for the later manual verification stuff.

1:43:05.000 --> 1:43:08.000
 This is also enabled in every FSD vehicle,

1:43:08.000 --> 1:43:12.000
 so we get pre-processed trajectories and structures

1:43:12.000 --> 1:43:16.000
 along with the trip data.

1:43:16.000 --> 1:43:19.000
 The second step is multi-trip reconstruction,

1:43:19.000 --> 1:43:22.000
 which is the big and core piece of this machine.

1:43:22.000 --> 1:43:25.000
 So the video shows how the previously shown trip

1:43:25.000 --> 1:43:28.000
 is reconstructed and aligned with other trips,

1:43:28.000 --> 1:43:30.000
 basically other trips from different vehicles,

1:43:30.000 --> 1:43:32.000
 not the same vehicle.

1:43:32.000 --> 1:43:34.000
 So this is done by multiple internal steps

1:43:34.000 --> 1:43:37.000
 like course alignment, pairwise matching,

1:43:37.000 --> 1:43:40.000
 joint optimization, then further surface refinement.

1:43:40.000 --> 1:43:43.000
 In the end, the human analyst comes in

1:43:43.000 --> 1:43:45.000
 and finalizes the label.

1:43:45.000 --> 1:43:49.000
 So each habit steps are already fully parallelized

1:43:49.000 --> 1:43:52.000
 on the cluster, so the entire process

1:43:52.000 --> 1:43:57.000
 usually takes just a couple of hours.

1:43:57.000 --> 1:44:01.000
 The last step is actually auto-labeling the new trips.

1:44:01.000 --> 1:44:05.000
 So here we use the same multi-trip alignment engine,

1:44:05.000 --> 1:44:08.000
 but only between pre-built reconstruction

1:44:08.000 --> 1:44:10.000
 and each new trip.

1:44:10.000 --> 1:44:13.000
 So it's much, much simpler than fully reconstructing

1:44:13.000 --> 1:44:15.000
 all the trips all together.

1:44:15.000 --> 1:44:18.000
 That's why it only takes 30 minutes per trip

1:44:18.000 --> 1:44:24.000
 to auto-label instead of several hours of manual labeling.

1:44:24.000 --> 1:44:32.000
 And this is also the key of scalability of this machine.

1:44:32.000 --> 1:44:35.000
 This machine easily scales as long as we have

1:44:35.000 --> 1:44:38.000
 available compute and trip data.

1:44:38.000 --> 1:44:41.000
 So about 50 trips were newly auto-labeled from this scene,

1:44:41.000 --> 1:44:43.000
 and some of them are shown here.

1:44:43.000 --> 1:44:47.000
 So 53 from different vehicles.

1:44:47.000 --> 1:44:50.000
 So this is how we capture and transform

1:44:50.000 --> 1:44:52.000
 the space-time slices of the world

1:44:52.000 --> 1:44:54.000
 into the network supervision.

1:44:54.000 --> 1:44:56.000
 Yeah, one thing I'd like to note is that

1:44:56.000 --> 1:45:00.000
 Jagen just talked about how we auto-label our lanes,

1:45:00.000 --> 1:45:02.000
 but we have auto-labels for almost every task

1:45:02.000 --> 1:45:04.000
 that we do, including our planner,

1:45:04.000 --> 1:45:06.000
 and many of these are fully automatic.

1:45:06.000 --> 1:45:07.000
 There's no humans involved.

1:45:07.000 --> 1:45:09.000
 For example, for objects, all the kinematics,

1:45:09.000 --> 1:45:11.000
 their shapes, their futures,

1:45:11.000 --> 1:45:13.000
 everything just comes from auto-labeling,

1:45:13.000 --> 1:45:15.000
 and the same is true for occupancy too.

1:45:15.000 --> 1:45:17.000
 And we have really just built a machine around this.

1:45:17.000 --> 1:45:21.000
 Yeah, so if you can go back one slide.

1:45:21.000 --> 1:45:23.000
 One more.

1:45:23.000 --> 1:45:26.000
 It says, parallelized on cluster.

1:45:26.000 --> 1:45:28.000
 So that sounds pretty straightforward,

1:45:28.000 --> 1:45:30.000
 but it really wasn't.

1:45:30.000 --> 1:45:33.000
 Maybe it's fun to share how something like this comes about.

1:45:33.000 --> 1:45:37.000
 So a while ago, we didn't have any auto-labeling at all,

1:45:37.000 --> 1:45:39.000
 and then someone makes a script,

1:45:39.000 --> 1:45:41.000
 it starts to work, it starts working better,

1:45:41.000 --> 1:45:43.000
 until we reach a volume that's pretty high,

1:45:43.000 --> 1:45:45.000
 and we clearly need a solution.

1:45:45.000 --> 1:45:47.000
 And so there were two other engineers in our team

1:45:47.000 --> 1:45:51.000
 who were like, you know, that's an interesting thing.

1:45:51.000 --> 1:45:53.000
 What we needed to do was build a whole graph

1:45:53.000 --> 1:45:55.000
 of essentially Python functions

1:45:55.000 --> 1:45:57.000
 that we need to run one after the other.

1:45:57.000 --> 1:45:59.000
 First you pull the clip, then you do some cleaning,

1:45:59.000 --> 1:46:01.000
 then you do some network inference,

1:46:01.000 --> 1:46:04.000
 then another network inference, until you finally get this.

1:46:04.000 --> 1:46:06.000
 So you need to do this at a large scale.

1:46:06.000 --> 1:46:09.000
 So I tell them, we probably need to shoot for, you know,

1:46:09.000 --> 1:46:12.000
 100,000 clips per day, or like 100,000 items.

1:46:12.000 --> 1:46:14.000
 That seems good.

1:46:14.000 --> 1:46:18.000
 And so the engineers said, well, we can do, you know,

1:46:18.000 --> 1:46:20.000
 a bit of post-gres and a bit of elbow grease.

1:46:20.000 --> 1:46:22.000
 We can do it.

1:46:22.000 --> 1:46:24.000
 Meanwhile, we are a bit later,

1:46:24.000 --> 1:46:28.000
 and we're doing 20 million of these functions every single day.

1:46:28.000 --> 1:46:30.000
 Again, we pull in around half a million clips,

1:46:30.000 --> 1:46:33.000
 and on those we run a ton of functions, each of these,

1:46:33.000 --> 1:46:35.000
 in a streaming fashion.

1:46:35.000 --> 1:46:37.000
 So that's kind of the back-end info that's also needed

1:46:37.000 --> 1:46:40.000
 to not just run training, but also auto-labeling.

1:46:40.000 --> 1:46:43.000
 Yeah, it really is like a factory that produces labels,

1:46:43.000 --> 1:46:46.000
 and production lines, yield, quality, inventory,

1:46:46.000 --> 1:46:49.000
 like all of the same concepts apply to this label factory

1:46:49.000 --> 1:46:52.000
 that applies for, you know, the factory for our cars.

1:46:52.000 --> 1:46:55.000
 That's right.

1:46:55.000 --> 1:46:58.000
 Okay. Thanks, Tim and Ashok.

1:46:58.000 --> 1:47:01.000
 So, yeah, so concluding this section,

1:47:01.000 --> 1:47:03.000
 I'd like to share a few more challenging

1:47:03.000 --> 1:47:06.000
 and interesting examples for network, for sure,

1:47:06.000 --> 1:47:08.000
 and even for humans, probably.

1:47:08.000 --> 1:47:11.000
 So from the top, there's examples for, like,

1:47:11.000 --> 1:47:13.000
 lack of lights, case, or foggy night,

1:47:13.000 --> 1:47:18.000
 or roundabout and heavy occlusions by parked cars,

1:47:18.000 --> 1:47:22.000
 and even rainy night with raindrops on camera lenses.

1:47:22.000 --> 1:47:24.000
 These are challenging, but once their original scenes

1:47:24.000 --> 1:47:27.000
 are fully reconstructed by other clips,

1:47:27.000 --> 1:47:29.000
 all of them can be auto-labeled

1:47:29.000 --> 1:47:31.000
 so that our cars can drive even better

1:47:31.000 --> 1:47:34.000
 through these challenging scenarios.

1:47:34.000 --> 1:47:36.000
 So now let me pass the mic to David

1:47:36.000 --> 1:47:38.000
 to learn more about how Sim is creating

1:47:38.000 --> 1:47:40.000
 the new world on top of these labels.

1:47:40.000 --> 1:47:41.000
 Thank you.

1:47:41.000 --> 1:47:47.000
 Thank you, Jaegen.

1:47:47.000 --> 1:47:51.000
 My name is David, and I'm going to talk about simulation.

1:47:51.000 --> 1:47:54.000
 So simulation plays a critical role in providing data

1:47:54.000 --> 1:47:58.000
 that is difficult to source and or hard to label.

1:47:58.000 --> 1:48:02.000
 However, 3D scenes are notoriously slow to produce.

1:48:02.000 --> 1:48:06.000
 Take, for example, the simulated scene playing behind me,

1:48:06.000 --> 1:48:10.000
 a complex intersection from Market Street in San Francisco.

1:48:10.000 --> 1:48:13.000
 It would take two weeks for artists to complete,

1:48:13.000 --> 1:48:16.000
 and for us, that is painfully slow.

1:48:16.000 --> 1:48:19.000
 However, I'm going to talk about using Jaegen's automated

1:48:19.000 --> 1:48:22.000
 ground truth labels along with some brand new tooling

1:48:22.000 --> 1:48:25.000
 that allows us to procedurally generate this scene

1:48:25.000 --> 1:48:27.000
 and many like it in just five minutes.

1:48:27.000 --> 1:48:31.000
 That's an amazing 1,000 times faster than before.

1:48:31.000 --> 1:48:36.000
 So let's dive in to how a scene like this is created.

1:48:36.000 --> 1:48:39.000
 We start by piping the automated ground truth labels

1:48:39.000 --> 1:48:41.000
 into our simulated world creator tooling

1:48:41.000 --> 1:48:43.000
 inside the software Houdini.

1:48:43.000 --> 1:48:45.000
 Starting with road boundary labels,

1:48:45.000 --> 1:48:47.000
 we can generate a solid road mesh

1:48:47.000 --> 1:48:50.000
 and retopologize it with the lane graph labels.

1:48:50.000 --> 1:48:52.000
 This helps inform important road details

1:48:52.000 --> 1:48:57.000
 like crossroad slope and detailed material blending.

1:48:57.000 --> 1:48:59.000
 Next, we can use the line data and sweep geometry

1:48:59.000 --> 1:49:02.000
 across its surface and project it to the road,

1:49:02.000 --> 1:49:07.000
 creating lane paint decals.

1:49:07.000 --> 1:49:10.000
 Next, using median edges, we can spawn island geometry

1:49:10.000 --> 1:49:13.000
 and populate it with randomized foliage.

1:49:13.000 --> 1:49:16.000
 This drastically changes the visibility of the scene.

1:49:16.000 --> 1:49:18.000
 Now, the outside world can be generated

1:49:18.000 --> 1:49:20.000
 through a series of randomized heuristics,

1:49:20.000 --> 1:49:24.000
 modular building generators create visual obstructions

1:49:24.000 --> 1:49:26.000
 while randomly placed objects like hydrants

1:49:26.000 --> 1:49:28.000
 can change the color of the curbs

1:49:28.000 --> 1:49:30.000
 while trees can drop leaves below it,

1:49:30.000 --> 1:49:33.000
 obscuring lines or edges.

1:49:33.000 --> 1:49:36.000
 Next, we can bring in map data to inform positions

1:49:36.000 --> 1:49:39.000
 of things like traffic lights or stop signs.

1:49:39.000 --> 1:49:43.000
 We can trace along its normal to collect important information

1:49:43.000 --> 1:49:46.000
 like number of lanes and even get accurate street names

1:49:46.000 --> 1:49:48.000
 on the signs themselves.

1:49:48.000 --> 1:49:51.000
 Next, using lane graph, we can determine lane connectivity

1:49:51.000 --> 1:49:54.000
 and spawn directional road markings on the road

1:49:54.000 --> 1:49:57.000
 and their accompanying road signs.

1:49:57.000 --> 1:49:59.000
 And finally, with lane graph itself,

1:49:59.000 --> 1:50:02.000
 we can determine lane adjacency and other useful metrics

1:50:02.000 --> 1:50:05.000
 to spawn randomized traffic permutations

1:50:05.000 --> 1:50:07.000
 inside our simulator.

1:50:07.000 --> 1:50:10.000
 And again, this is all automatic, no artists in the loop,

1:50:10.000 --> 1:50:12.000
 and happens within minutes.

1:50:12.000 --> 1:50:15.000
 And now this sets us up to do some pretty cool things.

1:50:15.000 --> 1:50:18.000
 Since everything is based on data and heuristics,

1:50:18.000 --> 1:50:20.000
 we can start to fuzz parameters

1:50:20.000 --> 1:50:23.000
 to create visual variations of the single ground truth.

1:50:23.000 --> 1:50:26.000
 It can be as subtle as object placement

1:50:26.000 --> 1:50:29.000
 and random material swapping to more drastic changes

1:50:29.000 --> 1:50:32.000
 like entirely new biomes or locations of environment

1:50:32.000 --> 1:50:35.000
 like urban, suburban, or rural.

1:50:35.000 --> 1:50:38.000
 This allows us to create infinite targeted permutations

1:50:38.000 --> 1:50:43.000
 for specific ground truths that we need more ground truth for.

1:50:43.000 --> 1:50:47.000
 And all this happens within a click of a button.

1:50:47.000 --> 1:50:50.000
 And we can even take this one step further

1:50:50.000 --> 1:50:52.000
 by altering our ground truth itself.

1:50:52.000 --> 1:50:55.000
 Say John wants his network to pay more attention

1:50:55.000 --> 1:50:57.000
 to directional road markings to better detect

1:50:57.000 --> 1:50:59.000
 an upcoming captive left turn lane.

1:50:59.000 --> 1:51:02.000
 We can start to procedurally alter our lane graph

1:51:02.000 --> 1:51:06.000
 inside the simulator to create entirely new flows

1:51:06.000 --> 1:51:09.000
 through this intersection to help focus the network's attention

1:51:09.000 --> 1:51:13.000
 to the road markings to create more accurate predictions.

1:51:13.000 --> 1:51:16.000
 And this is a great example of how this tooling allows us

1:51:16.000 --> 1:51:18.000
 to create new data that can never be collected

1:51:18.000 --> 1:51:20.000
 from the real world.

1:51:20.000 --> 1:51:24.000
 And the true power of this tool is in its architecture

1:51:24.000 --> 1:51:28.000
 and how we can run all tasks in parallel to infinitely scale.

1:51:28.000 --> 1:51:31.000
 So you saw the tile creator tool in action

1:51:31.000 --> 1:51:35.000
 converting the ground truth labels into their counterparts.

1:51:35.000 --> 1:51:38.000
 Next, we can use our tile extractor tool

1:51:38.000 --> 1:51:40.000
 to divide this data into geohash tiles

1:51:40.000 --> 1:51:43.000
 about 150 meters square in size.

1:51:43.000 --> 1:51:46.000
 We then save out that data into separate geometry

1:51:46.000 --> 1:51:48.000
 and instance files.

1:51:48.000 --> 1:51:51.000
 This gives us a clean source of data that's easy to load

1:51:51.000 --> 1:51:56.000
 and allows us to be rendering engine agnostic for the future.

1:51:56.000 --> 1:51:59.000
 Then, using a tile loader tool, we can summon any number

1:51:59.000 --> 1:52:02.000
 of those cache tiles using a geohash ID.

1:52:02.000 --> 1:52:06.000
 Currently we're doing about these 5x5 tiles or 3x3,

1:52:06.000 --> 1:52:08.000
 usually centered around fleet hotspots

1:52:08.000 --> 1:52:11.000
 or interesting land graph locations.

1:52:11.000 --> 1:52:14.000
 And the tile loader also converts these tile sets

1:52:14.000 --> 1:52:17.000
 into U assets for consumption by the Unreal Engine

1:52:17.000 --> 1:52:20.000
 and gives you a finished product

1:52:20.000 --> 1:52:23.000
 from what you saw in the first slide.

1:52:23.000 --> 1:52:26.000
 And this really sets us up for size and scale.

1:52:26.000 --> 1:52:28.000
 And as you can see on the map behind us,

1:52:28.000 --> 1:52:32.000
 we can easily generate most of San Francisco's city streets.

1:52:32.000 --> 1:52:35.000
 And this didn't take years or even months of work,

1:52:35.000 --> 1:52:38.000
 but rather two weeks by one person.

1:52:38.000 --> 1:52:41.000
 We can continue to manage and grow all this data

1:52:41.000 --> 1:52:44.000
 using our PDG network inside of the tooling.

1:52:44.000 --> 1:52:46.000
 This allows us to throw compute at it

1:52:46.000 --> 1:52:50.000
 and regenerate all these tile sets overnight.

1:52:50.000 --> 1:52:53.000
 This ensures all environments are of consistent quality

1:52:53.000 --> 1:52:56.000
 and features, which is super important for training

1:52:56.000 --> 1:53:00.000
 since new ontologies and signals are constantly released.

1:53:00.000 --> 1:53:02.000
 And now to come full circle,

1:53:02.000 --> 1:53:04.000
 because we generated all these tile sets

1:53:04.000 --> 1:53:07.000
 from ground truth data that contain all the weird intricacies

1:53:07.000 --> 1:53:09.000
 from the real world, and we can combine that

1:53:09.000 --> 1:53:12.000
 with the procedural, visual, and traffic variety

1:53:12.000 --> 1:53:14.000
 to create limitless, targeted data

1:53:14.000 --> 1:53:17.000
 for the network to learn from.

1:53:17.000 --> 1:53:18.000
 And that concludes the Sim section.

1:53:18.000 --> 1:53:20.000
 I'll pass it to Kate to talk about

1:53:20.000 --> 1:53:23.000
 how we can use all this data to improve Autopilot.

1:53:23.000 --> 1:53:24.000
 Thank you.

1:53:24.000 --> 1:53:34.000
 Thanks, David.

1:53:34.000 --> 1:53:36.000
 Hi, everyone. My name is Kate Park,

1:53:36.000 --> 1:53:38.000
 and I'm here to talk about the data engine,

1:53:38.000 --> 1:53:40.000
 which is the process by which we improve

1:53:40.000 --> 1:53:43.000
 our neural networks via data.

1:53:43.000 --> 1:53:45.000
 We're going to show you how we deterministically

1:53:45.000 --> 1:53:48.000
 solve interventions via data

1:53:48.000 --> 1:53:51.000
 and walk you through the life of this particular clip.

1:53:51.000 --> 1:53:54.000
 In this scenario, Autopilot is approaching a turn

1:53:54.000 --> 1:53:57.000
 and incorrectly predicts that crossing vehicle

1:53:57.000 --> 1:53:59.000
 as stopped for traffic, and thus a vehicle

1:53:59.000 --> 1:54:01.000
 that we would slow down for.

1:54:01.000 --> 1:54:03.000
 In reality, there's nobody in the car.

1:54:03.000 --> 1:54:05.000
 It's just awkwardly parked.

1:54:05.000 --> 1:54:08.000
 We've built this tooling to identify the mispredictions,

1:54:08.000 --> 1:54:11.000
 correct the label, and categorize this clip

1:54:11.000 --> 1:54:13.000
 into an evaluation set.

1:54:13.000 --> 1:54:16.000
 This particular clip happens to be one of 126

1:54:16.000 --> 1:54:18.000
 that we've used in our training.

1:54:18.000 --> 1:54:21.000
 This particular clip happens to be one of 126

1:54:21.000 --> 1:54:25.000
 that we've diagnosed as challenging parked cars at turns.

1:54:25.000 --> 1:54:29.000
 Because of this infra, we can curate this evaluation set

1:54:29.000 --> 1:54:31.000
 without any engineering resources

1:54:31.000 --> 1:54:35.000
 custom to this particular challenge case.

1:54:35.000 --> 1:54:37.000
 To actually solve that challenge case

1:54:37.000 --> 1:54:40.000
 requires mining thousands of examples like it,

1:54:40.000 --> 1:54:43.000
 and it's something Tesla can trivially do.

1:54:43.000 --> 1:54:45.000
 We simply use our data sourcing infra,

1:54:45.000 --> 1:54:48.000
 test data, and use the tooling shown previously

1:54:48.000 --> 1:54:50.000
 to correct the labels.

1:54:50.000 --> 1:54:52.000
 By surgically targeting the mispredictions

1:54:52.000 --> 1:54:54.000
 of the current model,

1:54:54.000 --> 1:54:56.000
 we're only adding the most valuable examples

1:54:56.000 --> 1:54:59.000
 to our training set.

1:54:59.000 --> 1:55:02.000
 We surgically fixed 13,900 clips,

1:55:02.000 --> 1:55:04.000
 and because those were examples

1:55:04.000 --> 1:55:06.000
 where the current model struggles,

1:55:06.000 --> 1:55:09.000
 we don't even need to change the model architecture.

1:55:09.000 --> 1:55:12.000
 A simple weight update with this new valuable data

1:55:12.000 --> 1:55:14.000
 is enough to solve the challenge case.

1:55:14.000 --> 1:55:17.000
 So you see, we no longer predict that crossing vehicle

1:55:17.000 --> 1:55:19.000
 as stopped, as shown in orange,

1:55:19.000 --> 1:55:21.000
 but parked, as shown in red.

1:55:21.000 --> 1:55:25.000
 In academia, we often see that people keep data constant,

1:55:25.000 --> 1:55:28.000
 but at Tesla, it's very much the opposite.

1:55:28.000 --> 1:55:30.000
 We see time and time and again

1:55:30.000 --> 1:55:32.000
 that data is one of the best,

1:55:32.000 --> 1:55:34.000
 if not the most deterministic lever

1:55:34.000 --> 1:55:37.000
 to solving these interventions.

1:55:37.000 --> 1:55:39.000
 We just showed you the data engine loop

1:55:39.000 --> 1:55:42.000
 for one challenge case, namely these parked cars at turns,

1:55:42.000 --> 1:55:44.000
 but there are many challenge cases

1:55:44.000 --> 1:55:46.000
 even for one signal of vehicle movement.

1:55:46.000 --> 1:55:48.000
 We apply this data engine loop

1:55:48.000 --> 1:55:51.000
 to every single challenge case we've diagnosed,

1:55:51.000 --> 1:55:53.000
 whether it's buses, curvy roads,

1:55:53.000 --> 1:55:55.000
 stopped vehicles, parking lots.

1:55:55.000 --> 1:55:57.000
 And we don't just add data once.

1:55:57.000 --> 1:56:00.000
 We do this again and again to perfect the semantic.

1:56:00.000 --> 1:56:02.000
 In fact, this year,

1:56:02.000 --> 1:56:05.000
 we updated our vehicle movement signal five times,

1:56:05.000 --> 1:56:08.000
 and with every weight update trained on the new data,

1:56:08.000 --> 1:56:12.000
 we push our vehicle movement accuracy up and up.

1:56:12.000 --> 1:56:16.000
 This data engine framework applies to all our signals,

1:56:16.000 --> 1:56:19.000
 whether they're 3D, multi-cam video,

1:56:19.000 --> 1:56:21.000
 whether the data is human-labeled,

1:56:21.000 --> 1:56:23.000
 auto-labeled, or simulated,

1:56:23.000 --> 1:56:26.000
 whether it's an offline model or an online model.

1:56:26.000 --> 1:56:29.000
 And Tesla's able to do this at scale

1:56:29.000 --> 1:56:31.000
 because of the fleet advantage,

1:56:31.000 --> 1:56:33.000
 the info that our eng team has built,

1:56:33.000 --> 1:56:36.000
 and the labeling resources that feed our networks.

1:56:36.000 --> 1:56:38.000
 To train on all this data,

1:56:38.000 --> 1:56:40.000
 we need a massive amount of compute,

1:56:40.000 --> 1:56:42.000
 so I'll hand it off to Pete and Ganesh

1:56:42.000 --> 1:56:45.000
 to talk about the DOJO supercomputing platform.

1:56:45.000 --> 1:56:47.000
 Thank you.

1:56:47.000 --> 1:56:49.000
 applause

1:56:49.000 --> 1:56:51.000
 Thank you, Katie.

1:56:55.000 --> 1:56:57.000
 Thanks, everybody. Thanks for hanging in there.

1:56:57.000 --> 1:56:59.000
 We're almost there.

1:56:59.000 --> 1:57:01.000
 My name is Pete Bannon.

1:57:01.000 --> 1:57:04.000
 I run the custom silicon and low-voltage teams at Tesla.

1:57:04.000 --> 1:57:07.000
 And my name is Ganesh Venkat.

1:57:07.000 --> 1:57:09.000
 I run the DOJO program.

1:57:09.000 --> 1:57:12.000
 applause

1:57:14.000 --> 1:57:16.000
 Thank you.

1:57:16.000 --> 1:57:18.000
 I'm frequently asked,

1:57:18.000 --> 1:57:21.000
 why is a car company building a supercomputer for training?

1:57:21.000 --> 1:57:23.000
 And this question fundamentally

1:57:23.000 --> 1:57:27.000
 misunderstands the nature of Tesla.

1:57:27.000 --> 1:57:31.000
 At its heart, Tesla is a hard-core technology company.

1:57:31.000 --> 1:57:33.000
 All across the company,

1:57:33.000 --> 1:57:36.000
 people are working hard in science and engineering

1:57:36.000 --> 1:57:39.000
 to advance the fundamental understanding

1:57:39.000 --> 1:57:43.000
 and methods that we have available to build cars,

1:57:43.000 --> 1:57:45.000
 energy solutions, robots,

1:57:45.000 --> 1:57:48.000
 and anything else that we can do

1:57:48.000 --> 1:57:51.000
 to improve the human condition around the world.

1:57:51.000 --> 1:57:53.000
 It's a super exciting thing to be a part of,

1:57:53.000 --> 1:57:56.000
 and it's a privilege to run a very small piece of it

1:57:56.000 --> 1:57:58.000
 in this semiconductor group.

1:57:58.000 --> 1:58:01.000
 Tonight we're going to talk a little bit about DOJO

1:58:01.000 --> 1:58:03.000
 and give you an update on what we've been able to do

1:58:03.000 --> 1:58:05.000
 over the last year.

1:58:05.000 --> 1:58:07.000
 But before we do that, I wanted to give a little bit of background

1:58:07.000 --> 1:58:10.000
 on the initial design that we started a few years ago.

1:58:10.000 --> 1:58:12.000
 When we got started, the goal was to provide

1:58:12.000 --> 1:58:15.000
 a substantial improvement to the training latency

1:58:15.000 --> 1:58:17.000
 for our autopilot team.

1:58:17.000 --> 1:58:20.000
 Some of the largest neural networks they train today

1:58:20.000 --> 1:58:22.000
 run for over a month,

1:58:22.000 --> 1:58:26.000
 which inhibits their ability to rapidly explore alternatives

1:58:26.000 --> 1:58:28.000
 and evaluate them.

1:58:28.000 --> 1:58:32.000
 A 30x speedup would be really nice if we could provide it

1:58:32.000 --> 1:58:35.000
 at a cost-competitive and energy-competitive way.

1:58:35.000 --> 1:58:38.000
 To do that, we wanted to build a chip

1:58:38.000 --> 1:58:41.000
 with a lot of arithmetic units

1:58:41.000 --> 1:58:44.000
 that we could utilize at a very high efficiency.

1:58:44.000 --> 1:58:47.000
 We spent a lot of time studying whether we could do that

1:58:47.000 --> 1:58:50.000
 using DRAM, various packaging ideas,

1:58:50.000 --> 1:58:52.000
 all of which failed.

1:58:52.000 --> 1:58:55.000
 In the end, even though it felt like an unnatural act,

1:58:55.000 --> 1:58:58.000
 we decided to reject DRAM as the primary storage medium

1:58:58.000 --> 1:59:01.000
 for this system, and instead focus on SRAM

1:59:01.000 --> 1:59:03.000
 embedded in the chip.

1:59:03.000 --> 1:59:07.000
 SRAM provides, unfortunately, a modest amount of capacity,

1:59:07.000 --> 1:59:09.000
 but extremely high bandwidth and very low latency,

1:59:09.000 --> 1:59:12.000
 and that enables us to achieve high utilization

1:59:12.000 --> 1:59:14.000
 with the arithmetic units.

1:59:14.000 --> 1:59:17.000
 Those choices...

1:59:17.000 --> 1:59:20.000
 That particular choice led to a whole bunch of other choices.

1:59:20.000 --> 1:59:23.000
 For example, if you want to have virtual memory,

1:59:23.000 --> 1:59:25.000
 these tables, they take up a lot of space.

1:59:25.000 --> 1:59:28.000
 We didn't have space, so no virtual memory.

1:59:28.000 --> 1:59:31.000
 We also don't have interrupts.

1:59:31.000 --> 1:59:35.000
 The accelerator is a bare-bones, raw piece of hardware

1:59:35.000 --> 1:59:38.000
 that's presented to a compiler, and the compiler is responsible

1:59:38.000 --> 1:59:41.000
 for scheduling everything that happens in a deterministic way,

1:59:41.000 --> 1:59:45.000
 so there's no need or even desire for interrupts in the system.

1:59:45.000 --> 1:59:49.000
 We also chose to pursue model parallelism

1:59:49.000 --> 1:59:51.000
 as a training methodology,

1:59:51.000 --> 1:59:53.000
 which is not the typical situation.

1:59:53.000 --> 1:59:57.000
 Most machines today use data parallelism,

1:59:57.000 --> 1:59:59.000
 which consumes additional memory capacity,

1:59:59.000 --> 2:00:01.000
 which we obviously don't have.

2:00:01.000 --> 2:00:05.000
 So all of those choices led us to build a machine

2:00:05.000 --> 2:00:07.000
 that is pretty radically different

2:00:07.000 --> 2:00:10.000
 from what's available today.

2:00:10.000 --> 2:00:12.000
 We also had a whole bunch of other goals.

2:00:12.000 --> 2:00:14.000
 One of the most important ones was no limits,

2:00:14.000 --> 2:00:16.000
 so we wanted to build a compute fabric

2:00:16.000 --> 2:00:20.000
 that would scale in an unbounded way, for the most part.

2:00:20.000 --> 2:00:23.000
 I mean, obviously there's physical limits now and then,

2:00:23.000 --> 2:00:27.000
 but pretty much if your model was too big for the computer,

2:00:27.000 --> 2:00:29.000
 you just had to go buy a bigger computer.

2:00:29.000 --> 2:00:31.000
 That's what we were looking for.

2:00:31.000 --> 2:00:33.000
 Today, the way machines are packaged,

2:00:33.000 --> 2:00:36.000
 there's a pretty fixed ratio of, for example,

2:00:36.000 --> 2:00:40.000
 GPUs, CPUs, and DRAM capacity, and network capacity,

2:00:40.000 --> 2:00:43.000
 and we really wanted to desegregate all that

2:00:43.000 --> 2:00:45.000
 so that as models evolved,

2:00:45.000 --> 2:00:48.000
 we could vary the ratios of those various elements

2:00:48.000 --> 2:00:50.000
 and make the system more flexible

2:00:50.000 --> 2:00:54.000
 to meet the needs of the autopilot team.

2:00:54.000 --> 2:00:56.000
 Yeah, and it's so true, Pete.

2:00:56.000 --> 2:01:00.000
 Like, no limits philosophy was our guiding star all the way.

2:01:00.000 --> 2:01:04.000
 All of our choices were centered around that,

2:01:04.000 --> 2:01:06.000
 and to the point that we didn't want

2:01:06.000 --> 2:01:09.000
 traditional data center infrastructure

2:01:09.000 --> 2:01:14.000
 to limit our capacity to execute these programs at speed.

2:01:14.000 --> 2:01:24.000
 So that's why we integrated vertically our data center.

2:01:24.000 --> 2:01:26.000
 The entire data center,

2:01:26.000 --> 2:01:30.000
 by doing a vertical integration of the data center,

2:01:30.000 --> 2:01:33.000
 we could extract new levels of efficiency.

2:01:33.000 --> 2:01:37.000
 We could optimize power delivery, cooling,

2:01:37.000 --> 2:01:39.000
 and as well as system management

2:01:39.000 --> 2:01:42.000
 across the whole data center stack

2:01:42.000 --> 2:01:44.000
 rather than doing box by box

2:01:44.000 --> 2:01:49.000
 and integrating those boxes into data centers.

2:01:49.000 --> 2:01:55.000
 And to do this, we also wanted to integrate early

2:01:55.000 --> 2:01:59.000
 to figure out limits of scale for our software workloads.

2:01:59.000 --> 2:02:01.000
 So we integrated Dojo environment

2:02:01.000 --> 2:02:04.000
 into our autopilot software very early,

2:02:04.000 --> 2:02:06.000
 and we learned a lot of lessons,

2:02:06.000 --> 2:02:11.000
 and today Bill Chang will go over our hardware update

2:02:11.000 --> 2:02:13.000
 as well as some of the challenges

2:02:13.000 --> 2:02:15.000
 that we faced along the way,

2:02:15.000 --> 2:02:18.000
 and Rajiv Kurian will give you a glimpse

2:02:18.000 --> 2:02:20.000
 of our compiler technology

2:02:20.000 --> 2:02:25.000
 as well as go over some of our cool results.

2:02:25.000 --> 2:02:27.000
 Great.

2:02:27.000 --> 2:02:31.000
 Here we go.

2:02:31.000 --> 2:02:34.000
 Thanks, Pete. Thanks, Ganesh.

2:02:34.000 --> 2:02:38.000
 I'll start tonight with a high-level vision of our system

2:02:38.000 --> 2:02:42.000
 that will help set the stage for the challenges

2:02:42.000 --> 2:02:44.000
 and the problems we're solving

2:02:44.000 --> 2:02:47.000
 and then also how software will then leverage this

2:02:47.000 --> 2:02:49.000
 for performance.

2:02:49.000 --> 2:02:53.000
 Now, our vision for Dojo is to build a single unified accelerator,

2:02:53.000 --> 2:02:55.000
 a very large one.

2:02:55.000 --> 2:02:58.000
 Software would see a seamless compute plane

2:02:58.000 --> 2:03:01.000
 with globally addressable, very fast memory

2:03:01.000 --> 2:03:03.000
 and all connected together

2:03:03.000 --> 2:03:09.000
 with uniform high bandwidth and low latency.

2:03:09.000 --> 2:03:12.000
 Now, to realize this, we need to use density

2:03:12.000 --> 2:03:14.000
 to achieve performance.

2:03:14.000 --> 2:03:17.000
 Now, we leverage technology to get this density

2:03:17.000 --> 2:03:19.000
 in order to break levels of hierarchy

2:03:19.000 --> 2:03:24.000
 all the way from the chip to the scale-out systems.

2:03:24.000 --> 2:03:28.000
 Now, silicon technology has done this for decades.

2:03:28.000 --> 2:03:33.000
 Chips have followed Moore's Law for density integration

2:03:33.000 --> 2:03:37.000
 to get performance scaling.

2:03:37.000 --> 2:03:39.000
 Now, a key step in realizing that vision

2:03:39.000 --> 2:03:41.000
 was our training tile.

2:03:41.000 --> 2:03:46.000
 Not only can we integrate 25 dies at extremely high bandwidth,

2:03:46.000 --> 2:03:49.000
 but we can scale that to any number of additional tiles

2:03:49.000 --> 2:03:53.000
 by just connecting them together.

2:03:53.000 --> 2:03:57.000
 Now, last year, we showcased our first functional training tile.

2:03:57.000 --> 2:04:02.000
 And at that time, we already had workloads running on it.

2:04:02.000 --> 2:04:06.000
 And since then, the team here has been working hard

2:04:06.000 --> 2:04:10.000
 and diligently to deploy this at scale.

2:04:10.000 --> 2:04:12.000
 Now, we've made amazing progress

2:04:12.000 --> 2:04:15.000
 and had a lot of milestones along the way.

2:04:15.000 --> 2:04:18.000
 And of course, we've had a lot of unexpected challenges.

2:04:18.000 --> 2:04:21.000
 But this is where our fail-fast philosophy

2:04:21.000 --> 2:04:26.000
 has allowed us to push our boundaries.

2:04:26.000 --> 2:04:29.000
 Now, pushing density for performance

2:04:29.000 --> 2:04:31.000
 presents all new challenges.

2:04:31.000 --> 2:04:34.000
 One area is power delivery.

2:04:34.000 --> 2:04:38.000
 Here, we need to deliver the power to our compute die,

2:04:38.000 --> 2:04:42.000
 and this directly impacts our top-line compute performance.

2:04:42.000 --> 2:04:46.000
 But we need to do this at unprecedented density.

2:04:46.000 --> 2:04:49.000
 We need to be able to match our die pitch

2:04:49.000 --> 2:04:53.000
 with a power density of almost 1 amp per millimeter squared.

2:04:53.000 --> 2:04:56.000
 And because of this extreme integration,

2:04:56.000 --> 2:05:00.000
 this needs to be a multi-tiered vertical power solution.

2:05:00.000 --> 2:05:04.000
 And because there's a complex heterogeneous material stack-up,

2:05:04.000 --> 2:05:08.000
 we have to carefully manage the material transition,

2:05:08.000 --> 2:05:12.000
 especially CTE.

2:05:12.000 --> 2:05:15.000
 Now, why does the coefficient of thermal expansion matter

2:05:15.000 --> 2:05:17.000
 in this case?

2:05:17.000 --> 2:05:20.000
 CTE is a fundamental material property.

2:05:20.000 --> 2:05:22.000
 And if it's not carefully managed,

2:05:22.000 --> 2:05:27.000
 that stack-up would literally rip itself apart.

2:05:27.000 --> 2:05:31.000
 So we started this effort by working with vendors

2:05:31.000 --> 2:05:34.000
 to develop this power solution.

2:05:34.000 --> 2:05:38.000
 But we realized that we actually had to develop this in-house.

2:05:38.000 --> 2:05:41.000
 Now, to balance schedule and risk,

2:05:41.000 --> 2:05:44.000
 we built quick iterations to support

2:05:44.000 --> 2:05:47.000
 both our system bring-up and software development

2:05:47.000 --> 2:05:50.000
 and also to find the optimal design and stack-up

2:05:50.000 --> 2:05:53.000
 that would meet our final production goals.

2:05:53.000 --> 2:05:58.000
 And in the end, we were able to reduce CTE over 50%

2:05:58.000 --> 2:06:03.000
 and meet our performance by 3x over our initial version.

2:06:03.000 --> 2:06:07.000
 Now, needless to say, finding this optimal material stack-up

2:06:07.000 --> 2:06:15.000
 while maximizing performance at density is extremely difficult.

2:06:15.000 --> 2:06:18.000
 Now, we did have unexpected challenges along the way.

2:06:18.000 --> 2:06:20.000
 Here's an example where we pushed

2:06:20.000 --> 2:06:25.000
 the boundaries of integration that led to component failures.

2:06:25.000 --> 2:06:29.000
 This started when we scaled up to larger and longer workloads,

2:06:29.000 --> 2:06:35.000
 and then intermittently, a single site on a tile would fail.

2:06:35.000 --> 2:06:38.000
 Now, they started out as recoverable failures,

2:06:38.000 --> 2:06:41.000
 but as we pushed to much higher and higher power,

2:06:41.000 --> 2:06:45.000
 these would become permanent failures.

2:06:45.000 --> 2:06:47.000
 Now, to understand this failure,

2:06:47.000 --> 2:06:52.000
 you have to understand why and how we build our power modules.

2:06:52.000 --> 2:06:57.000
 Solving density at every level is the cornerstone

2:06:57.000 --> 2:07:00.000
 of actually achieving our system performance.

2:07:00.000 --> 2:07:04.000
 Now, because our XY plane is used for high bandwidth communication,

2:07:04.000 --> 2:07:08.000
 everything else must be stacked vertically.

2:07:08.000 --> 2:07:11.000
 This means all other components other than our die

2:07:11.000 --> 2:07:14.000
 must be integrated into our power modules.

2:07:14.000 --> 2:07:17.000
 Now, that includes our clock and our power supplies

2:07:17.000 --> 2:07:21.000
 and also our system controllers.

2:07:21.000 --> 2:07:23.000
 Now, in this case, the failures were due

2:07:23.000 --> 2:07:27.000
 to losing clock output from our oscillators.

2:07:27.000 --> 2:07:31.000
 And after an extensive debug, we found that the root cause

2:07:31.000 --> 2:07:35.000
 was due to vibrations on the module from piezoelectric effects

2:07:35.000 --> 2:07:39.000
 on nearby capacitors.

2:07:39.000 --> 2:07:42.000
 Now, singing caps are not a new phenomenon,

2:07:42.000 --> 2:07:45.000
 and in fact, very common in power design.

2:07:45.000 --> 2:07:47.000
 But normally, clock chips are placed

2:07:47.000 --> 2:07:49.000
 in a very quiet area of the board

2:07:49.000 --> 2:07:52.000
 and often not affected by power circuits.

2:07:52.000 --> 2:07:55.000
 But because we needed to achieve this level of integration,

2:07:55.000 --> 2:08:00.000
 these oscillators need to be placed in very close proximity.

2:08:00.000 --> 2:08:02.000
 Now, due to our switching frequency

2:08:02.000 --> 2:08:05.000
 and then the vibration resonance created,

2:08:05.000 --> 2:08:09.000
 it caused out-of-plane vibration on our MEMS oscillator

2:08:09.000 --> 2:08:13.000
 that caused it to crack.

2:08:13.000 --> 2:08:16.000
 Now, the solution to this problem is a multi-prong approach.

2:08:16.000 --> 2:08:22.000
 We can reduce the vibration by using soft terminal caps.

2:08:22.000 --> 2:08:26.000
 We can update our MEMS part with a lower Q factor

2:08:26.000 --> 2:08:30.000
 for the out-of-plane direction.

2:08:30.000 --> 2:08:33.000
 And we can also update our switching frequency

2:08:33.000 --> 2:08:35.000
 to push the resonance further away

2:08:35.000 --> 2:08:40.000
 from these sensitive bands.

2:08:40.000 --> 2:08:44.000
 Now, in addition to the density at the system level,

2:08:44.000 --> 2:08:46.000
 we've been making a lot of progress

2:08:46.000 --> 2:08:49.000
 at the infrastructure level.

2:08:49.000 --> 2:08:52.000
 We knew that we had to re-examine every aspect

2:08:52.000 --> 2:08:54.000
 of the data center infrastructure

2:08:54.000 --> 2:08:57.000
 in order to support our unprecedented power

2:08:57.000 --> 2:08:59.000
 and cooling density.

2:08:59.000 --> 2:09:02.000
 We brought in a fully custom-designed CDU

2:09:02.000 --> 2:09:06.000
 to support DOJO's dense cooling requirements.

2:09:06.000 --> 2:09:08.000
 And the amazing part is we're able to do this

2:09:08.000 --> 2:09:11.000
 at a fraction of the cost versus buying off the shelf

2:09:11.000 --> 2:09:13.000
 and modifying it.

2:09:13.000 --> 2:09:16.000
 And since our DOJO cabinet integrates enough power

2:09:16.000 --> 2:09:21.000
 and cooling to match an entire row of standard IT racks,

2:09:21.000 --> 2:09:23.000
 we need to carefully design our cabinet

2:09:23.000 --> 2:09:26.000
 and infrastructure together.

2:09:26.000 --> 2:09:28.000
 And we've already gone through several iterations

2:09:28.000 --> 2:09:31.000
 of this cabinet to optimize this.

2:09:31.000 --> 2:09:34.000
 And earlier this year, we started load testing

2:09:34.000 --> 2:09:36.000
 our power and cooling infrastructure,

2:09:36.000 --> 2:09:39.000
 and we were able to push it over 2 megawatts

2:09:39.000 --> 2:09:46.000
 before we tripped our substation and got a call from the city.

2:09:46.000 --> 2:09:49.000
 Now, last year, we introduced only a couple of components

2:09:49.000 --> 2:09:53.000
 of our system, the custom D1 die and the training tile,

2:09:53.000 --> 2:09:57.000
 but we teased the exit pod as our end goal.

2:09:57.000 --> 2:09:59.000
 We'll walk through the remaining parts of our system

2:09:59.000 --> 2:10:04.000
 that are required to build out this exit pod.

2:10:04.000 --> 2:10:07.000
 Now, the system tray is a key part of realizing our vision

2:10:07.000 --> 2:10:09.000
 of a single accelerator.

2:10:09.000 --> 2:10:13.000
 It enables us to seamlessly connect tiles together,

2:10:13.000 --> 2:10:17.000
 not only within the cabinet but between cabinets.

2:10:17.000 --> 2:10:21.000
 We can connect these tiles at very tight spacing

2:10:21.000 --> 2:10:23.000
 across the entire accelerator,

2:10:23.000 --> 2:10:27.000
 and this is how we achieve our uniform communication.

2:10:27.000 --> 2:10:30.000
 This is a laminated bus bar that allows us to integrate

2:10:30.000 --> 2:10:33.000
 very high power, mechanical and thermal support,

2:10:33.000 --> 2:10:36.000
 and an extremely dense integration.

2:10:36.000 --> 2:10:41.000
 It's 75 millimeters in height and supports six tiles

2:10:41.000 --> 2:10:43.000
 at 135 kilograms.

2:10:43.000 --> 2:10:46.000
 This is the equivalent of three to four

2:10:46.000 --> 2:10:52.000
 fully loaded high-performance racks.

2:10:52.000 --> 2:10:55.000
 Next, we need to feed data to the training tiles.

2:10:55.000 --> 2:10:59.000
 This is where we've developed the Dojo interface processor.

2:10:59.000 --> 2:11:02.000
 It provides our system with high bandwidth DRAM

2:11:02.000 --> 2:11:05.000
 to stage our training data,

2:11:05.000 --> 2:11:08.000
 and it provides full memory bandwidth to our training tiles

2:11:08.000 --> 2:11:12.000
 using TTP, our custom protocol that we use to communicate

2:11:12.000 --> 2:11:15.000
 across our entire accelerator.

2:11:15.000 --> 2:11:18.000
 It also has high-speed Ethernet that helps us extend

2:11:18.000 --> 2:11:21.000
 this custom protocol over standard Ethernet,

2:11:21.000 --> 2:11:24.000
 and we provide native hardware support for this

2:11:24.000 --> 2:11:27.000
 with little to no software overhead.

2:11:27.000 --> 2:11:30.000
 And lastly, we can connect to it through

2:11:30.000 --> 2:11:35.000
 a standard Gen4 PCIe interface.

2:11:35.000 --> 2:11:38.000
 Now, we pair 20 of these cards per tray,

2:11:38.000 --> 2:11:43.000
 and that gives us 640 gigabytes of high-bandwidth DRAM,

2:11:43.000 --> 2:11:46.000
 and this provides our disaggregated memory layer

2:11:46.000 --> 2:11:48.000
 for our training tiles.

2:11:48.000 --> 2:11:51.000
 These cards are our high-bandwidth ingest path,

2:11:51.000 --> 2:11:55.000
 both through PCIe and Ethernet.

2:11:55.000 --> 2:11:58.000
 They also provide a high-radix Z-connectivity path

2:11:58.000 --> 2:12:04.000
 that allows shortcuts across our large Dojo accelerator.

2:12:04.000 --> 2:12:07.000
 Now, we actually integrate the host directly

2:12:07.000 --> 2:12:09.000
 underneath our system tray.

2:12:09.000 --> 2:12:12.000
 These hosts provide our ingest processing

2:12:12.000 --> 2:12:16.000
 and connect to our interface processors through PCIe.

2:12:16.000 --> 2:12:21.000
 These hosts can provide hardware video decoder support

2:12:21.000 --> 2:12:24.000
 for video-based training,

2:12:24.000 --> 2:12:27.000
 and our user applications land on these hosts

2:12:27.000 --> 2:12:35.000
 so we can provide them with the standard x86 Linux environment.

2:12:35.000 --> 2:12:39.000
 Now, we can put two of these assemblies into one cabinet

2:12:39.000 --> 2:12:41.000
 and pair it with redundant power supplies

2:12:41.000 --> 2:12:46.000
 that do direct conversion of three-phase 480-volt AC power

2:12:46.000 --> 2:12:52.000
 to 52-volt DC power.

2:12:52.000 --> 2:12:56.000
 Now, by focusing on density at every level,

2:12:56.000 --> 2:13:00.000
 we can realize the vision of a single accelerator.

2:13:00.000 --> 2:13:04.000
 Now, starting with the uniform nodes on our custom D1 die,

2:13:04.000 --> 2:13:09.000
 we can connect them together in our fully integrated training tile,

2:13:09.000 --> 2:13:12.000
 and then finally, seamlessly connecting them

2:13:12.000 --> 2:13:17.000
 across cabinet boundaries to form our Dojo accelerator.

2:13:17.000 --> 2:13:22.000
 And altogether, we can house two full accelerators in our ExaPod

2:13:22.000 --> 2:13:26.000
 for a combined one exaflop of ML compute.

2:13:26.000 --> 2:13:30.000
 Now, altogether, this amount of technology and integration

2:13:30.000 --> 2:13:35.000
 has only ever been done a couple of times in the history of compute.

2:13:35.000 --> 2:13:48.000
 Next, we'll see how software can leverage this to accelerate their performance.

2:13:48.000 --> 2:13:53.000
 Thanks, Bill. My name is Rajeev, and I'm going to talk some numbers.

2:13:53.000 --> 2:13:56.000
 So our software stack begins with the PyTorch extension

2:13:56.000 --> 2:14:01.000
 that speaks to our commitment to run standard PyTorch models out of the box.

2:14:01.000 --> 2:14:03.000
 We're going to talk more about our JIT compiler

2:14:03.000 --> 2:14:07.000
 and the ingest pipeline that feeds the hardware with data.

2:14:07.000 --> 2:14:13.000
 Abstractly, performance is tops times utilization times accelerator occupancy.

2:14:13.000 --> 2:14:16.000
 We've seen how the hardware provides peak performance.

2:14:16.000 --> 2:14:19.000
 It's the job of the compiler to extract utilization from the hardware

2:14:19.000 --> 2:14:21.000
 while code is running on it.

2:14:21.000 --> 2:14:24.000
 And it's the job of the ingest pipeline to make sure

2:14:24.000 --> 2:14:26.000
 that data can be fed at a throughput high enough

2:14:26.000 --> 2:14:29.000
 for the hardware to not ever starve.

2:14:29.000 --> 2:14:33.000
 So let's talk about why communication-bound models are difficult to scale.

2:14:33.000 --> 2:14:38.000
 But before that, let's look at why ResNet-50-like models are easier to scale.

2:14:38.000 --> 2:14:42.000
 You start off with a single accelerator, run the forward and backward passes,

2:14:42.000 --> 2:14:44.000
 followed by the optimizer.

2:14:44.000 --> 2:14:48.000
 Then to scale this up, you run multiple copies of this on multiple accelerators.

2:14:48.000 --> 2:14:51.000
 And while the gradients produced by the backward pass do need to be reduced,

2:14:51.000 --> 2:14:54.000
 and this introduces some communication,

2:14:54.000 --> 2:15:00.000
 this can be done pipelined with the backward pass.

2:15:00.000 --> 2:15:05.000
 This setup scales fairly well, almost linearly.

2:15:05.000 --> 2:15:07.000
 For models with much larger activations,

2:15:07.000 --> 2:15:11.000
 we run into a problem as soon as we want to run the forward pass.

2:15:11.000 --> 2:15:13.000
 The batch size that fits in a single accelerator

2:15:13.000 --> 2:15:16.000
 is often smaller than the batch norm surface.

2:15:16.000 --> 2:15:18.000
 So to get around this, researchers typically run this setup

2:15:18.000 --> 2:15:22.000
 on multiple accelerators in sync batch norm mode.

2:15:22.000 --> 2:15:26.000
 This introduces latency-bound communication to the critical path of the forward pass,

2:15:26.000 --> 2:15:30.000
 and we already have a communication bottleneck.

2:15:30.000 --> 2:15:31.000
 And while there are ways to get around this,

2:15:31.000 --> 2:15:36.000
 they usually involve tedious manual work best suited for a compiler.

2:15:36.000 --> 2:15:38.000
 And ultimately, there's no skirting around the fact

2:15:38.000 --> 2:15:41.000
 that if your state does not fit in a single accelerator,

2:15:41.000 --> 2:15:45.000
 you can be communication-bound.

2:15:45.000 --> 2:15:48.000
 And even with significant efforts from our ML engineers,

2:15:48.000 --> 2:15:52.000
 we see such models don't scale linearly.

2:15:52.000 --> 2:15:57.000
 The Dojo system was built to make such models work at high utilization.

2:15:57.000 --> 2:16:00.000
 The high-density integration was built to not only accelerate

2:16:00.000 --> 2:16:02.000
 the compute-bound portions of a model,

2:16:02.000 --> 2:16:06.000
 but also the latency-bound portions, like a batch norm,

2:16:06.000 --> 2:16:13.000
 or the bandwidth-bound portions, like a gradient all-reduce or a parameter all-gather.

2:16:13.000 --> 2:16:17.000
 A slice of the Dojo mesh can be carved out to run any model.

2:16:17.000 --> 2:16:21.000
 The only thing users need to do is to make the slice large enough

2:16:21.000 --> 2:16:25.000
 to fit a batch norm surface for their particular model.

2:16:25.000 --> 2:16:29.000
 After that, the partition presents itself as one large accelerator,

2:16:29.000 --> 2:16:34.000
 freeing the users from having to worry about the internal details of execution.

2:16:34.000 --> 2:16:38.000
 And it's the job of the compiler to maintain this abstraction.

2:16:38.000 --> 2:16:41.000
 Fine-grained synchronization primitives and uniform low latency

2:16:41.000 --> 2:16:47.000
 makes it easy to accelerate all forms of parallelism across integration boundaries.

2:16:47.000 --> 2:16:49.000
 Tensors are usually stored sharded in SRAM

2:16:49.000 --> 2:16:52.000
 and replicated just in time for a layer's execution.

2:16:52.000 --> 2:16:57.000
 We depend on the high Dojo bandwidth to hide this replication time.

2:16:57.000 --> 2:17:01.000
 Tensor replication and other data transfers are overlapped with compute,

2:17:01.000 --> 2:17:06.000
 and the compiler can also recompute layers when it's profitable to do so.

2:17:06.000 --> 2:17:09.000
 We expect most models to work out of the box.

2:17:09.000 --> 2:17:13.000
 As an example, we took the recently released stable diffusion model

2:17:13.000 --> 2:17:15.000
 and got it running on Dojo in minutes.

2:17:15.000 --> 2:17:19.000
 Out of the box, the compiler was able to map it in a model parallel manner

2:17:19.000 --> 2:17:22.000
 on 25 Dojo dyes.

2:17:22.000 --> 2:17:25.000
 Here are some pictures of a Cybertruck on Mars

2:17:25.000 --> 2:17:29.000
 generated by stable diffusion running on Dojo.

2:17:29.000 --> 2:17:30.000
 Looks...

2:17:30.000 --> 2:17:42.000
 Looks like it still has some ways to go before matching the Tesla Design Studio team.

2:17:42.000 --> 2:17:46.000
 So we've talked about how communication bottlenecks can hamper scalability.

2:17:46.000 --> 2:17:50.000
 Perhaps an acid test of a compiler and the underlying hardware

2:17:50.000 --> 2:17:52.000
 is executing a cross-dye batch norm layer.

2:17:52.000 --> 2:17:55.000
 Like mentioned before, this can be a serial bottleneck.

2:17:55.000 --> 2:17:58.000
 The communication phase of a batch norm begins with nodes

2:17:58.000 --> 2:18:01.000
 computing their local mean and standard deviations,

2:18:01.000 --> 2:18:05.000
 then coordinating to reduce these values, then broadcasting these values back,

2:18:05.000 --> 2:18:09.000
 and then they resume their work in parallel.

2:18:09.000 --> 2:18:13.000
 So what would an ideal batch norm look like on 25 Dojo dyes?

2:18:13.000 --> 2:18:19.000
 Let's say the previous laser activations are already split across dyes.

2:18:19.000 --> 2:18:22.000
 We would expect the 350 nodes on each dye to coordinate

2:18:22.000 --> 2:18:26.000
 and produce dye local mean and standard deviation values.

2:18:26.000 --> 2:18:29.000
 Ideally, these would get further reduced with the final value ending

2:18:29.000 --> 2:18:33.000
 somewhere towards the middle of the tile.

2:18:33.000 --> 2:18:38.000
 We would then hope to see a broadcast of this value radiating from the center.

2:18:38.000 --> 2:18:40.000
 Let's see how the compiler actually executes

2:18:40.000 --> 2:18:43.000
 a real batch norm operation across 25 dyes.

2:18:43.000 --> 2:18:46.000
 The communication trees were extracted from the compiler,

2:18:46.000 --> 2:18:49.000
 and the timing is from a real hardware one.

2:18:49.000 --> 2:18:53.000
 We're about to see 8,750 nodes on 25 dyes

2:18:53.000 --> 2:18:56.000
 coordinating to reduce and then broadcast

2:18:56.000 --> 2:18:59.000
 the batch norm mean and standard deviation values.

2:18:59.000 --> 2:19:03.000
 Dye local reduction, followed by global reduction

2:19:03.000 --> 2:19:07.000
 towards the middle of the tile, then the reduced value broadcast

2:19:07.000 --> 2:19:15.000
 radiating from the middle, accelerated by the hardware's broadcast facility.

2:19:15.000 --> 2:19:20.000
 This operation takes only 5 microseconds on 25 Dojo dyes.

2:19:20.000 --> 2:19:24.000
 The same operation takes 150 microseconds on 24 GPUs.

2:19:24.000 --> 2:19:28.000
 This is in orders of magnitude improvement over GPUs.

2:19:28.000 --> 2:19:30.000
 And while we talked about an all-reduce operation

2:19:30.000 --> 2:19:33.000
 in the context of a batch norm, it's important to reiterate

2:19:33.000 --> 2:19:38.000
 that the same advantages apply to all other communication primitives,

2:19:38.000 --> 2:19:42.000
 and these primitives are essential for large-scale training.

2:19:42.000 --> 2:19:45.000
 So how about full model performance?

2:19:45.000 --> 2:19:48.000
 So while we think that the ResNet-50 is not a good representation

2:19:48.000 --> 2:19:52.000
 of real-world Tesla workloads, it is a standard benchmark,

2:19:52.000 --> 2:19:54.000
 so let's start there.

2:19:54.000 --> 2:19:57.000
 We are already able to match the A100 dye-for-dye.

2:19:57.000 --> 2:20:00.000
 However, perhaps a hint of Dojo's capabilities

2:20:00.000 --> 2:20:04.000
 is that we're able to hit this number with just a batch of 8 per dye.

2:20:04.000 --> 2:20:09.000
 But Dojo was really built to tackle larger, complex models.

2:20:09.000 --> 2:20:11.000
 So when we set out to tackle real-world workloads,

2:20:11.000 --> 2:20:15.000
 we looked at the usage patterns of our current GPU cluster,

2:20:15.000 --> 2:20:16.000
 and two models stood out.

2:20:16.000 --> 2:20:19.000
 The auto-labeling networks, a class of offline models

2:20:19.000 --> 2:20:21.000
 that are used to generate ground truth,

2:20:21.000 --> 2:20:24.000
 and the occupancy networks that you heard about.

2:20:24.000 --> 2:20:26.000
 The auto-labeling networks are large models

2:20:26.000 --> 2:20:28.000
 that have high arithmetic intensity,

2:20:28.000 --> 2:20:31.000
 while the occupancy networks can be ingest bound.

2:20:31.000 --> 2:20:34.000
 We chose these models because together they account

2:20:34.000 --> 2:20:37.000
 for a large chunk of our current GPU cluster usage,

2:20:37.000 --> 2:20:42.000
 and they would challenge the system in different ways.

2:20:42.000 --> 2:20:44.000
 So how did we do on these two networks?

2:20:44.000 --> 2:20:47.000
 The results we're about to see were measured on multi-dye systems

2:20:47.000 --> 2:20:52.000
 for both the GPU and Dojo, but normalized to per-dye numbers.

2:20:52.000 --> 2:20:54.000
 On our auto-labeling network,

2:20:54.000 --> 2:20:57.000
 we're already able to surpass the performance of an A100

2:20:57.000 --> 2:21:01.000
 with our current hardware running on our older generation VRMs.

2:21:01.000 --> 2:21:04.000
 On our production hardware with our newer VRMs,

2:21:04.000 --> 2:21:08.000
 that translates to doubling the throughput of an A100.

2:21:08.000 --> 2:21:11.000
 And our model showed that with some key compiler optimizations,

2:21:11.000 --> 2:21:16.000
 we could get to more than 3X the performance of an A100.

2:21:16.000 --> 2:21:20.000
 We see even bigger leaps on the occupancy network.

2:21:20.000 --> 2:21:26.000
 Almost 3X with our production hardware, with room for more.

2:21:26.000 --> 2:21:35.000
 So what does that mean for Tesla?

2:21:35.000 --> 2:21:38.000
 With the current level of compiler performance,

2:21:38.000 --> 2:21:43.000
 we can replace the ML compute of one, two, three, four, five,

2:21:43.000 --> 2:21:55.000
 and six GPU boxes with just a single Dojo tile.

2:21:55.000 --> 2:22:04.000
 And this Dojo tile costs less than one of these GPU boxes.

2:22:04.000 --> 2:22:06.000
 What it really means is that

2:22:06.000 --> 2:22:08.000
 the networks that took more than a month to train

2:22:08.000 --> 2:22:13.000
 now take less than a week.

2:22:13.000 --> 2:22:17.000
 Alas, when we measured things, it did not turn out so well.

2:22:17.000 --> 2:22:18.000
 At the PyTorch level,

2:22:18.000 --> 2:22:21.000
 we did not see our expected performance out of the gate.

2:22:21.000 --> 2:22:24.000
 And this timeline chart shows our problem.

2:22:24.000 --> 2:22:26.000
 The teeny tiny little green bars,

2:22:26.000 --> 2:22:29.000
 that's the compile code running on the accelerator.

2:22:29.000 --> 2:22:36.000
 The row is mostly white space where the hardware is just waiting for data.

2:22:36.000 --> 2:22:37.000
 With our dense ML compute,

2:22:37.000 --> 2:22:42.000
 Dojo hosts effectively have 10X more ML compute than the GPU hosts.

2:22:42.000 --> 2:22:44.000
 The data loaders running on this one host

2:22:44.000 --> 2:22:48.000
 simply couldn't keep up with all that ML hardware.

2:22:48.000 --> 2:22:51.000
 So to solve our data loader scalability issues,

2:22:51.000 --> 2:22:54.000
 we knew we had to get over the limit of this single host.

2:22:54.000 --> 2:22:58.000
 The Tesla transport protocol moves data seamlessly across hosts,

2:22:58.000 --> 2:23:00.000
 tiles, and ingest processors.

2:23:00.000 --> 2:23:04.000
 So we extended the Tesla transport protocol to work over Ethernet.

2:23:04.000 --> 2:23:07.000
 We then built the Dojo network interface card, the DNIC,

2:23:07.000 --> 2:23:09.000
 to leverage TTP over Ethernet.

2:23:09.000 --> 2:23:13.000
 This allows any host with a DNIC card to be able to DM it to

2:23:13.000 --> 2:23:17.000
 and from other TTP endpoints.

2:23:17.000 --> 2:23:19.000
 So we started with the Dojo mesh.

2:23:19.000 --> 2:23:25.000
 Then we added a tier of data loading hosts equipped with the DNIC card.

2:23:25.000 --> 2:23:29.000
 We connected these hosts to the mesh via an Ethernet switch.

2:23:29.000 --> 2:23:32.000
 Now every host in this data loading tier is capable of reaching

2:23:32.000 --> 2:23:38.000
 all TTP endpoints in the Dojo mesh via hardware-accelerated DMA.

2:23:38.000 --> 2:23:41.000
 After these optimizations went in,

2:23:41.000 --> 2:23:46.000
 our occupancy went from 4% to 97%.

2:23:46.000 --> 2:23:48.000
 So the data loading sections have reduced.

2:23:48.000 --> 2:23:56.000
 The data loading sections have reduced drastically,

2:23:56.000 --> 2:23:58.000
 and the ML hardware has kept busy.

2:23:58.000 --> 2:24:01.000
 We actually expect this number to go to 100% pretty soon.

2:24:01.000 --> 2:24:03.000
 After these changes went in,

2:24:03.000 --> 2:24:06.000
 we saw the full expected speedup from the PyTorch layer,

2:24:06.000 --> 2:24:09.000
 and we were back in business.

2:24:09.000 --> 2:24:12.000
 So we started with hardware design that breaks through

2:24:12.000 --> 2:24:15.000
 traditional integration boundaries in service of our vision

2:24:15.000 --> 2:24:17.000
 of a single giant accelerator.

2:24:17.000 --> 2:24:21.000
 We've seen how the compiler and ingest layers build on top of that hardware.

2:24:21.000 --> 2:24:25.000
 So after proving our performance on these complex real-world networks,

2:24:25.000 --> 2:24:28.000
 we knew what our first large-scale deployment would target,

2:24:28.000 --> 2:24:32.000
 our high arithmetic intensity auto-labeling networks.

2:24:32.000 --> 2:24:37.000
 Today, that occupies 4,000 GPUs over 72 GPU racks.

2:24:37.000 --> 2:24:40.000
 With our dense compute and our high performance,

2:24:40.000 --> 2:24:45.000
 we expect to provide the same throughput with just four Dojo cabinets.

2:24:45.000 --> 2:24:57.000
 And these four Dojo cabinets will be part of our first exapod

2:24:57.000 --> 2:25:00.000
 that we plan to build by quarter one of 2023.

2:25:00.000 --> 2:25:04.000
 This will more than double Tesla's auto-labeling capacity.

2:25:04.000 --> 2:25:14.000
 The first exapod is part of a total of seven exapods

2:25:14.000 --> 2:25:21.000
 that we plan to build in Palo Alto right here across the wall.

2:25:21.000 --> 2:25:23.000
 And we have a display cabinet from one of these exapods

2:25:23.000 --> 2:25:28.000
 for everyone to look at.

2:25:28.000 --> 2:25:33.000
 Six tiles densely packed on a tray, 54 petaflops of compute,

2:25:33.000 --> 2:25:39.000
 640 gigabytes of high bandwidth memory with power and host defeated.

2:25:39.000 --> 2:25:44.000
 A lot of compute.

2:25:44.000 --> 2:25:47.000
 And we're building out new versions of all our cluster components

2:25:47.000 --> 2:25:51.000
 and constantly improving our software to hit new limits of scale.

2:25:51.000 --> 2:25:54.000
 We believe that we can get another 10x improvement

2:25:54.000 --> 2:25:58.000
 with our next generation hardware.

2:25:58.000 --> 2:26:00.000
 And to realize our ambitious goals,

2:26:00.000 --> 2:26:02.000
 we need the best software and hardware engineers.

2:26:02.000 --> 2:26:05.000
 So please come talk to us or visit tesla.com.ai.

2:26:05.000 --> 2:26:07.000
 Thank you.

2:26:07.000 --> 2:26:27.000
 All right.

2:26:27.000 --> 2:26:34.000
 All right, so hopefully that was enough detail.

2:26:34.000 --> 2:26:38.000
 And now we can move to questions.

2:26:38.000 --> 2:26:46.000
 And guys, I think the team can come out on stage.

2:26:46.000 --> 2:26:50.000
 We really wanted to show the depth and breadth of Tesla

2:26:50.000 --> 2:26:58.000
 in artificial intelligence, compute hardware, robotics actuators,

2:26:58.000 --> 2:27:02.000
 and try to really shift the perception of the company

2:27:02.000 --> 2:27:07.000
 away from, you know, a lot of people think we're like just a car company

2:27:07.000 --> 2:27:11.000
 or we make cool cars, whatever, but they don't have,

2:27:11.000 --> 2:27:16.000
 most people have no idea that Tesla is arguably the leader

2:27:16.000 --> 2:27:20.000
 in real world AI hardware and software.

2:27:20.000 --> 2:27:26.000
 And that we're building what is arguably the first,

2:27:26.000 --> 2:27:33.000
 the most radical computer architecture since the Cray-1 supercomputer.

2:27:33.000 --> 2:27:37.000
 And I think if you're interested in developing some of the most advanced

2:27:37.000 --> 2:27:40.000
 technology in the world that's going to really affect the world

2:27:40.000 --> 2:27:44.000
 in a positive way, Tesla's the place to be.

2:27:44.000 --> 2:27:49.000
 So yeah, let's fire away with some questions.

2:27:49.000 --> 2:27:56.000
 I think there's a mic at the front and a mic at the back.

2:27:56.000 --> 2:28:01.000
 Or just throw mics at people.

2:28:01.000 --> 2:28:03.000
 Jump off of the mic.

2:28:03.000 --> 2:28:06.000
 Yeah, hi, thank you very much.

2:28:06.000 --> 2:28:08.000
 I was impressed here.

2:28:08.000 --> 2:28:09.000
 Yeah.

2:28:09.000 --> 2:28:15.000
 I was impressed very much by Optimus, but I wonder why tendon driven the hunt?

2:28:15.000 --> 2:28:18.000
 Why did you choose a tendon driven approach for the hunt?

2:28:18.000 --> 2:28:21.000
 Because tendons are not very durable.

2:28:21.000 --> 2:28:25.000
 And why spring-loaded?

2:28:25.000 --> 2:28:26.000
 Yeah.

2:28:26.000 --> 2:28:27.000
 Hello, is this working?

2:28:27.000 --> 2:28:28.000
 Cool, awesome.

2:28:28.000 --> 2:28:30.000
 Yes, that's a great question.

2:28:30.000 --> 2:28:33.000
 When it comes to any type of actuation scheme,

2:28:33.000 --> 2:28:36.000
 there's trade-offs between whether or not it's a tendon driven system

2:28:36.000 --> 2:28:38.000
 or some type of linkage based system.

2:28:38.000 --> 2:28:39.000
 Just keep the mic close to your mouth.

2:28:39.000 --> 2:28:40.000
 A little bit closer.

2:28:40.000 --> 2:28:41.000
 Yeah.

2:28:41.000 --> 2:28:42.000
 Hear me?

2:28:42.000 --> 2:28:43.000
 Cool.

2:28:43.000 --> 2:28:44.000
 Yeah.

2:28:44.000 --> 2:28:47.000
 So yeah, the main reason why we went for a tendon based system

2:28:47.000 --> 2:28:50.000
 is that first we actually investigated some synthetic tendons,

2:28:50.000 --> 2:28:54.000
 but we found that metallic boating cables are a lot stronger.

2:28:54.000 --> 2:29:00.000
 One of the advantages of these cables is that it's very good for part reduction.

2:29:00.000 --> 2:29:04.000
 We do want to make a lot of these hands, so having a bunch of parts,

2:29:04.000 --> 2:29:07.000
 a bunch of small linkages ends up being a problem

2:29:07.000 --> 2:29:09.000
 when you're making a lot of something.

2:29:09.000 --> 2:29:15.000
 One of the big reasons that tendons are better than linkages in a sense

2:29:15.000 --> 2:29:18.000
 is that you can be anti-backlash.

2:29:18.000 --> 2:29:22.000
 Anti-backlash essentially allows you to not have any gaps

2:29:22.000 --> 2:29:26.000
 or stuttering motion in your fingers.

2:29:26.000 --> 2:29:30.000
 Spring-loaded, mainly what spring-loaded allows us to do

2:29:30.000 --> 2:29:33.000
 is allows us to have active opening.

2:29:33.000 --> 2:29:37.000
 So instead of having to have two actuators to drive the fingers closed

2:29:37.000 --> 2:29:41.000
 and then open, we have the ability to have the tendon drive them closed

2:29:41.000 --> 2:29:44.000
 and then the springs passively extend.

2:29:44.000 --> 2:29:46.000
 This is something that's seen in our hands as well.

2:29:46.000 --> 2:29:52.000
 We have the ability to actively flex, and then we also have the ability to extend.

2:29:52.000 --> 2:29:57.000
 Our goal with Optimus is to have a robot that is maximally useful

2:29:57.000 --> 2:29:59.000
 as quickly as possible.

2:29:59.000 --> 2:30:05.000
 There's a lot of ways to solve the various problems of a humanoid robot.

2:30:05.000 --> 2:30:10.000
 We're probably not barking up the right tree on all the technical solutions.

2:30:10.000 --> 2:30:14.000
 We're aware that we're open to evolving the technical solutions

2:30:14.000 --> 2:30:16.000
 that you see here over time.

2:30:16.000 --> 2:30:18.000
 They're not locked in stone.

2:30:18.000 --> 2:30:21.000
 But we have to pick something,

2:30:21.000 --> 2:30:24.000
 and we want to pick something that's going to allow us

2:30:24.000 --> 2:30:27.000
 to produce the robot as quickly as possible

2:30:27.000 --> 2:30:30.000
 and, like I said, be useful as quickly as possible.

2:30:30.000 --> 2:30:34.000
 We're trying to follow the goal of fastest path to a useful robot

2:30:34.000 --> 2:30:37.000
 that can be made at volume.

2:30:37.000 --> 2:30:42.000
 We're going to test the robot internally at Tesla in our factory

2:30:42.000 --> 2:30:45.000
 and just see how useful is it.

2:30:45.000 --> 2:30:54.000
 You've got to close the loop on reality to confirm that the robot is, in fact, useful.

2:30:54.000 --> 2:30:58.000
 So we're just going to use it to build things.

2:30:58.000 --> 2:31:02.000
 We're confident we can do that with the hand that we have currently designed,

2:31:02.000 --> 2:31:05.000
 but for sure there will be hand version 2, version 3,

2:31:05.000 --> 2:31:08.000
 that will change the architecture quite significantly over time.

2:31:12.000 --> 2:31:14.000
 Hi.

2:31:14.000 --> 2:31:17.000
 The Optimus robot is really impressive.

2:31:17.000 --> 2:31:19.000
 You did a great job.

2:31:19.000 --> 2:31:22.000
 Bipedal robots are really difficult.

2:31:22.000 --> 2:31:26.000
 But what I noticed might be missing from your plan

2:31:26.000 --> 2:31:30.000
 is to acknowledge the utility of the human spirit,

2:31:30.000 --> 2:31:33.000
 and I'm wondering if Optimus will ever get a personality

2:31:33.000 --> 2:31:38.000
 and be able to laugh at our jokes while it folds our clothes.

2:31:38.000 --> 2:31:40.000
 Yeah, absolutely.

2:31:40.000 --> 2:31:46.000
 I think we want to have really fun versions of Optimus

2:31:46.000 --> 2:31:52.000
 so that Optimus can both be utilitarian and do tasks,

2:31:52.000 --> 2:31:59.000
 but can also be kind of like a friend and a buddy and hang out with you.

2:31:59.000 --> 2:32:05.000
 And I'm sure people will think of all sorts of creative uses for this robot.

2:32:05.000 --> 2:32:12.000
 And once you have the core intelligence and actuators figured out,

2:32:12.000 --> 2:32:21.000
 then you can actually put all sorts of costumes, I guess, on the robot.

2:32:21.000 --> 2:32:29.000
 I mean, you can skin the robot in many different ways,

2:32:29.000 --> 2:32:34.000
 and I'm sure people will find very interesting ways to, yeah,

2:32:34.000 --> 2:32:36.000
 versions of Optimus.

2:32:40.000 --> 2:32:42.000
 Thanks for the great presentation.

2:32:42.000 --> 2:32:47.000
 I wanted to know if there was an equivalent to interventions in Optimus.

2:32:47.000 --> 2:32:50.000
 It seems like labeling through moments where humans disagree

2:32:50.000 --> 2:32:54.000
 with what's going on is important, and in a humanoid robot,

2:32:54.000 --> 2:32:58.000
 that might be also a desirable source of information.

2:33:06.000 --> 2:33:10.000
 Yeah, I think we'll have ways to remote operate the robot

2:33:10.000 --> 2:33:12.000
 and intervene when it does something bad,

2:33:12.000 --> 2:33:15.000
 especially when we are training the robot and bringing it up.

2:33:15.000 --> 2:33:19.000
 And hopefully we design it in a way that we can stop the robot

2:33:19.000 --> 2:33:22.000
 from, if it's going to hit something, we can just hold it,

2:33:22.000 --> 2:33:23.000
 and then it will stop.

2:33:23.000 --> 2:33:25.000
 It won't crush your hand or something.

2:33:25.000 --> 2:33:28.000
 And those are all intervention data.

2:33:28.000 --> 2:33:30.000
 And we can learn a lot from our simulation systems, too,

2:33:30.000 --> 2:33:35.000
 where we can check for collisions and supervise that those are bad actions.

2:33:35.000 --> 2:33:41.000
 Yeah, so Optimus, we want, over time, for it to be an Android,

2:33:41.000 --> 2:33:44.000
 the kind of Android that you see in sci-fi movies,

2:33:44.000 --> 2:33:47.000
 like Star Trek, The Next Generation, like Data.

2:33:47.000 --> 2:33:51.000
 But obviously we could program the robot to be less robot-like

2:33:51.000 --> 2:33:56.000
 and more friendly, and it can obviously learn to emulate humans

2:33:56.000 --> 2:33:58.000
 and feel very natural.

2:33:58.000 --> 2:34:04.000
 So as AI in general improves, we can add that to the robot,

2:34:04.000 --> 2:34:10.000
 and it should be obviously able to do simple instructions

2:34:10.000 --> 2:34:13.000
 or even intuit what it is that you want.

2:34:13.000 --> 2:34:16.000
 So you could give it a high-level instruction,

2:34:16.000 --> 2:34:19.000
 and then it can break that down into a series of actions

2:34:19.000 --> 2:34:25.000
 and take those actions.

2:34:25.000 --> 2:34:29.000
 Hi. Yeah, it's exciting to think that with the Optimus,

2:34:29.000 --> 2:34:32.000
 you will think that you can achieve orders of magnitude

2:34:32.000 --> 2:34:36.000
 of improvement in economic output.

2:34:36.000 --> 2:34:38.000
 That's really exciting.

2:34:38.000 --> 2:34:41.000
 And when Tesla started, the mission was to accelerate

2:34:41.000 --> 2:34:45.000
 the advent of renewable energy or sustainable transport.

2:34:45.000 --> 2:34:49.000
 So with the Optimus, do you still see that mission

2:34:49.000 --> 2:34:54.000
 being the mission statement of Tesla, or is it going to be updated

2:34:54.000 --> 2:34:59.000
 with mission to accelerate the advent of infinite abundance

2:34:59.000 --> 2:35:03.000
 or limitless economy?

2:35:03.000 --> 2:35:07.000
 Yeah, I mean, it is not strictly speaking,

2:35:07.000 --> 2:35:12.000
 Optimus is not strictly speaking directly in line

2:35:12.000 --> 2:35:15.000
 with accelerating sustainable energy.

2:35:15.000 --> 2:35:20.000
 To the degree that it is more efficient at getting things done

2:35:20.000 --> 2:35:25.000
 than a person, it does, I guess, help with sustainable energy.

2:35:25.000 --> 2:35:29.000
 But I think the mission effectively does somewhat broaden

2:35:29.000 --> 2:35:34.000
 with the advent of Optimus to, I don't know,

2:35:34.000 --> 2:35:36.000
 making the future awesome.

2:35:36.000 --> 2:35:40.000
 So I think you look at Optimus, and I don't know about you,

2:35:40.000 --> 2:35:44.000
 but I'm excited to see what Optimus will become.

2:35:44.000 --> 2:35:48.000
 And, you know, this is like, you know, if you could,

2:35:48.000 --> 2:35:52.000
 I mean, we can tell like any given technology,

2:35:52.000 --> 2:35:55.000
 do you want to see what it's like in a year, two years,

2:35:55.000 --> 2:35:58.000
 three years, four years, five years, ten?

2:35:58.000 --> 2:36:00.000
 I'd say for sure.

2:36:00.000 --> 2:36:03.000
 You definitely want to see what's happened with Optimus.

2:36:03.000 --> 2:36:05.000
 Whereas, you know, a bunch of other technologies

2:36:05.000 --> 2:36:09.000
 are, you know, sort of plateaued.

2:36:09.000 --> 2:36:11.000
 I don't want to name names here.

2:36:11.000 --> 2:36:21.000
 But, you know, so I think Optimus is going to be incredible

2:36:21.000 --> 2:36:24.000
 in like five years, ten years, like mind-blowing.

2:36:24.000 --> 2:36:26.000
 And I'm really interested to see that happen,

2:36:26.000 --> 2:36:29.000
 and I hope you are too.

2:36:29.000 --> 2:36:31.000
 Thank you.

2:36:31.000 --> 2:36:34.000
 I have a quick question here, Justin.

2:36:34.000 --> 2:36:37.000
 And I was wondering, like, are you planning to extend

2:36:37.000 --> 2:36:41.000
 like conversational capabilities for the robot?

2:36:41.000 --> 2:36:44.000
 And my second follow-up question to that is,

2:36:44.000 --> 2:36:46.000
 what's like the end goal?

2:36:46.000 --> 2:36:49.000
 What's the end goal with Optimus?

2:36:49.000 --> 2:36:52.000
 Yeah, Optimus would definitely have conversational capabilities.

2:36:52.000 --> 2:36:58.000
 So, you'd be able to talk to it and have a conversation,

2:36:58.000 --> 2:37:00.000
 and it would feel quite natural.

2:37:00.000 --> 2:37:05.000
 So, from an end goal standpoint, I'm, I don't know,

2:37:05.000 --> 2:37:07.000
 I think it's going to keep evolving,

2:37:07.000 --> 2:37:12.000
 and I'm not sure where it ends up,

2:37:12.000 --> 2:37:16.000
 but some place interesting for sure.

2:37:16.000 --> 2:37:19.000
 You know, we always have to be careful about the, you know,

2:37:19.000 --> 2:37:21.000
 don't go down the Terminator path.

2:37:21.000 --> 2:37:25.000
 That's a, you know, I thought maybe we should start off

2:37:25.000 --> 2:37:27.000
 with a video of like the Terminator starting off

2:37:27.000 --> 2:37:29.000
 with this, you know, skull-crushing,

2:37:29.000 --> 2:37:32.000
 but that might be, you know, people might take that too seriously.

2:37:32.000 --> 2:37:36.000
 So, you know, we do want Optimus to be safe,

2:37:36.000 --> 2:37:40.000
 so we are designing in safeguards

2:37:40.000 --> 2:37:44.000
 where you can locally stop the robot,

2:37:44.000 --> 2:37:50.000
 and, you know, with like basically a localized control ROM

2:37:50.000 --> 2:37:52.000
 that you can't update over the Internet,

2:37:52.000 --> 2:37:54.000
 which I think that's quite important.

2:37:54.000 --> 2:37:58.000
 It's essential, frankly.

2:37:58.000 --> 2:38:05.000
 So, like a localized stop button or remote control,

2:38:05.000 --> 2:38:12.000
 something like that, that cannot be changed.

2:38:12.000 --> 2:38:15.000
 But, I mean, it's definitely going to be interesting.

2:38:15.000 --> 2:38:22.000
 It won't be boring.

2:38:22.000 --> 2:38:23.000
 Okay, yeah.

2:38:23.000 --> 2:38:26.000
 I see today you have a very attractive product with Dojo

2:38:26.000 --> 2:38:28.000
 and its applications.

2:38:28.000 --> 2:38:31.000
 So, I'm wondering what's the future for the Dojo platform?

2:38:31.000 --> 2:38:35.000
 Will you like provide like infrastructure as service,

2:38:35.000 --> 2:38:39.000
 like AWS, or will you like sell the chip like the NVIDIA?

2:38:39.000 --> 2:38:41.000
 So, basically, what's the future?

2:38:41.000 --> 2:38:43.000
 Because I see you use 7 nanometers,

2:38:43.000 --> 2:38:47.000
 so the development cost is like easily over $10 million.

2:38:47.000 --> 2:38:51.000
 How do you make the business like business-wise?

2:38:51.000 --> 2:38:56.000
 Yeah, I mean, Dojo is a very big computer,

2:38:56.000 --> 2:39:00.000
 and actually will use a lot of power and need a lot of cooling.

2:39:00.000 --> 2:39:02.000
 So, I think it's probably going to make more sense

2:39:02.000 --> 2:39:06.000
 to have Dojo operate in like an Amazon Web Services manner

2:39:06.000 --> 2:39:10.000
 than to try to sell it to someone else.

2:39:10.000 --> 2:39:14.000
 So, that would be the most efficient way to operate Dojo

2:39:14.000 --> 2:39:19.000
 is just have it be a service that you can use

2:39:19.000 --> 2:39:21.000
 that's available online,

2:39:21.000 --> 2:39:24.000
 and that where you can train your models way faster

2:39:24.000 --> 2:39:26.000
 and for less money.

2:39:26.000 --> 2:39:34.000
 And as the world transitions to software 2.0...

2:39:34.000 --> 2:39:37.000
 And that's on the bingo card.

2:39:37.000 --> 2:39:41.000
 As someone who has to know how to drink five tequilas.

2:39:41.000 --> 2:39:45.000
 So, let's see.

2:39:45.000 --> 2:39:53.000
 Software 2.0 will use a lot of neural net training.

2:39:53.000 --> 2:39:58.000
 So, it kind of makes sense that over time,

2:39:58.000 --> 2:40:00.000
 as there's more neural net stuff,

2:40:00.000 --> 2:40:05.000
 people will want to use the fastest, lowest cost

2:40:05.000 --> 2:40:07.000
 neural net training system.

2:40:07.000 --> 2:40:14.000
 So, I think there's a lot of opportunity in that direction.

2:40:14.000 --> 2:40:17.000
 Hi. My name is Ali Jahanian.

2:40:17.000 --> 2:40:19.000
 Thank you for this event.

2:40:19.000 --> 2:40:21.000
 It's very inspirational.

2:40:21.000 --> 2:40:24.000
 My question is, I'm wondering,

2:40:24.000 --> 2:40:29.000
 what is your vision for human robots

2:40:29.000 --> 2:40:34.000
 that understand our emotions and art

2:40:34.000 --> 2:40:40.000
 and can contribute to our creativity?

2:40:40.000 --> 2:40:42.000
 Well, I think there's...

2:40:42.000 --> 2:40:45.000
 We're already seeing robots that at least

2:40:45.000 --> 2:40:48.000
 are able to generate very interesting art,

2:40:48.000 --> 2:40:53.000
 like Dully and Dully 2.

2:40:53.000 --> 2:40:58.000
 And I think we'll start seeing AI that can actually generate

2:40:58.000 --> 2:41:01.000
 even movies that have coherence,

2:41:01.000 --> 2:41:04.000
 like interesting movies and tell jokes.

2:41:04.000 --> 2:41:10.000
 So, it's quite remarkable how fast AI is advancing

2:41:10.000 --> 2:41:14.000
 at many companies besides Tesla.

2:41:14.000 --> 2:41:17.000
 We're headed for a very interesting future.

2:41:17.000 --> 2:41:22.000
 And, yeah. So, any guys want to comment on that?

2:41:22.000 --> 2:41:26.000
 I guess the optimus robot can come up with physical art,

2:41:26.000 --> 2:41:28.000
 not just digital art.

2:41:28.000 --> 2:41:31.000
 You can ask for some dance moves in text or voice,

2:41:31.000 --> 2:41:33.000
 and then you can produce those in the future.

2:41:33.000 --> 2:41:38.000
 So, it's a lot of physical art, not just digital art.

2:41:38.000 --> 2:41:41.000
 Oh, yeah. Computers can absolutely make physical art.

2:41:41.000 --> 2:41:42.000
 Yeah, 100%.

2:41:42.000 --> 2:41:45.000
 Yeah, like dance, play soccer, or whatever you...

2:41:45.000 --> 2:41:51.000
 It needs to get more agile, but over time, for sure.

2:41:51.000 --> 2:41:53.000
 Thanks so much for the presentation.

2:41:53.000 --> 2:41:56.000
 For the Tesla autopilot slides,

2:41:56.000 --> 2:41:58.000
 I noticed that the models that you were using

2:41:58.000 --> 2:42:01.000
 were heavily motivated by language models.

2:42:01.000 --> 2:42:03.000
 And I was wondering what the history of that was

2:42:03.000 --> 2:42:05.000
 and how much of an improvement it gave.

2:42:05.000 --> 2:42:07.000
 I thought that that was a really interesting, curious choice

2:42:07.000 --> 2:42:10.000
 to use language models for the lane transitioning.

2:42:10.000 --> 2:42:12.000
 So, there are sort of two aspects

2:42:12.000 --> 2:42:14.000
 for why we transitioned to language modeling.

2:42:14.000 --> 2:42:15.000
 So, the first...

2:42:15.000 --> 2:42:17.000
 Talk loud and close.

2:42:17.000 --> 2:42:18.000
 Okay.

2:42:18.000 --> 2:42:19.000
 It's not coming through very clearly.

2:42:19.000 --> 2:42:20.000
 Okay, got it.

2:42:20.000 --> 2:42:23.000
 Yeah, so the language models help us in two ways.

2:42:23.000 --> 2:42:25.000
 The first way is that it lets us predict lanes

2:42:25.000 --> 2:42:27.000
 that we couldn't have otherwise.

2:42:27.000 --> 2:42:29.000
 As Ashok mentioned earlier, basically,

2:42:29.000 --> 2:42:33.000
 when we predicted lanes in sort of a dense 3D fashion,

2:42:33.000 --> 2:42:35.000
 you can only model certain kinds of lanes,

2:42:35.000 --> 2:42:37.000
 but we want to get those crisscrossing connections

2:42:37.000 --> 2:42:38.000
 inside of intersections.

2:42:38.000 --> 2:42:39.000
 It's just not possible to do that

2:42:39.000 --> 2:42:41.000
 without making it a graph prediction.

2:42:41.000 --> 2:42:43.000
 If you try to do this with dense segmentation,

2:42:43.000 --> 2:42:45.000
 it just doesn't work.

2:42:45.000 --> 2:42:48.000
 Also, the lane prediction is a multimodal problem.

2:42:48.000 --> 2:42:51.000
 Sometimes you just don't have sufficient visual information

2:42:51.000 --> 2:42:52.000
 to know precisely how things look

2:42:52.000 --> 2:42:54.000
 on the other side of the intersection.

2:42:54.000 --> 2:42:56.000
 So, you need a method that can generalize

2:42:56.000 --> 2:42:59.000
 and produce coherent predictions.

2:42:59.000 --> 2:43:01.000
 You don't want to be predicting two lanes

2:43:01.000 --> 2:43:02.000
 and three lanes at the same time.

2:43:02.000 --> 2:43:03.000
 You want to commit to one.

2:43:03.000 --> 2:43:06.000
 And a general model like these language models provides that.

2:43:10.000 --> 2:43:12.000
 Hi.

2:43:12.000 --> 2:43:15.000
 Hi, my name is Giovanni.

2:43:15.000 --> 2:43:17.000
 Yeah, thanks for the presentation.

2:43:17.000 --> 2:43:19.000
 That's really nice.

2:43:19.000 --> 2:43:21.000
 I have a question for FSD team.

2:43:21.000 --> 2:43:27.000
 So, for the neural networks, how do you test,

2:43:27.000 --> 2:43:31.000
 how do you do unit tests, software unit tests on that?

2:43:31.000 --> 2:43:34.000
 Like, do you have a bunch or, I don't know,

2:43:34.000 --> 2:43:40.000
 thousands of cases where the neural network,

2:43:40.000 --> 2:43:43.000
 after you train it, you have to pass it

2:43:43.000 --> 2:43:46.000
 before you release it as a product, right?

2:43:46.000 --> 2:43:49.000
 Yeah, what's your software unit testing strategies

2:43:49.000 --> 2:43:50.000
 for this, basically?

2:43:50.000 --> 2:43:51.000
 Yeah, glad you asked.

2:43:51.000 --> 2:43:54.000
 There's a series of tests that we have defined,

2:43:54.000 --> 2:43:56.000
 starting from unit tests for the software itself,

2:43:56.000 --> 2:43:58.000
 but then for the neural network models,

2:43:58.000 --> 2:44:02.000
 we have VIP sets defined where you can define,

2:44:02.000 --> 2:44:04.000
 if you just have a large test set,

2:44:04.000 --> 2:44:05.000
 that's not enough what we find.

2:44:05.000 --> 2:44:10.000
 We need sophisticated VIP sets for different failure modes.

2:44:10.000 --> 2:44:12.000
 And then we curate them and grow them

2:44:12.000 --> 2:44:13.000
 over the time of the product.

2:44:13.000 --> 2:44:16.000
 So, over the years, we have hundreds of thousands

2:44:16.000 --> 2:44:19.000
 of examples where we have been failing in the past

2:44:19.000 --> 2:44:20.000
 that we have curated.

2:44:20.000 --> 2:44:24.000
 And so, for any new model, we test against the entire history

2:44:24.000 --> 2:44:28.000
 of these failures and then keep adding to this test set.

2:44:28.000 --> 2:44:30.000
 On top of this, we have shadow modes

2:44:30.000 --> 2:44:33.000
 where we ship these models in silent to the car,

2:44:33.000 --> 2:44:36.000
 and we get data back on where they are failing or succeeding,

2:44:36.000 --> 2:44:39.000
 and there's an extensive QA program.

2:44:39.000 --> 2:44:41.000
 It's very hard to ship a regression.

2:44:41.000 --> 2:44:43.000
 There's like nine levels of filters

2:44:43.000 --> 2:44:46.000
 before it hits customers, but then we have really good infra

2:44:46.000 --> 2:44:49.000
 to make this all efficient.

2:44:49.000 --> 2:44:52.000
 I'm one of the QA testers, so I curate the car.

2:44:52.000 --> 2:44:54.000
 Yeah, QA tester.

2:44:54.000 --> 2:44:55.000
 Yeah.

2:44:55.000 --> 2:44:59.000
 So, I'm constantly in the car just being QA-ing

2:44:59.000 --> 2:45:04.000
 whatever the latest alpha build is that doesn't totally crash.

2:45:04.000 --> 2:45:08.000
 Finds a lot of bugs.

2:45:08.000 --> 2:45:09.000
 Hi.

2:45:09.000 --> 2:45:10.000
 Great event.

2:45:10.000 --> 2:45:13.000
 I have a question about foundational models

2:45:13.000 --> 2:45:14.000
 for autonomous driving.

2:45:14.000 --> 2:45:18.000
 We have all seen that big models that really can,

2:45:18.000 --> 2:45:21.000
 when you scale up with data and model parameter

2:45:21.000 --> 2:45:25.000
 from GPT-3 to POM, it can actually now do reasoning.

2:45:25.000 --> 2:45:30.000
 Do you see that it's essential scaling up foundational models

2:45:30.000 --> 2:45:35.000
 with data and size, and then at least you can get a teacher model

2:45:35.000 --> 2:45:38.000
 that potentially can solve all the problems,

2:45:38.000 --> 2:45:41.000
 and then you distill to a student model.

2:45:41.000 --> 2:45:46.000
 Is that how you see foundational models relevant for autonomous driving?

2:45:46.000 --> 2:45:48.000
 That's quite similar to our auto labeling models.

2:45:48.000 --> 2:45:51.000
 So, we don't just have models that run in the car.

2:45:51.000 --> 2:45:53.000
 We train models that are entirely offline,

2:45:53.000 --> 2:45:57.000
 that are extremely large, that can't run in real time on the car.

2:45:57.000 --> 2:45:59.000
 So, we just run those offline on our servers,

2:45:59.000 --> 2:46:05.000
 producing really good labels that can then train the online networks.

2:46:05.000 --> 2:46:10.000
 So, that's one form of distillation of these teacher student models.

2:46:10.000 --> 2:46:12.000
 In terms of foundational models, we are building some really,

2:46:12.000 --> 2:46:16.000
 really large data sets that are multiple petabytes,

2:46:16.000 --> 2:46:19.000
 and we are seeing that some of these tasks work really well

2:46:19.000 --> 2:46:21.000
 when we have these large data sets.

2:46:21.000 --> 2:46:23.000
 Like kinematics, like I mentioned, video in,

2:46:23.000 --> 2:46:25.000
 all the kinematics out of all the objects,

2:46:25.000 --> 2:46:27.000
 and up to the fourth derivative,

2:46:27.000 --> 2:46:30.000
 and people thought we couldn't do detection with cameras.

2:46:30.000 --> 2:46:32.000
 Detection, depth, velocity, acceleration,

2:46:32.000 --> 2:46:35.000
 and imagine how precise these have to be

2:46:35.000 --> 2:46:38.000
 for these higher order derivatives to be accurate,

2:46:38.000 --> 2:46:42.000
 and this all comes from these large data sets and large models.

2:46:42.000 --> 2:46:44.000
 So, we're seeing the equivalent of foundational models

2:46:44.000 --> 2:46:50.000
 in our own way for geometry and kinematics and things like those.

2:46:50.000 --> 2:46:53.000
 Do you want to add anything, John?

2:46:53.000 --> 2:46:54.000
 Yeah, I'll keep it brief.

2:46:54.000 --> 2:46:58.000
 Basically, whenever we train on a larger data set, we see big...

2:46:58.000 --> 2:46:59.000
 Okay.

2:46:59.000 --> 2:47:01.000
 Basically, whenever we train on a larger data set,

2:47:01.000 --> 2:47:03.000
 we see big improvements in our model performance,

2:47:03.000 --> 2:47:06.000
 and basically, whenever we initialize our networks with,

2:47:06.000 --> 2:47:09.000
 you know, some pre-training step from some other auxiliary task,

2:47:09.000 --> 2:47:11.000
 we basically see improvements.

2:47:11.000 --> 2:47:18.000
 So, self-supervised or supervised with large data sets both help a lot.

2:47:18.000 --> 2:47:19.000
 Hi.

2:47:19.000 --> 2:47:20.000
 So, at the beginning,

2:47:20.000 --> 2:47:22.000
 Elon said that Tesla was potentially interested

2:47:22.000 --> 2:47:25.000
 in building artificial general intelligence systems.

2:47:25.000 --> 2:47:29.000
 Given the potentially transformative impact of technology like that,

2:47:29.000 --> 2:47:34.000
 it seems prudent to invest in technical AGI safety expertise specifically.

2:47:34.000 --> 2:47:39.000
 I know Tesla does a lot of technical narrow AI safety research.

2:47:39.000 --> 2:47:43.000
 I was curious if Tesla was intending to try to build expertise

2:47:43.000 --> 2:47:48.000
 in technical artificial general intelligence safety specifically.

2:47:48.000 --> 2:47:53.000
 Well, I mean, if we start looking like we're going to be making

2:47:53.000 --> 2:47:56.000
 a significant contribution to artificial general intelligence,

2:47:56.000 --> 2:47:59.000
 then we'll for sure invest in safety.

2:47:59.000 --> 2:48:01.000
 I'm a big believer in AI safety.

2:48:01.000 --> 2:48:06.000
 I think there should be an AI sort of regulatory authority

2:48:06.000 --> 2:48:10.000
 at the government level just as there is a regulatory authority

2:48:10.000 --> 2:48:13.000
 for anything that affects public safety.

2:48:13.000 --> 2:48:16.000
 So, we have a regulatory authority for aircraft and cars

2:48:16.000 --> 2:48:21.000
 and sort of food and drugs because they affect public safety,

2:48:21.000 --> 2:48:23.000
 and AI also affects public safety.

2:48:23.000 --> 2:48:26.000
 So, I think, and this is not really something

2:48:26.000 --> 2:48:28.000
 a government I think understands yet,

2:48:28.000 --> 2:48:33.000
 but I think there should be a referee that is ensuring

2:48:33.000 --> 2:48:37.000
 trying to ensure public safety for AGI.

2:48:37.000 --> 2:48:42.000
 And you think of like, well, what are the elements

2:48:42.000 --> 2:48:44.000
 that are necessary to create AGI?

2:48:44.000 --> 2:48:50.000
 Like the accessible data set is extremely important,

2:48:50.000 --> 2:48:55.000
 and if you've got a large number of cars and humanoid robots

2:48:55.000 --> 2:49:02.000
 processing petabytes of video data and audio data

2:49:02.000 --> 2:49:05.000
 from the real world, just like humans,

2:49:05.000 --> 2:49:08.000
 that might be the biggest data set.

2:49:08.000 --> 2:49:11.000
 It probably is the biggest data set,

2:49:11.000 --> 2:49:14.000
 because in addition to that, you can obviously incrementally

2:49:14.000 --> 2:49:18.000
 scan the Internet, but what the Internet can't quite do

2:49:18.000 --> 2:49:22.000
 is have millions or hundreds of millions of cameras

2:49:22.000 --> 2:49:29.000
 in the real world, like I said, with audio and other senses as well.

2:49:29.000 --> 2:49:34.000
 So, I think we probably will have the most amount of data,

2:49:34.000 --> 2:49:39.000
 and probably the most amount of training power.

2:49:39.000 --> 2:49:48.000
 Therefore, probably we will make a contribution to AGI.

2:49:48.000 --> 2:49:51.000
 Hey, I noticed the semi was back there,

2:49:51.000 --> 2:49:53.000
 but we haven't talked about it too much.

2:49:53.000 --> 2:49:55.000
 I was just wondering for the semi truck,

2:49:55.000 --> 2:49:58.000
 what are the changes you're thinking about

2:49:58.000 --> 2:50:00.000
 from a sensing perspective?

2:50:00.000 --> 2:50:02.000
 I imagine there's very different requirements, obviously,

2:50:02.000 --> 2:50:05.000
 than just a car, and if you don't think that's true,

2:50:05.000 --> 2:50:07.000
 why is that true?

2:50:07.000 --> 2:50:10.000
 No, I think basically you can drive a car.

2:50:10.000 --> 2:50:13.000
 I mean, think about what drives any vehicle.

2:50:13.000 --> 2:50:19.000
 It's a biological neural net with eyes, with cameras, essentially.

2:50:19.000 --> 2:50:28.000
 And really, your primary sensors are two cameras on a slow gimbal,

2:50:28.000 --> 2:50:30.000
 a very slow gimbal.

2:50:30.000 --> 2:50:32.000
 That's your head.

2:50:32.000 --> 2:50:38.000
 So, if a biological neural net with two cameras on a slow gimbal

2:50:38.000 --> 2:50:42.000
 can drive a semi truck, then if you've got like eight cameras

2:50:42.000 --> 2:50:45.000
 with continuous 360 degree vision,

2:50:45.000 --> 2:50:48.000
 operating at a higher frame rate and much higher reaction rate,

2:50:48.000 --> 2:50:51.000
 then I think it is obvious that you should be able to drive a semi

2:50:51.000 --> 2:50:56.000
 or any vehicle much better than human.

2:50:56.000 --> 2:50:58.000
 Hi, my name is Akshay.

2:50:58.000 --> 2:51:00.000
 Thank you for the event.

2:51:00.000 --> 2:51:04.000
 Assuming Optimus would be used for different use cases

2:51:04.000 --> 2:51:08.000
 and would evolve at different pace for these use cases,

2:51:08.000 --> 2:51:12.000
 would it be possible to sort of develop and deploy

2:51:12.000 --> 2:51:16.000
 different software and hardware components independently

2:51:16.000 --> 2:51:21.000
 and deploy them in Optimus

2:51:21.000 --> 2:51:28.000
 so that the overall feature development is faster for Optimus?

2:51:28.000 --> 2:51:31.000
 I'm trying to see the question.

2:51:31.000 --> 2:51:32.000
 Okay, all right.

2:51:32.000 --> 2:51:33.000
 We did not comprehend.

2:51:33.000 --> 2:51:39.000
 Unfortunately, our neural net did not comprehend the question.

2:51:39.000 --> 2:51:44.000
 So, next question.

2:51:44.000 --> 2:51:46.000
 Hi, I want to switch gear to the autopilot.

2:51:46.000 --> 2:51:51.000
 So, when you guys plan to roll out the FSD beta to countries

2:51:51.000 --> 2:51:55.000
 other than U.S. and Canada, and also my next question is

2:51:55.000 --> 2:51:58.000
 what's the biggest bottleneck or the technological barrier

2:51:58.000 --> 2:52:00.000
 you think in the current autopilot stack

2:52:00.000 --> 2:52:04.000
 and how you envision to solve that to make the autopilot

2:52:04.000 --> 2:52:08.000
 is considerably better than human in terms of performance matrix

2:52:08.000 --> 2:52:11.000
 like safety assurance and the human confidence?

2:52:11.000 --> 2:52:14.000
 I think you also mentioned for FSD V11,

2:52:14.000 --> 2:52:17.000
 you guys are going to combine the highway and the city

2:52:17.000 --> 2:52:21.000
 as a single stack and some architectural big improvements.

2:52:21.000 --> 2:52:23.000
 Can you maybe expand a bit on that?

2:52:23.000 --> 2:52:24.000
 Thank you.

2:52:24.000 --> 2:52:29.000
 Well, that's a whole bunch of questions.

2:52:29.000 --> 2:52:33.000
 We're hopeful to be able to, I think from a technical standpoint,

2:52:33.000 --> 2:52:39.000
 FSD beta should be possible to roll out FSD beta worldwide

2:52:39.000 --> 2:52:43.000
 by the end of this year.

2:52:43.000 --> 2:52:47.000
 But for a lot of countries we need regulatory approval

2:52:47.000 --> 2:52:50.000
 and so we are somewhat gated by the regulatory approval

2:52:50.000 --> 2:52:54.000
 in other countries.

2:52:54.000 --> 2:52:57.000
 But I think from a technical standpoint it will be ready

2:52:57.000 --> 2:53:02.000
 to go to a worldwide beta by the end of this year.

2:53:02.000 --> 2:53:05.000
 And there's quite a big improvement that we're expecting

2:53:05.000 --> 2:53:07.000
 to release next month.

2:53:07.000 --> 2:53:13.000
 That will be especially good at assessing the velocity

2:53:13.000 --> 2:53:16.000
 of fast-moving cross traffic and a bunch of other things.

2:53:16.000 --> 2:53:22.000
 So anyone want to elaborate?

2:53:22.000 --> 2:53:23.000
 Yeah, I guess so.

2:53:23.000 --> 2:53:25.000
 There used to be a lot of differences between

2:53:25.000 --> 2:53:27.000
 production autopilot and the full self-driving beta,

2:53:27.000 --> 2:53:29.000
 but those differences have been getting smaller

2:53:29.000 --> 2:53:31.000
 and smaller over time.

2:53:31.000 --> 2:53:34.000
 I think just a few months ago we now use the same vision-only

2:53:34.000 --> 2:53:38.000
 object detection stack in both FSD and in the production

2:53:38.000 --> 2:53:40.000
 autopilot on all vehicles.

2:53:40.000 --> 2:53:43.000
 There's still a few differences, the primary one being the way

2:53:43.000 --> 2:53:45.000
 that we predict lanes right now.

2:53:45.000 --> 2:53:47.000
 So we upgraded the modeling of lanes so that it could handle

2:53:47.000 --> 2:53:50.000
 these more complex geometries like I mentioned in the talk.

2:53:50.000 --> 2:53:54.000
 In production autopilot we still use a simpler lane model,

2:53:54.000 --> 2:53:58.000
 but we're extending our current FSD beta models to work

2:53:58.000 --> 2:54:01.000
 in all sort of highway scenarios as well.

2:54:01.000 --> 2:54:04.000
 Yeah, and the version of FSD beta that I drive actually

2:54:04.000 --> 2:54:06.000
 does have the integrated stack.

2:54:06.000 --> 2:54:11.000
 So it uses the FSD stack both in city streets and highway,

2:54:11.000 --> 2:54:14.000
 and it works quite well for me.

2:54:14.000 --> 2:54:17.000
 But we need to validate it in all kinds of weather,

2:54:17.000 --> 2:54:23.000
 like heavy rain, snow, dust, and just make sure it's working

2:54:23.000 --> 2:54:27.000
 better than the production stack across a wide range

2:54:27.000 --> 2:54:29.000
 of environments.

2:54:29.000 --> 2:54:32.000
 But we're pretty close to that.

2:54:32.000 --> 2:54:36.000
 I mean, I think it's, I don't know, maybe,

2:54:36.000 --> 2:54:38.000
 it'll definitely be before the end of the year,

2:54:38.000 --> 2:54:41.000
 and maybe November.

2:54:41.000 --> 2:54:44.000
 Yeah, in our personal drives, the FSD stack on highway drives

2:54:44.000 --> 2:54:46.000
 already way better than the production stack we have.

2:54:46.000 --> 2:54:50.000
 And we do expect to also include the parking lot stack

2:54:50.000 --> 2:54:53.000
 as a part of the FSD stack before the end of this year.

2:54:53.000 --> 2:54:56.000
 So that will basically bring us to, you sit in the car

2:54:56.000 --> 2:54:59.000
 in the parking lot and drive to the end of the parking lot

2:54:59.000 --> 2:55:02.000
 at a parking spot before the end of this year.

2:55:02.000 --> 2:55:06.000
 Yeah, and in terms of the, like, the fundamental metric

2:55:06.000 --> 2:55:10.000
 to optimize against is how many miles between

2:55:10.000 --> 2:55:12.000
 a necessary intervention.

2:55:12.000 --> 2:55:19.000
 So just massively improving how many miles the car can drive

2:55:19.000 --> 2:55:22.000
 in full autonomy before an intervention is required

2:55:22.000 --> 2:55:25.000
 that is safety critical.

2:55:25.000 --> 2:55:29.000
 So, yeah, that's the fundamental metric

2:55:29.000 --> 2:55:31.000
 that we're measuring every week,

2:55:31.000 --> 2:55:36.000
 and we're making radical improvements on that.

2:55:36.000 --> 2:55:38.000
 Oh, hi. Thank you.

2:55:38.000 --> 2:55:40.000
 Hi. Thank you so much for the presentation.

2:55:40.000 --> 2:55:42.000
 Very inspiring.

2:55:42.000 --> 2:55:43.000
 My name is Daisy.

2:55:43.000 --> 2:55:46.000
 I actually have a non-technical question for you.

2:55:46.000 --> 2:55:50.000
 I'm curious if you are back to your 20s,

2:55:50.000 --> 2:55:53.000
 what are some of the things you wish you knew back then?

2:55:53.000 --> 2:56:07.000
 What are some advice you would give to your younger self?

2:56:07.000 --> 2:56:14.000
 Well, I'm trying to figure out something useful to say.

2:56:14.000 --> 2:56:20.000
 Yeah, join Tesla would be one thing.

2:56:20.000 --> 2:56:24.000
 Yeah, I think just trying to try to expose yourself

2:56:24.000 --> 2:56:28.000
 to as many smart people as possible.

2:56:28.000 --> 2:56:34.000
 I don't read a lot of books.

2:56:34.000 --> 2:56:37.000
 You know, I did do that, though.

2:56:37.000 --> 2:56:44.000
 So I think there's some merit to just also, like,

2:56:44.000 --> 2:56:48.000
 not being, like, necessarily too intense

2:56:48.000 --> 2:56:51.000
 and, like, enjoying the moment a bit more,

2:56:51.000 --> 2:56:54.000
 I would say to 20-something me,

2:56:54.000 --> 2:56:58.000
 just, you know, stop and smell the roses occasionally

2:56:58.000 --> 2:57:02.000
 would probably be a good idea.

2:57:02.000 --> 2:57:07.000
 You know, it's like when we were developing the Falcon 1 rocket

2:57:07.000 --> 2:57:10.000
 on the Kwajalein Atoll,

2:57:10.000 --> 2:57:12.000
 and we had this beautiful little island

2:57:12.000 --> 2:57:14.000
 that we were developing the rocket on,

2:57:14.000 --> 2:57:16.000
 and not once during that entire time

2:57:16.000 --> 2:57:18.000
 did I even have a drink on the beach.

2:57:18.000 --> 2:57:21.000
 I'm like, I should have had a drink on the beach.

2:57:21.000 --> 2:57:26.000
 That would have been fine.

2:57:26.000 --> 2:57:27.000
 Thank you very much.

2:57:27.000 --> 2:57:32.000
 I think you have excited all of the robotics people with Optimus.

2:57:32.000 --> 2:57:35.000
 This feels very much like 10 years ago in driving,

2:57:35.000 --> 2:57:38.000
 but as driving has proved to be harder

2:57:38.000 --> 2:57:40.000
 than it actually looked 10 years ago,

2:57:40.000 --> 2:57:43.000
 what do we know now that we didn't 10 years ago

2:57:43.000 --> 2:57:49.000
 that would make, for example, AGI on a humanoid come faster?

2:57:49.000 --> 2:57:55.000
 Well, I mean, it seems to me that AGI is advancing very quickly.

2:57:55.000 --> 2:58:00.000
 Hardly a week goes by without some significant announcement.

2:58:00.000 --> 2:58:05.000
 And, yeah, I mean, at this point,

2:58:05.000 --> 2:58:11.000
 like, AI seems to be able to win at almost any rule-based game.

2:58:11.000 --> 2:58:18.000
 It's able to create extremely impressive art,

2:58:18.000 --> 2:58:24.000
 engage in conversations that are very sophisticated,

2:58:24.000 --> 2:58:31.000
 write essays, and these just keep improving.

2:58:31.000 --> 2:58:37.000
 And there's so many more talented people working on AI,

2:58:37.000 --> 2:58:39.000
 and the hardware is getting better.

2:58:39.000 --> 2:58:46.000
 I think AI is on a strong exponential curve of improvement,

2:58:46.000 --> 2:58:50.000
 independent of what we do at Tesla.

2:58:50.000 --> 2:58:53.000
 And, obviously, we'll benefit somewhat from that exponential curve

2:58:53.000 --> 2:58:57.000
 of improvement with AI.

2:58:57.000 --> 2:59:00.000
 Tesla just also happens to be very good at actuators,

2:59:00.000 --> 2:59:06.000
 at motors, gearboxes, controllers, power electronics, batteries,

2:59:06.000 --> 2:59:12.000
 sensors, and, you know, really, I'd say the biggest difference

2:59:12.000 --> 2:59:16.000
 between the robot on four wheels and the robot with arms and legs

2:59:16.000 --> 2:59:19.000
 is getting the actuators right.

2:59:19.000 --> 2:59:23.000
 It's an actuators and sensors problem.

2:59:23.000 --> 2:59:27.000
 And, obviously, how you control those actuators and sensors,

2:59:27.000 --> 2:59:33.000
 but it's, yeah, actuators and sensors and how you control the actuators.

2:59:33.000 --> 2:59:36.000
 I don't know, we happen to have the ingredients necessary

2:59:36.000 --> 2:59:42.000
 to create a compelling robot, and we're doing it.

2:59:42.000 --> 2:59:44.000
 Hi, Ilan.

2:59:44.000 --> 2:59:47.000
 You are actually bringing the humanity to the next level.

2:59:47.000 --> 2:59:51.000
 Literally, Tesla and you are bringing the humanity to the next level.

2:59:51.000 --> 2:59:57.000
 So, you said Optimus Prime, Optimus will be used in the next Tesla factory.

2:59:57.000 --> 3:00:04.000
 My question is, will a new Tesla factory will be fully run by Optimus program?

3:00:04.000 --> 3:00:10.000
 And when can general public order a humanoid?

3:00:10.000 --> 3:00:13.000
 Yeah, I think it'll, you know, we're going to start Optimus

3:00:13.000 --> 3:00:16.000
 with very simple tasks in the factory.

3:00:16.000 --> 3:00:19.000
 You know, like maybe just like loading a part, like you saw in the video,

3:00:19.000 --> 3:00:25.000
 loading a part, you know, carrying a part from one place to another

3:00:25.000 --> 3:00:32.000
 or loading a part into one of our more conventional robot cells to,

3:00:32.000 --> 3:00:35.000
 you know, that welds a body together.

3:00:35.000 --> 3:00:40.000
 So, we'll start, you know, just trying to, how do we make it useful at all?

3:00:40.000 --> 3:00:45.000
 And then gradually expand the number of situations where it's useful.

3:00:45.000 --> 3:00:50.000
 And I think that the number of situations where Optimus is useful

3:00:50.000 --> 3:00:56.000
 will grow exponentially, like really, really fast.

3:00:56.000 --> 3:01:02.000
 In terms of when people can order one, I don't know, I think it's not that far away.

3:01:02.000 --> 3:01:07.000
 Well, I think you mean when can people receive one?

3:01:07.000 --> 3:01:13.000
 So, I don't know, I'm like, I'd say probably within three years

3:01:13.000 --> 3:01:15.000
 and not more than five years.

3:01:15.000 --> 3:01:24.000
 Within three to five years, you could probably receive an Optimus.

3:01:24.000 --> 3:01:28.000
 I feel the best way to make the progress for AGI is to involve as many smart people

3:01:28.000 --> 3:01:30.000
 across the world as possible.

3:01:30.000 --> 3:01:34.000
 And given the size and resource of Tesla compared to robot companies

3:01:34.000 --> 3:01:37.000
 and given the state of human research at the moment,

3:01:37.000 --> 3:01:41.000
 would it make sense for the kind of Tesla to sort of open source

3:01:41.000 --> 3:01:44.000
 some of the simulation hardware parts?

3:01:44.000 --> 3:01:47.000
 I think Tesla can still be the dominant platformer,

3:01:47.000 --> 3:01:51.000
 where it can be something like Android OS or iOS stuff

3:01:51.000 --> 3:01:53.000
 for the entire human research.

3:01:53.000 --> 3:01:56.000
 Would that be something that, rather than keeping the Optimus

3:01:56.000 --> 3:02:00.000
 to just Tesla researchers or the factory itself,

3:02:00.000 --> 3:02:10.000
 can you open it and let the whole world explore human research?

3:02:10.000 --> 3:02:14.000
 I think we have to be careful about Optimus being potentially used

3:02:14.000 --> 3:02:20.000
 in ways that are bad, because that is one of the possible things to do.

3:02:20.000 --> 3:02:30.000
 So, I think we'd provide Optimus where you can provide instructions to Optimus,

3:02:30.000 --> 3:02:37.000
 but where those instructions are governed by some laws of robotics

3:02:37.000 --> 3:02:41.000
 you cannot overcome.

3:02:41.000 --> 3:02:47.000
 So, not doing harm to others.

3:02:47.000 --> 3:02:53.000
 I think probably quite a few safety-related things with Optimus.

3:02:53.000 --> 3:02:59.000
 We'll just take maybe a few more questions, and then thank you all for coming.

3:02:59.000 --> 3:03:04.000
 Questions, one deep and one broad.

3:03:04.000 --> 3:03:09.000
 For Optimus, what's the current and what's the ideal controller bandwidth?

3:03:09.000 --> 3:03:11.000
 And then in the broader question,

3:03:11.000 --> 3:03:15.000
 there's this big advertisement for the depth and breadth of the company.

3:03:15.000 --> 3:03:21.000
 What is it uniquely about Tesla that enables that?

3:03:21.000 --> 3:03:25.000
 Anyone want to tackle the bandwidth question?

3:03:25.000 --> 3:03:27.000
 So, the technical bandwidth of the...

3:03:27.000 --> 3:03:29.000
 Close to your mouth and mouth.

3:03:29.000 --> 3:03:30.000
 Okay.

3:03:30.000 --> 3:03:33.000
 For the bandwidth question, you have to understand or figure out

3:03:33.000 --> 3:03:36.000
 what is the task that you want it to do.

3:03:36.000 --> 3:03:39.000
 And if you took a frequency transform of that task,

3:03:39.000 --> 3:03:41.000
 what is it that you want your limbs to do?

3:03:41.000 --> 3:03:43.000
 And that's where you get your bandwidth from.

3:03:43.000 --> 3:03:45.000
 It's not a number that you can specifically just say.

3:03:45.000 --> 3:03:47.000
 You need to understand your use case.

3:03:47.000 --> 3:03:50.000
 And that's where the bandwidth comes from.

3:03:50.000 --> 3:03:52.000
 What was the broad question?

3:03:52.000 --> 3:03:54.000
 I don't quite remember.

3:03:54.000 --> 3:03:55.000
 The breadth and depth thing.

3:03:55.000 --> 3:03:57.000
 I can answer the breadth and depth question.

3:03:57.000 --> 3:04:05.000
 We'll just...

3:04:05.000 --> 3:04:07.000
 I mean, seriously, on the bandwidth question,

3:04:07.000 --> 3:04:10.000
 I think we probably will just end up increasing the bandwidth

3:04:10.000 --> 3:04:15.000
 or, you know, which translates to the effect of dexterity

3:04:15.000 --> 3:04:19.000
 and reaction time of the robot.

3:04:19.000 --> 3:04:23.000
 Like, it's safe to say it's not one hertz.

3:04:23.000 --> 3:04:27.000
 And maybe you don't need to go all the way to 100 hertz,

3:04:27.000 --> 3:04:31.000
 but maybe 10, 25, I don't know.

3:04:31.000 --> 3:04:35.000
 Over time, I think the bandwidth will increase quite a bit

3:04:35.000 --> 3:04:39.000
 or translate it to dexterity and latency.

3:04:39.000 --> 3:04:42.000
 You'd want to minimize that over time.

3:04:42.000 --> 3:04:44.000
 Yeah.

3:04:44.000 --> 3:04:48.000
 Minimize latency, maximize dexterity.

3:04:48.000 --> 3:04:50.000
 In terms of breadth and depth,

3:04:50.000 --> 3:04:53.000
 I guess, you know, we've got...

3:04:53.000 --> 3:04:55.000
 We're a pretty big company at this point,

3:04:55.000 --> 3:04:57.000
 so we've got a lot of different areas of expertise

3:04:57.000 --> 3:05:00.000
 that we necessarily had to develop in order to make autonomous...

3:05:00.000 --> 3:05:02.000
 in order to make electric cars

3:05:02.000 --> 3:05:06.000
 and then in order to make autonomous electric cars.

3:05:06.000 --> 3:05:07.000
 We've just...

3:05:07.000 --> 3:05:11.000
 I mean, Tesla is like a whole series of startups, basically.

3:05:11.000 --> 3:05:17.000
 And so far, they've almost all been quite successful.

3:05:17.000 --> 3:05:20.000
 So we must be doing something right.

3:05:20.000 --> 3:05:24.000
 And, you know, I consider one of my core responsibilities

3:05:24.000 --> 3:05:27.000
 in running the company is to have an environment

3:05:27.000 --> 3:05:31.000
 where great engineers can flourish.

3:05:31.000 --> 3:05:33.000
 And I think in a lot of companies,

3:05:33.000 --> 3:05:36.000
 I don't know, maybe in most companies,

3:05:36.000 --> 3:05:39.000
 if somebody's a really talented, driven engineer,

3:05:39.000 --> 3:05:43.000
 they're unable to actually...

3:05:43.000 --> 3:05:47.000
 their talents are suppressed at a lot of companies.

3:05:47.000 --> 3:05:50.000
 And some of the companies,

3:05:50.000 --> 3:05:52.000
 the engineering talent is suppressed

3:05:52.000 --> 3:05:56.000
 in a way that is maybe not obviously bad,

3:05:56.000 --> 3:05:58.000
 but where it's just so comfortable

3:05:58.000 --> 3:06:00.000
 and you're paid so much money,

3:06:00.000 --> 3:06:04.000
 but the output you actually have to produce is so low

3:06:04.000 --> 3:06:06.000
 that it's like a honey trap.

3:06:06.000 --> 3:06:10.000
 So, like, there's a few honey trap places in Silicon Valley

3:06:10.000 --> 3:06:14.000
 where they don't necessarily seem like bad places for engineers,

3:06:14.000 --> 3:06:17.000
 but you have to say, like, a good engineer went in,

3:06:17.000 --> 3:06:19.000
 and what did they get out?

3:06:19.000 --> 3:06:26.000
 And the output of that engineering talent seems very low,

3:06:26.000 --> 3:06:29.000
 even though they seem to be enjoying themselves.

3:06:29.000 --> 3:06:30.000
 That's why I call it...

3:06:30.000 --> 3:06:32.000
 there's a few honey trap companies in Silicon Valley.

3:06:32.000 --> 3:06:34.000
 Tesla is not a honey trap.

3:06:34.000 --> 3:06:36.000
 We're demanding, and it's like,

3:06:36.000 --> 3:06:38.000
 we're going to get a lot of shit done,

3:06:38.000 --> 3:06:42.000
 and it's going to be really cool,

3:06:42.000 --> 3:06:44.000
 and it's not going to be easy.

3:06:44.000 --> 3:06:50.000
 But if you are a super-talented engineer,

3:06:50.000 --> 3:06:54.000
 your talents will be used, I think,

3:06:54.000 --> 3:06:59.000
 to a greater degree than anywhere else.

3:06:59.000 --> 3:07:00.000
 You know?

3:07:00.000 --> 3:07:04.000
 SpaceX also that way.

3:07:04.000 --> 3:07:09.000
 Hi, Lan. I have two questions, so both to the Autopilot team.

3:07:09.000 --> 3:07:11.000
 So the thing is, like, I have been following your progress

3:07:11.000 --> 3:07:14.000
 for the past few years, so today you have made changes

3:07:14.000 --> 3:07:16.000
 on, like, the lane detection.

3:07:16.000 --> 3:07:18.000
 You said that, like, previously you were doing instant

3:07:18.000 --> 3:07:19.000
 semantic segmentation.

3:07:19.000 --> 3:07:21.000
 Now you guys have built transfer models

3:07:21.000 --> 3:07:23.000
 for, like, building the lanes.

3:07:23.000 --> 3:07:25.000
 So what are some other common challenges

3:07:25.000 --> 3:07:27.000
 which you guys are facing right now,

3:07:27.000 --> 3:07:30.000
 like, which you are solving in future as a curious engineer

3:07:30.000 --> 3:07:33.000
 so that, like, we as a researcher can work on those,

3:07:33.000 --> 3:07:34.000
 start working on those?

3:07:34.000 --> 3:07:36.000
 And the second question is, like, I'm really curious

3:07:36.000 --> 3:07:37.000
 about the data engine.

3:07:37.000 --> 3:07:40.000
 Like, you guys have, like, told a case,

3:07:40.000 --> 3:07:42.000
 like, where the car is stopped.

3:07:42.000 --> 3:07:45.000
 So how are you finding cases which is very much similar

3:07:45.000 --> 3:07:47.000
 to that from the data which you have?

3:07:47.000 --> 3:07:50.000
 So a little bit more on the data engine would be great.

3:07:50.000 --> 3:07:51.000
 That's it.

3:07:51.000 --> 3:07:52.000
 Okay.

3:07:52.000 --> 3:07:53.000
 I'll start.

3:07:53.000 --> 3:07:55.000
 I'll answer the first question using occupancy network

3:07:55.000 --> 3:07:56.000
 as an example.

3:07:56.000 --> 3:07:59.000
 So what you saw in the presentation

3:07:59.000 --> 3:08:01.000
 did not exist a year ago.

3:08:01.000 --> 3:08:04.000
 So we only spent one year on time.

3:08:04.000 --> 3:08:06.000
 We actually shaped more than 12 occupancy network.

3:08:06.000 --> 3:08:10.000
 And to have a one foundation model actually

3:08:10.000 --> 3:08:14.000
 to represent the entire physical world around everywhere

3:08:14.000 --> 3:08:16.000
 and in all weather conditions is actually really,

3:08:16.000 --> 3:08:17.000
 really challenging.

3:08:17.000 --> 3:08:21.000
 So only over a year ago, we're kind of, like,

3:08:21.000 --> 3:08:22.000
 driving a 2D world.

3:08:22.000 --> 3:08:24.000
 If there's a wall and if there's a curve,

3:08:24.000 --> 3:08:27.000
 we kind of represent with the same static edge,

3:08:27.000 --> 3:08:30.000
 which is obviously, you know, not ideal, right?

3:08:30.000 --> 3:08:32.000
 There's a big difference between a curve and a wall.

3:08:32.000 --> 3:08:34.000
 When you drive, you make different choices, right?

3:08:34.000 --> 3:08:37.000
 So after we realized that we have to go to 3D,

3:08:37.000 --> 3:08:40.000
 we have to basically rethink the entire problem

3:08:40.000 --> 3:08:42.000
 and think about how we address that.

3:08:42.000 --> 3:08:45.000
 So this will be, like, one example of challenges

3:08:45.000 --> 3:08:51.000
 we have conquered in the past year.

3:08:51.000 --> 3:08:53.000
 Yeah, to answer the question about how we actually

3:08:53.000 --> 3:08:56.000
 source examples of the tricky stopped cars,

3:08:56.000 --> 3:08:58.000
 there's a few ways to go about this.

3:08:58.000 --> 3:08:59.000
 But two examples are, one,

3:08:59.000 --> 3:09:02.000
 we can trigger for disagreements within our signals.

3:09:02.000 --> 3:09:05.000
 So let's say that parked bit flickers between parked

3:09:05.000 --> 3:09:08.000
 and driving, we'll trigger that back.

3:09:08.000 --> 3:09:10.000
 And the second is we can leverage more of the shadow

3:09:10.000 --> 3:09:11.000
 mode logic.

3:09:11.000 --> 3:09:13.000
 So if the customer ignores the car,

3:09:13.000 --> 3:09:15.000
 but we think we should stop for it,

3:09:15.000 --> 3:09:16.000
 we'll get that data back too.

3:09:16.000 --> 3:09:19.000
 So these are just different, like, various trigger logic

3:09:19.000 --> 3:09:25.000
 that allows us to get those data campaigns back.

3:09:25.000 --> 3:09:26.000
 Hi.

3:09:26.000 --> 3:09:27.000
 Sorry, go ahead.

3:09:27.000 --> 3:09:29.000
 Thank you for the amazing presentation.

3:09:29.000 --> 3:09:30.000
 Thanks so much.

3:09:30.000 --> 3:09:33.000
 So there are a lot of companies that are focusing

3:09:33.000 --> 3:09:35.000
 on the AGI problem.

3:09:35.000 --> 3:09:37.000
 And one of the reasons why it's such a hard problem

3:09:37.000 --> 3:09:40.000
 is because the problem itself is so hard to define.

3:09:40.000 --> 3:09:42.000
 Several companies have several different definitions.

3:09:42.000 --> 3:09:44.000
 They focus on different things.

3:09:44.000 --> 3:09:47.000
 So how is Tesla defining the AGI problem,

3:09:47.000 --> 3:09:51.000
 and what are you focusing on specifically?

3:09:51.000 --> 3:09:55.000
 Well, we're not actually specifically focused on AGI.

3:09:55.000 --> 3:09:59.000
 I'm simply saying that AGI seems likely to be an emergent

3:09:59.000 --> 3:10:03.000
 property of what we're doing.

3:10:03.000 --> 3:10:06.000
 Because we're creating all these autonomous cars

3:10:06.000 --> 3:10:11.000
 and autonomous humanoids that are actually

3:10:11.000 --> 3:10:15.000
 within a truly gigantic data stream that's

3:10:15.000 --> 3:10:18.000
 coming in and being processed.

3:10:18.000 --> 3:10:21.000
 It's by far the most amount of real world data,

3:10:21.000 --> 3:10:24.000
 and data you can't get by just searching the internet,

3:10:24.000 --> 3:10:26.000
 because you have to be out there in the world,

3:10:26.000 --> 3:10:30.000
 and interacting with people, and interacting with the roads.

3:10:30.000 --> 3:10:33.000
 And just, you know, Earth is a big place,

3:10:33.000 --> 3:10:36.000
 and reality is messy and complicated.

3:10:36.000 --> 3:10:40.000
 So I think it's sort of like likely to just,

3:10:40.000 --> 3:10:42.000
 it just seems likely to be an emergent property

3:10:42.000 --> 3:10:45.000
 of if you've got tens or hundreds of millions

3:10:45.000 --> 3:10:48.000
 of autonomous vehicles, and maybe even a comparable

3:10:48.000 --> 3:10:50.000
 number of humanoids, maybe more than that

3:10:50.000 --> 3:10:52.000
 on the humanoid front.

3:10:52.000 --> 3:10:55.000
 Well, that's just the most amount of data.

3:10:55.000 --> 3:10:58.000
 And if that video is being processed,

3:10:58.000 --> 3:11:04.000
 it just seems likely that the cars will definitely

3:11:04.000 --> 3:11:06.000
 get way better than human drivers,

3:11:06.000 --> 3:11:12.000
 and the humanoid robots will become increasingly

3:11:12.000 --> 3:11:16.000
 indistinguishable from humans, perhaps.

3:11:16.000 --> 3:11:21.000
 And so then, like I said, you have this emergent property

3:11:21.000 --> 3:11:27.000
 of AGI.

3:11:27.000 --> 3:11:29.000
 And arguably, you know, humans collectively

3:11:29.000 --> 3:11:32.000
 are sort of a super intelligence as well,

3:11:32.000 --> 3:11:36.000
 especially as we improve the data rate between humans.

3:11:36.000 --> 3:11:38.000
 I mean, the thing like, that seems to be way back

3:11:38.000 --> 3:11:40.000
 in the early days, the internet was like,

3:11:40.000 --> 3:11:44.000
 the internet was like humanity acquiring a nervous system,

3:11:44.000 --> 3:11:48.000
 where now all of a sudden any one element of humanity

3:11:48.000 --> 3:11:51.000
 could know all of the knowledge of humans

3:11:51.000 --> 3:11:54.000
 by connecting to the internet, almost all the knowledge,

3:11:54.000 --> 3:11:56.000
 or certainly a huge part of it.

3:11:56.000 --> 3:11:58.000
 Whereas previously, we would exchange information

3:11:58.000 --> 3:12:01.000
 by osmosis.

3:12:01.000 --> 3:12:04.000
 Like, in order to transfer data, you

3:12:04.000 --> 3:12:05.000
 would have to write a letter.

3:12:05.000 --> 3:12:07.000
 Someone would have to carry the letter by person

3:12:07.000 --> 3:12:10.000
 to another person, and then a whole bunch of things

3:12:10.000 --> 3:12:11.000
 in between.

3:12:11.000 --> 3:12:16.000
 And then it was like, yeah, I mean,

3:12:16.000 --> 3:12:19.000
 it's insanely slow when you think about it.

3:12:19.000 --> 3:12:21.000
 And even if you were in the Library of Congress,

3:12:21.000 --> 3:12:24.000
 you still didn't have access to all the world's information.

3:12:24.000 --> 3:12:26.000
 And you certainly couldn't search it.

3:12:26.000 --> 3:12:29.000
 And obviously, very few people are

3:12:29.000 --> 3:12:30.000
 in the Library of Congress.

3:12:30.000 --> 3:12:39.000
 So I mean, one of the great sort of equality elements,

3:12:39.000 --> 3:12:43.000
 the internet has been the biggest equalizer in history

3:12:43.000 --> 3:12:48.000
 in terms of access to information and knowledge.

3:12:48.000 --> 3:12:51.000
 And any student of history, I think, would agree with this.

3:12:51.000 --> 3:12:56.000
 Because you go back 1,000 years, there were very few books.

3:12:56.000 --> 3:12:58.000
 And books would be incredibly expensive.

3:12:58.000 --> 3:13:00.000
 But only a few people knew how to read.

3:13:00.000 --> 3:13:04.000
 And even a small number of people even had a book.

3:13:04.000 --> 3:13:05.000
 Now look at it.

3:13:05.000 --> 3:13:08.000
 You can access any book instantly.

3:13:08.000 --> 3:13:11.000
 You can learn anything basically for free.

3:13:11.000 --> 3:13:13.000
 It's pretty incredible.

3:13:13.000 --> 3:13:22.000
 So I was asked recently, what period of history

3:13:22.000 --> 3:13:25.000
 would I prefer to be at the most?

3:13:25.000 --> 3:13:28.000
 And my answer was right now.

3:13:28.000 --> 3:13:31.000
 This is the most interesting time in history.

3:13:31.000 --> 3:13:33.000
 And I read a lot of history.

3:13:33.000 --> 3:13:37.000
 So let's do our best to keep that going.

3:13:37.000 --> 3:13:39.000
 Yeah.

3:13:39.000 --> 3:13:41.000
 And to go back to one of the earlier questions

3:13:41.000 --> 3:13:46.000
 I would ask, the thing that's happened over time with respect

3:13:46.000 --> 3:13:54.000
 to Tesla autopilot is that the neural nets have gradually

3:13:54.000 --> 3:13:56.000
 absorbed more and more software.

3:13:56.000 --> 3:13:59.000
 And in the limit, of course, you could simply

3:13:59.000 --> 3:14:03.000
 take the videos as seen by the car

3:14:03.000 --> 3:14:06.000
 and compare those to the steering inputs

3:14:06.000 --> 3:14:08.000
 from the steering wheel and pedals, which

3:14:08.000 --> 3:14:10.000
 are very simple inputs.

3:14:10.000 --> 3:14:13.000
 And in principle, you could train

3:14:13.000 --> 3:14:15.000
 with nothing in between, because that's

3:14:15.000 --> 3:14:18.000
 what humans are doing with a biological neural net,

3:14:18.000 --> 3:14:20.000
 you could train based on video.

3:14:20.000 --> 3:14:26.000
 And what trains the video is the moving of the steering

3:14:26.000 --> 3:14:30.000
 wheel and the pedals with no other software in between.

3:14:30.000 --> 3:14:32.000
 We're not there yet, but it's gradually

3:14:32.000 --> 3:14:33.000
 going in that direction.

3:14:33.000 --> 3:14:34.000
 All right.

3:14:34.000 --> 3:14:35.000
 What about the last question?

3:14:39.000 --> 3:14:41.000
 I think we've got a question at the front here.

3:14:41.000 --> 3:14:42.000
 Hello.

3:14:42.000 --> 3:14:43.000
 Right there.

3:14:43.000 --> 3:14:44.000
 We'll do two questions.

3:14:44.000 --> 3:14:47.000
 Fine.

3:14:47.000 --> 3:14:48.000
 Hi.

3:14:48.000 --> 3:14:49.000
 Thanks for such a great presentation.

3:14:49.000 --> 3:14:51.000
 We'll do your question last.

3:14:51.000 --> 3:14:52.000
 OK.

3:14:52.000 --> 3:14:53.000
 Cool.

3:14:53.000 --> 3:14:56.000
 With FSD being used by so many people,

3:14:56.000 --> 3:14:58.000
 how do you evaluate the company's risk tolerance

3:14:58.000 --> 3:15:01.000
 in terms of performance statistics?

3:15:01.000 --> 3:15:03.000
 And do you think there needs to be more transparency

3:15:03.000 --> 3:15:07.000
 or regulation from third parties as to what's good enough

3:15:07.000 --> 3:15:12.000
 in defining thresholds for performance

3:15:12.000 --> 3:15:15.000
 across so many miles?

3:15:15.000 --> 3:15:15.500
 Sure.

3:15:19.500 --> 3:15:26.000
 The number one design requirement at Tesla is safety.

3:15:26.000 --> 3:15:27.000
 And that goes across the board.

3:15:27.000 --> 3:15:31.000
 So in terms of the mechanical safety of the car,

3:15:31.000 --> 3:15:33.000
 we have the lowest probability of injury of any cars ever

3:15:33.000 --> 3:15:36.000
 tested by the government for just

3:15:36.000 --> 3:15:40.000
 a passive mechanical safety, essentially crash structure

3:15:40.000 --> 3:15:43.000
 and airbags and whatnot.

3:15:43.000 --> 3:15:49.000
 We have the highest rating for active safety as well.

3:15:49.000 --> 3:15:52.000
 And I think it's going to get to the point

3:15:52.000 --> 3:15:55.000
 where the active safety is so ridiculously good

3:15:55.000 --> 3:16:00.000
 it's just absurdly better than a human.

3:16:00.000 --> 3:16:03.000
 And then with respect to autopilot,

3:16:03.000 --> 3:16:06.000
 we do publish, broadly speaking, the statistics

3:16:06.000 --> 3:16:12.000
 on miles driven with cars that have no autonomy,

3:16:12.000 --> 3:16:16.000
 Tesla cars with no autonomy, with hardware one, hardware

3:16:16.000 --> 3:16:21.000
 two, hardware three, and then the ones that are in FSD beta.

3:16:21.000 --> 3:16:25.000
 And we see steady improvements all along the way.

3:16:25.000 --> 3:16:29.000
 Sometimes there's this dichotomy of,

3:16:29.000 --> 3:16:34.000
 should you wait until the car is three times safer

3:16:34.000 --> 3:16:36.000
 than a person before deploying any technology?

3:16:36.000 --> 3:16:39.000
 But I think that is actually morally wrong.

3:16:39.000 --> 3:16:41.000
 At the point at which you believe

3:16:41.000 --> 3:16:49.000
 that adding autonomy reduces injury and death,

3:16:49.000 --> 3:16:53.000
 I think you have a moral obligation to deploy it,

3:16:53.000 --> 3:16:57.000
 even though you're going to get sued and blamed

3:16:57.000 --> 3:17:00.000
 by a lot of people, because the people whose lives you saved

3:17:00.000 --> 3:17:02.000
 don't know that their lives are saved,

3:17:02.000 --> 3:17:06.000
 and the people who do occasionally die or get injured,

3:17:06.000 --> 3:17:09.000
 they definitely know, or their state does,

3:17:09.000 --> 3:17:13.000
 that there was a problem with autopilot.

3:17:13.000 --> 3:17:16.000
 That's why you have to look at the numbers

3:17:16.000 --> 3:17:20.000
 and sort of total miles driven, how many accidents occurred,

3:17:20.000 --> 3:17:23.000
 how many accidents were serious, how many fatalities.

3:17:23.000 --> 3:17:26.000
 And we've got well over three million cars on the road,

3:17:26.000 --> 3:17:29.000
 so that's a lot of miles driven every day.

3:17:29.000 --> 3:17:32.000
 It's not going to be perfect, but what matters

3:17:32.000 --> 3:17:38.000
 is that it is very clearly safer than not deploying it.

3:17:38.000 --> 3:17:39.000
 Yeah.

3:17:39.000 --> 3:17:41.000
 So I think last question.

3:17:41.000 --> 3:17:47.000
 I think, yeah.

3:17:47.000 --> 3:17:48.000
 Thanks.

3:17:48.000 --> 3:17:49.000
 Last question here.

3:17:49.000 --> 3:17:57.000
 Okay, hi.

3:17:57.000 --> 3:18:02.000
 So I do not work on hardware, so maybe the hardware team

3:18:02.000 --> 3:18:04.000
 and you guys can enlighten me.

3:18:04.000 --> 3:18:08.000
 Why is it required that there be symmetry

3:18:08.000 --> 3:18:11.000
 in the design of Optimus?

3:18:11.000 --> 3:18:15.000
 Because humans, we have handedness, right?

3:18:15.000 --> 3:18:18.000
 We use some set of muscles more than others.

3:18:18.000 --> 3:18:21.000
 Over time, there is wear and tear, right?

3:18:21.000 --> 3:18:24.000
 So maybe you'll start to see some joint failures

3:18:24.000 --> 3:18:28.000
 or some actuator failures more over time.

3:18:28.000 --> 3:18:31.000
 I understand that this is extremely pre-stage.

3:18:31.000 --> 3:18:37.000
 Also, we as humans have based so much fantasy and fiction

3:18:37.000 --> 3:18:39.000
 or superhuman capabilities.

3:18:39.000 --> 3:18:42.000
 Like all of us don't want to walk right over there.

3:18:42.000 --> 3:18:46.000
 We want to extend our arms, and we have all these, you know,

3:18:46.000 --> 3:18:49.000
 a lot of fantasy fantastical designs.

3:18:49.000 --> 3:18:53.000
 So considering everything else that is going on in terms

3:18:53.000 --> 3:18:57.000
 of batteries and intensity of compute,

3:18:57.000 --> 3:19:01.000
 maybe you can leverage all those aspects into coming up

3:19:01.000 --> 3:19:04.000
 with something, well, I don't know,

3:19:04.000 --> 3:19:08.000
 more interesting in terms of the robot that you're building,

3:19:08.000 --> 3:19:13.000
 and I'm hoping you're able to explore those directions.

3:19:13.000 --> 3:19:15.000
 Yeah, I mean, I think it would be cool to have like,

3:19:15.000 --> 3:19:17.000
 you know, make Inspector Gadget real.

3:19:17.000 --> 3:19:19.000
 That would be pretty sweet.

3:19:19.000 --> 3:19:23.000
 So yeah, I mean, right now we just want to make

3:19:23.000 --> 3:19:27.000
 a basic humanoid work well, and our goal is fastest path

3:19:27.000 --> 3:19:30.000
 to a useful humanoid robot.

3:19:30.000 --> 3:19:34.000
 I think this will ground us in reality, literally,

3:19:34.000 --> 3:19:39.000
 and ensure that we are doing something useful.

3:19:39.000 --> 3:19:43.000
 Like one of the hardest things to do is to be useful,

3:19:43.000 --> 3:19:48.000
 to actually, and then to have high utility under the curve

3:19:48.000 --> 3:19:51.000
 of like how many people did you help, you know,

3:19:51.000 --> 3:19:56.000
 how much help did you provide to each person on average,

3:19:56.000 --> 3:19:58.000
 and then how many people did you help?

3:19:58.000 --> 3:20:03.000
 The total utility, like trying to actually ship useful product

3:20:03.000 --> 3:20:06.000
 that people like to a large number of people

3:20:06.000 --> 3:20:08.000
 is so insanely hard.

3:20:08.000 --> 3:20:10.000
 It boggles the mind.

3:20:10.000 --> 3:20:12.000
 You know, so I can say like, man, there's a hell of a difference

3:20:12.000 --> 3:20:14.000
 between a company that has shipped product

3:20:14.000 --> 3:20:16.000
 and one that has not shipped product.

3:20:16.000 --> 3:20:19.000
 This is night and day, and then even once you ship product,

3:20:19.000 --> 3:20:22.000
 can you make the cost, the value of the output

3:20:22.000 --> 3:20:25.000
 worth more than the cost of the input,

3:20:25.000 --> 3:20:28.000
 which is, again, insanely difficult, especially with hardware.

3:20:28.000 --> 3:20:32.000
 So, but I think over time, I think it would be cool

3:20:32.000 --> 3:20:36.000
 to do creative things and have like eight arms and whatever,

3:20:36.000 --> 3:20:40.000
 and have different versions, and maybe, you know,

3:20:40.000 --> 3:20:45.000
 there'll be some hardware, like companies that are able

3:20:45.000 --> 3:20:49.000
 to add things to an optimist, like maybe we, you know,

3:20:49.000 --> 3:20:52.000
 add a power port or something like that or attach them,

3:20:52.000 --> 3:20:55.000
 and you can add, you know, add attachments to your optimist,

3:20:55.000 --> 3:20:57.000
 like you can add them to your phone.

3:20:57.000 --> 3:21:00.000
 There could be a lot of cool things that could be done over time,

3:21:00.000 --> 3:21:03.000
 and it could be maybe an ecosystem of small companies that,

3:21:03.000 --> 3:21:07.000
 or big companies that make add-ons for optimists.

3:21:07.000 --> 3:21:13.000
 So with that, I'd like to thank the team for their hard work.

3:21:13.000 --> 3:21:15.000
 You guys are awesome.

3:21:15.000 --> 3:21:24.000
 And thank you all for coming, and for everyone online,

3:21:24.000 --> 3:21:26.000
 thanks for tuning in.

3:21:26.000 --> 3:21:29.000
 And I think this will be one of those great videos where you can,

3:21:29.000 --> 3:21:33.000
 like, you can fast-forward to the bits that you find most interesting,

3:21:33.000 --> 3:21:38.000
 but we try to give you a tremendous amount of detail,

3:21:38.000 --> 3:21:41.000
 literally so that you can look at the video at your leisure,

3:21:41.000 --> 3:21:43.000
 and you can focus on the parts that you find interesting

3:21:43.000 --> 3:21:45.000
 and skip the other parts.

3:21:45.000 --> 3:21:49.000
 So thank you all, and we'll do this, try to do this every year,

3:21:49.000 --> 3:21:53.000
 and we might do a monthly podcast even,

3:21:53.000 --> 3:21:58.000
 but I think it would be, you know, great to sort of bring you along

3:21:58.000 --> 3:22:02.000
 for the ride and, like, show you what cool things are happening,

3:22:02.000 --> 3:22:04.000
 and, yeah, thank you.

3:22:04.000 --> 3:22:06.000
 All right, thanks.

3:22:06.000 --> 3:22:25.000
 Thank you.

3:22:25.000 --> 3:22:44.000
 Thank you.

3:22:44.000 --> 3:23:00.000
 Thank you.

